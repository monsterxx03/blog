<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tech on Shining Moon</title>
    <link>https://blog.monsterxx03.com/categories/tech/</link>
    <description>Recent content in Tech on Shining Moon</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Wed, 28 Feb 2018 21:45:23 +0800</lastBuildDate>
    
	<atom:link href="https://blog.monsterxx03.com/categories/tech/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Access sensitive variables on AWS lambda</title>
      <link>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</link>
      <pubDate>Wed, 28 Feb 2018 21:45:23 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</guid>
      <description>AWS lambda is convenient to run simple serverless application, but how to access sensitive data in code? like password,token&amp;hellip;
Usually, we inject secrets as environment variables, but they&amp;rsquo;re still visable on lambda console. I don&amp;rsquo;t use it in aws lambda.
The better way is use aws parameter store as configuration center. It can work with KMS to encrypt your data.
Code example:
client = boto3.client(&#39;ssm&#39;) resp = client.get_parameter( Name=&#39;/redshift/admin/password&#39;, WithDecryption=True ) resp: { &amp;quot;Parameter&amp;quot;: { &amp;quot;Name&amp;quot;: &amp;quot;/redshift/admin/password&amp;quot;, &amp;quot;Type&amp;quot;: &amp;quot;SecureString&amp;quot;, &amp;quot;Value&amp;quot;: &amp;quot;password value&amp;quot;, &amp;quot;Version&amp;quot;: 1 } }  Things you need to do to make it work:</description>
    </item>
    
    <item>
      <title>Glow Infra Evolution</title>
      <link>https://blog.monsterxx03.com/2018/02/23/glow-infra-evolution/</link>
      <pubDate>Fri, 23 Feb 2018 23:25:13 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/02/23/glow-infra-evolution/</guid>
      <description>Glow data infrastructure çš„æ¼”åŒ– Glow ä¸€å‘æ˜¯ä¸€ä¸ª data driven åšå†³ç­–çš„å…¬å¸ï¼Œç¨³å®šé«˜æ•ˆçš„å¹³å°æ˜¯å¿…ä¸å¯å°‘çš„æ”¯æ’‘, æœ¬æ–‡æ€»ç»“å‡ å¹´é‡Œå…¬å¸ data infrastructure çš„æ¼”è¿›è¿‡ç¨‹.
ç»“åˆä¸šåŠ¡ç‰¹ç‚¹åšæŠ€æœ¯é€‰å‹å’Œå®ç°æ—¶å€™çš„å‡ ä¸ªåŸåˆ™:
 real time åˆ†æçš„éœ€æ±‚ä¸é«˜ï¼Œæ—¶é—´ delta æ§åˆ¶åœ¨1 å°æ—¶ä»¥å†…å¯æ¥å— . æ”¯æŒå¿«é€Ÿçš„äº¤äº’å¼æŸ¥è¯¢. åº•å±‚å¹³å°å°½é‡é€‰æ‹© AWS æ‰˜ç®¡æœåŠ¡, å‡å°‘ç»´æŠ¤æˆæœ¬. é‡åˆ°æ•…éšœ, æ•°æ®å¯ä»¥ delay ä½†ä¸èƒ½ä¸¢. å¯å›æº¯å†å²æ•°æ®. æˆæœ¬å¯æ§.  ç”¨åˆ°çš„ AWS æœåŠ¡:
 æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢: S3, Redshift (spectrum), Athena ETL: DMS, EMR, Kinesis, Firehose, Lambda  å¼€æºè½¯ä»¶: td-agent, maxwell
æ•°æ®æ¥æº:
 çº¿ä¸Šä¸šåŠ¡æ•°æ®åº“ ç”¨æˆ·æ´»åŠ¨äº§ç”Ÿçš„ metrics log ä»å„ç§ç¬¬ä¸‰æ–¹æœåŠ¡ api æ‹‰ä¸‹æ¥çš„æ•°æ® (emailä¹‹ç±»)  æœ€æ—©æœŸ åˆšå¼€å§‹çš„æ—¶å€™ä¸šåŠ¡å•çº¯ï¼Œæ•°æ®é‡ä¹Ÿå°‘, æ‰€æœ‰æ•°æ®éƒ½ç”¨ MySQL å­˜å‚¨ï¼Œæ­äº†å° slave, åˆ†ææŸ¥è¯¢éƒ½åœ¨ slave ä¸Šè¿›è¡Œ.</description>
    </item>
    
    <item>
      <title>Get Real Client Ip on AWS</title>
      <link>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</link>
      <pubDate>Thu, 01 Feb 2018 15:20:37 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</guid>
      <description>If you run a webserver on AWS, get real client ip will be tricky if you didn&amp;rsquo;t configure server right and write code correctly.
Things related to client real ip:
 CloudFront (cdn) ALB (loadbalancer) nginx (on ec2) webserver (maybe a python flask application).  Request sequence diagram will be like following:
User&amp;rsquo;s real client ip is forwarded by front proxies one by one in head X-Forwarded-For.
For CloudFront:
 If user&amp;rsquo;s req header don&amp;rsquo;t have X-Forwarded-For, it will set user&amp;rsquo;s ip(from tcp connection) in X-Forwarded-For If user&amp;rsquo;s req already have X-Forwarded-For, it will append user&amp;rsquo;s ip(from tcp connection) to the end of X-Forwarded-For  For ALB, rule is same as CloudFront, so the X-Forwarded-For header pass to nginx will be the value received from CloudFront + CloudFront&amp;rsquo;s ip.</description>
    </item>
    
    <item>
      <title>DynamoDB</title>
      <link>https://blog.monsterxx03.com/2017/12/15/dynamodb/</link>
      <pubDate>Fri, 15 Dec 2017 22:24:36 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/12/15/dynamodb/</guid>
      <description>DynamoDB æ˜¯ AWS çš„æ‰˜ç®¡ NoSQL æ•°æ®åº“ï¼Œå¯ä»¥å½“ä½œç®€å•çš„ KV æ•°æ®åº“ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ–‡æ¡£æ•°æ®åº“ä½¿ç”¨.
Data model ç»„ç»‡æ•°æ®çš„å•ä½æ˜¯ table, æ¯å¼  table å¿…é¡»è®¾ç½® primary key, å¯ä»¥è®¾ç½®å¯é€‰çš„ sort key æ¥åšç´¢å¼•.
æ¯æ¡æ•°æ®è®°ä½œä¸€ä¸ª item, æ¯ä¸ª item å«æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª attribute, å…¶ä¸­å¿…é¡»åŒ…æ‹¬ primary key.
attribute å¯¹åº”çš„ value æ”¯æŒä»¥ä¸‹å‡ ç§ç±»å‹:
 Number, ç”±äº DynamoDB çš„ä¼ è¾“åè®®æ˜¯ http + json, ä¸ºäº†è·¨è¯­è¨€çš„å…¼å®¹æ€§, number ä¸€å¾‹ä¼šè¢«è½¬æˆ string ä¼ è¾“. Binary, ç”¨æ¥è¡¨ç¤ºä»»æ„çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œä¼šç”¨ base64 encode åä¼ è¾“. Boolean, true or false Null Document ç±»å‹åŒ…å« List å’Œ Map, å¯ä»¥äº’ç›¸åµŒå¥—.  List, ä¸ªæ•°æ— é™åˆ¶, æ€»å¤§å°ä¸è¶…è¿‡ 400KB Map, å±æ€§ä¸ªæ•°æ— é™åˆ¶ï¼Œæ€»å¤§å°ä¸è¶…è¿‡ 400 KB, åµŒå¥—å±‚çº§ä¸è¶…è¿‡ 32 çº§.</description>
    </item>
    
    <item>
      <title>Handle outage</title>
      <link>https://blog.monsterxx03.com/2017/12/10/handle-outage/</link>
      <pubDate>Sun, 10 Dec 2017 11:13:53 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/12/10/handle-outage/</guid>
      <description>A few weeks ago, production environment came to an outage, solve it cost me 8 hours (from 3am to 11am) although total down time is not long, really a bad expenrience. Finally, impact was mitigated, and I&amp;rsquo;m working on a long term solution. I learned some important things from this accident.
The outage I received alarms about live performance issue at 3am, first is server latency increaing, soon some service&amp;rsquo;s health check failed due to high load.</description>
    </item>
    
    <item>
      <title>AWS DMS notes</title>
      <link>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</link>
      <pubDate>Sat, 14 Oct 2017 22:33:36 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</guid>
      <description>AWS&amp;rsquo;s DMS (Data migration service) can be used to do incremental ETL between databases. I use it to load data from RDS (MySQL) to Redshift.
It works, but have some concerns. Take some notes when doing this project.
Prerequisites Source RDS must:
 Enable automatic backups Increase binlog remain time, call mysql.rds_set_configuration(&#39;binlog retention hours&#39;, 24); Set binlog_format to ROW. Privileges on source RDS: REPLICATION CLIENT, REPLICATION SLAVE, SELECT on replication target tables  DDL on source table Redshift has some limits on change columns:</description>
    </item>
    
    <item>
      <title>Get all invalid PTR record on  Route53</title>
      <link>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</link>
      <pubDate>Fri, 29 Sep 2017 08:55:18 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</guid>
      <description>I use autoscaling group to manage stateless servers. Servers go up and down every day.
Once server is up, I will add a PTR record for it&amp;#8217;s internal ip. But when it&amp;#8217;s down, I didn&amp;#8217;t cleanup the PTR record. As times fly, a lot of invalid PTR records left in Route53.
To cleanup those PTR records realtime, you can write a lambda function, use server termination event as trigger. But how to cleanup the old records at once?</description>
    </item>
    
    <item>
      <title>Build private static website on S3</title>
      <link>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</link>
      <pubDate>Sat, 19 Aug 2017 07:28:16 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</guid>
      <description>Build static website on S3 is very easy, but by default, it can be accessed by open internet.It will be super helpful if we can build website only available in VPC. Then we can use it to host internal deb repo, doc site&amp;#8230;
Steps are very easy, you only need VPC endpoints and S3 bucket policy.
AWS api is open to internet, if you need to access S3 in VPC, your requests will pass through VPC&amp;#8217;s internet gateway or NAT gateway.</description>
    </item>
    
    <item>
      <title>Use redshift spectrum to do query on s3</title>
      <link>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</link>
      <pubDate>Fri, 21 Jul 2017 03:10:58 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</guid>
      <description>ä½¿ç”¨ redshift spectrum æŸ¥è¯¢ S3 æ•°æ® é€šå¸¸ä½¿ç”¨ redshift åšæ•°æ®ä»“åº“çš„æ—¶å€™è¦åšå¤§é‡çš„ ETL å·¥ä½œï¼Œä¸€èˆ¬æµç¨‹æ˜¯æŠŠå„ç§æ¥æºçš„æ•°æ®æ£é¼“æ£é¼“ä¸¢åˆ° S3 ä¸Šå»ï¼Œå†ä» S3 å€’è…¾è¿› redshift. å¦‚æœä½ æœ‰å¤§é‡çš„å†å²æ•°æ®è¦å¯¼è¿› redshiftï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ä¼šå¾ˆç—›è‹¦ï¼Œredshift å¯¹ä¸€æ¬¡å€’å…¥å¤§é‡æ•°æ®å¹¶ä¸å‹å¥½ï¼Œä½ è¦åˆ†æ‰¹æ¥åšã€‚
ä»Šå¹´4æœˆçš„æ—¶å€™ï¼Œ redshift å‘å¸ƒäº†ä¸€ä¸ªæ–°åŠŸèƒ½ spectrum, å¯ä»¥ä» redshift é‡Œç›´æ¥æŸ¥è¯¢ s3 ä¸Šçš„ç»“æ„åŒ–æ•°æ®ã€‚æœ€è¿‘æŠŠéƒ¨åˆ†æ•°æ®ä»“åº“ç›´æ¥è¿ç§»åˆ°äº† spectrum, æ­£å¥½æ¥è®²è®²ã€‚
åŠ¨æœº Glow çš„æ•°æ®ä»“åº“å»ºåœ¨ redshift ä¸Šï¼Œ åˆåˆ†æˆäº†ä¸¤ä¸ªé›†ç¾¤ï¼Œä¸€ä¸ª ssd çš„é›†ç¾¤å­˜æ”¾æœ€è¿‘ 4 ä¸ªæœˆçš„æ•°æ®ï¼Œä¾›äº§å“åˆ†æï¼Œmetrics report, debug ç­‰ç­‰ adhoc çš„æŸ¥è¯¢ã€‚4ä¸ªæœˆä¹‹å‰çš„æ•°æ®å­˜æ”¾åœ¨ä¸€ä¸ª hdd çš„é›†ç¾¤é‡Œï¼Œä¾¿å®œå®¹é‡å¤§ï¼ŒæŸ¥è¯¢æ…¢ã€‚
ä½†æ˜¯æ—¶é—´é•¿äº† hdd çš„é›†ç¾¤ä¹Ÿæ˜¯æœ‰æ‰©å®¹éœ€æ±‚çš„ï¼Œè€Œä½¿ç”¨é¢‘ç‡åˆå®åœ¨æ˜¯ä¸é«˜ï¼Œå…¶å®å¾ˆæµªè´¹, è¿™å°±æ˜¯è¿ç§»åˆ° spectrum çš„åŠ¨æœºã€‚
ä½¿ç”¨ Spectrum Redshift spectrum åº•å±‚å…¶å®æ˜¯åŸºäº AWS çš„å¦ä¸€ä¸ªæœåŠ¡ athena çš„ã€‚athena æ˜¯ä¸ª Presto å’Œ Hive æ‚äº¤äº§ç‰©ï¼Œ DDL ç”¨ Hive è¯­æ³•ï¼Œ æŸ¥è¯¢ç”¨çš„ sql ç”± Presto æ”¯æŒ, æ„Ÿè§‰æ€ªæ€ªçš„ï¼Œè¿™é‡Œä¸å¤šå±•å¼€è®² athena, çŸ¥é“ redshift spectrum å…¶å®æ˜¯é€šè¿‡ athena å¯¹æ¥çš„ s3 å°±è¡Œäº†ã€‚</description>
    </item>
    
    <item>
      <title>Enable coredump on ubuntu 16.04</title>
      <link>https://blog.monsterxx03.com/2017/07/15/enable-coredump-on-ubuntu-16.04/</link>
      <pubDate>Sat, 15 Jul 2017 02:35:52 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/07/15/enable-coredump-on-ubuntu-16.04/</guid>
      <description>Coredump file is useful for debuging program crash. This post will show several settings related to coredump.
Enable coredump If you run program from shell , enable coredump via unlimit -c unlimitedï¼Œ then check unlimit -a | grep core, if it shows unlimited, coredump is enabled for your current session.
If your program is hosted by systemd, you need to edit your program&amp;#8217;s service unit file&amp;#8217;s [Service] section, add LimitCORE=infinity to enable coredump.</description>
    </item>
    
    <item>
      <title>Python Web åº”ç”¨æ€§èƒ½è°ƒä¼˜</title>
      <link>https://blog.monsterxx03.com/2017/07/01/python-web-%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</link>
      <pubDate>Sat, 01 Jul 2017 23:38:24 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/07/01/python-web-%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</guid>
      <description>Python web åº”ç”¨æ€§èƒ½è°ƒä¼˜ ä¸ºäº†å¿«é€Ÿä¸Šçº¿ï¼Œæ—©æœŸå¾ˆå¤šä»£ç åŸºæœ¬æ˜¯æ€ä¹ˆæ–¹ä¾¿æ€ä¹ˆæ¥ï¼Œè¿™æ ·å°±ç•™ä¸‹äº†å¾ˆå¤šéšæ‚£ï¼Œæ€§èƒ½ä¹Ÿä¸æ˜¯å¾ˆç†æƒ³ï¼Œpython å› ä¸º GIL çš„åŸå› ï¼Œåœ¨æ€§èƒ½ä¸Šæœ‰å¤©ç„¶åŠ£åŠ¿ï¼Œå³ä½¿ç”¨äº† gevent/eventlet è¿™ç§åç¨‹æ–¹æ¡ˆï¼Œä¹Ÿå¾ˆå®¹æ˜“å› ä¸ºè€—æ—¶çš„ CPU æ“ä½œé˜»å¡ä½æ•´ä¸ªè¿›ç¨‹ã€‚å‰é˜µå­å¯¹åŸºç¡€ä»£ç åšäº†äº›é‡æ„ï¼Œæ•ˆæœæ˜¾è‘—ï¼Œè®°å½•ä¸€äº›ã€‚
è®¾å®šç›®æ ‡:
 æ€§èƒ½æé«˜äº†ï¼Œæœ€ç›´æ¥çš„æ•ˆæœå½“ç„¶æ˜¯èƒ½ç”¨æ›´å°‘çš„æœºå™¨å¤„ç†ç›¸åŒæµé‡ï¼Œç›®æ ‡æ˜¯å…³é—­ 20% çš„ stateless webserver. å°½é‡åœ¨æ¡†æ¶ä»£ç ä¸Šåšæ”¹åŠ¨ï¼Œä¸åŠ¨ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚ ä½é£é™© (å†å²ç»éªŒå‘Šè¯‰æˆ‘ä»¬ï¼ŒåŠ¨æ€ä¸€æ—¶çˆ½ï¼Œé‡æ„ç«è‘¬åœº&amp;hellip;.)  æ²»æ ‡ å¸¸è§åœºæ™¯æ˜¯å¤§å®¶å¼€å¼€å¿ƒå¿ƒåšå®Œä¸€ä¸ª featureï¼Œ sandbox æµ‹è¯•ä¹Ÿæ²¡å•¥é—®é¢˜ï¼Œä¸Šçº¿äº†ï¼Œç»“æœ server load é£™å‡ï¼Œå„ç§ timeout éƒ½æ¥äº†ï¼Œè¦ä¹ˆ rollback ä»£ç ï¼Œè¦ä¹ˆåŠ æœºå™¨ã€‚é—®é¢˜ä»£ç åœ¨å“ª?
æˆ‘ä»¬ç›‘æ§ç”¨çš„æ˜¯ datadog (statsdåè®®)ï¼Œå¯¹è¿™ç§é—®é¢˜æœ€æœ‰æ•ˆçš„æŒ‡æ ‡æ˜¯çœ‹æ¯ä¸ªæ¥å£çš„ avg_latency * req_count å¾—åˆ°æ¯ä¸ªæ¥å£åœ¨ä¸€æ®µæ—¶é—´å†…çš„æ€»è€—æ—¶ï¼Œåœ¨æŸ±çŠ¶å›¾ä¸Šæœ€é•¿çš„é‚£å—å°±æ˜¯å¯¹æ€§èƒ½å½±å“æœ€å¤§çš„æ¥å£ã€‚è¿›ä¸€æ­¥çš„è°ƒè¯•å°±é  cProfile å’Œè¯»ä»£ç äº†ã€‚
ä½†å¾ˆå¤šæ—¶å€™å‡ºé—®é¢˜çš„ä»£ç é€»è¾‘å·¨å¤æ‚ï¼Œè¿˜å¾ˆå¤šäººæ”¹åŠ¨è¿‡ï¼Œå¼€å‘å’Œ sandbox ç¯å¢ƒæ•°æ®çš„é‡å’Œçº¿ä¸Šå·®è·å¤ªå¤§ï¼Œæ— æ³•å¤ç°é—®é¢˜ï¼Œåœ¨çº¿ä¸Šç”¨ cProfile åªèƒ½æµ‹åªè¯»æ¥å£(ä¸ºäº†ä¸å†™åç”¨æˆ·æ•°æ®)ã€‚
è€Œä¸”è¿™ç§æ–¹å¼åªèƒ½æ²»æ ‡ï¼Œè°ƒè¯•ä¸ªåˆ«æ…¢çš„ä¸šåŠ¡æ¥å£ï¼Œç›®æ ‡é‡Œè¯´äº†åªæƒ³æ”¹æ¡†æ¶ï¼Œæé«˜æ•´ä½“æ€§èƒ½ï¼Œæ€ä¹ˆæ•´?
æ²»æœ¬ æˆ‘å¸Œæœ›èƒ½å¯¹è¿è¡Œæ—¶è¿›ç¨‹çŠ¶æ€æ‰“ snapshotï¼Œæ¯æ¬¡å¿«ç…§è®°å½•ä¸‹å½“å‰çš„å‡½æ•°è°ƒç”¨æ ˆï¼Œå åˆå¤šæ¬¡é‡‡æ ·ï¼Œå‡ºç°æ¬¡æ•°å¤šçš„å‡½æ•°å¿…ç„¶å°±æ˜¯ç“¶é¢ˆæ‰€åœ¨. è¿™æ€æƒ³åœ¨å…¶ä»–è¯­è¨€é‡Œç”¨çš„ä¹Ÿå¾ˆå¤šï¼Œå…¶å®å°±æ˜¯ Brendan Gregg çš„ flamegraph.
ä»¥å‰å†…éƒ¨åšè¿‡ç±»ä¼¼çš„äº‹æƒ…ï¼Œä¸è¿‡ä»£ç æ˜¯ä¾µå…¥å¼çš„ï¼Œåœ¨è¿è¡Œæ—¶é€šè¿‡ signal, inspect, traceback ç­‰æ¨¡å—ï¼Œå®šæœŸæ‰“è°ƒç”¨æ ˆçš„ snapshot, è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œè½¬æˆ svg çš„ flamegraph æ¥çœ‹ï¼Œä½†æ˜¯ overhead å¤ªé«˜ï¼Œåæ¥å¼ƒç”¨äº†ã€‚</description>
    </item>
    
    <item>
      <title>Build deb repository with fpm , aptly and s3</title>
      <link>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</link>
      <pubDate>Fri, 23 Jun 2017 09:40:58 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</guid>
      <description>I&amp;#8217;m lazy, I don&amp;#8217;t want to be deb/rpm expert, I don&amp;#8217;t want to maintain repo server. I want as less maintenance effort as possible. ğŸ™‚
Combine tools fpm, aptly with aws s3, we can do it.
Use fpm to convert python package to deb fpm can transform python/gem/npm/dir/&amp;#8230; to deb/rpm/solaris/&amp;#8230; packages
Example:
fpm -s python -t deb -m xyj.asmy@gmail.com --verbose -v 0.10.1 --python-pip /usr/local/pip Flask  It will transform Flask 0.</description>
    </item>
    
    <item>
      <title>Debug python performance issue with pyflame</title>
      <link>https://blog.monsterxx03.com/2017/06/05/debug-python-performance-issue-with-pyflame/</link>
      <pubDate>Mon, 05 Jun 2017 09:50:44 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/06/05/debug-python-performance-issue-with-pyflame/</guid>
      <description>pyflame is an opensource tool developed by uber: https://github.com/uber/pyflame
It can take snapshots of running python process, combined with flamegraph.pl, can output flamegraph picture of python call stacks. Help analyze bottleneck of python program, needn&amp;#8217;t inject any perf code into your application, and overhead is very low.
Basic Usage sudo pyflame -s 10 -x -r 0.001 $pid | ./flamegraph.pl &amp;gt; perf.svg
 -s, how many seconds to run -r, sample rate (seconds)  Your output will be something like following:</description>
    </item>
    
    <item>
      <title>Designing data intensive application, reading notes, Part 2</title>
      <link>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</link>
      <pubDate>Wed, 17 May 2017 09:12:44 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</guid>
      <description>Chapter 4, 5, 6
Encoding formats xml, json, msgpack are text based encoding format, they can&amp;#8217;t carry binary bytes (useless you encode them in base64, size grows 33%). And they cary schema definition with data, wast a lot of space.
thrift, protobuf are binary format, can take binary bytes, only carry data, the schema is defined with IDL(interface definition language). They have code generation tool to generate code to encode and decode data, along with check.</description>
    </item>
    
    <item>
      <title>Designing data intensive application, reading notes, Part 1</title>
      <link>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</link>
      <pubDate>Thu, 04 May 2017 16:27:52 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</guid>
      <description>&lt;p&gt;Notes when reading chapter 2 &amp;#8220;Data models and query languages&amp;#8221;, chapter 3 &amp;#8220;Storage and retrieval&amp;#8221;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Infrastructure as Code</title>
      <link>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</link>
      <pubDate>Fri, 21 Apr 2017 16:25:07 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</guid>
      <description>&lt;p&gt;Create virtual resource on AWS is very convenient, but how to manage them will be a problem when your size grow.&lt;/p&gt;

&lt;p&gt;You will come to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to explain the detail online settings for your colleagues (like: how our prod vpc is setup?what&amp;#8217;s the DHCP option set?), navigate around AWS console is okay, but not convenient.&lt;/li&gt;
&lt;li&gt;Who did what to which resource at when? AWS have a service called &lt;code&gt;Config&lt;/code&gt;, can be used to track this change, but if you want to make things as clear as viewing git log, still a lot of works to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally, we should manage AWS resources like code, all changes kept in VCS, so called &lt;code&gt;Infrastructure as Code&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve tried three ways to do it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ansible&lt;/li&gt;
&lt;li&gt;CloudFormation&lt;/li&gt;
&lt;li&gt;terraform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article, I&amp;#8217;ll compare them, however, the conclusion is to use terraform ğŸ™‚&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Concurrency in Go, Reading Notes</title>
      <link>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</link>
      <pubDate>Wed, 19 Apr 2017 16:26:58 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</guid>
      <description>&lt;p&gt;A few notes taken when reading &lt;Concurrency in Go&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL partition table</title>
      <link>https://blog.monsterxx03.com/2017/04/05/mysql-partition-table/</link>
      <pubDate>Wed, 05 Apr 2017 16:23:32 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/04/05/mysql-partition-table/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;

&lt;p&gt;MySQL has buildin partition table support, which can help split data accross multi tables,&lt;/p&gt;

&lt;p&gt;and provide a unified query interface as normal tables.&lt;/p&gt;

&lt;p&gt;Benefit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy data management: If we need to archive old data, and our table is partitioned by datetime, we can drop old partition directly.&lt;/li&gt;
&lt;li&gt;Speed up query based on partition key(partitoin pruning)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Limit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For partition table, every unique key must use every column in table&amp;#8217;s partition expression(include primary key)&lt;/li&gt;
&lt;li&gt;For innodb engine, paritioned table can&amp;#8217;t have foreign key,and can&amp;#8217;t have columns referenced by foreign keys.&lt;/li&gt;
&lt;li&gt;For MyISAM engine, mysql version &amp;lt;= 5.6.5, DML operation will lock all partition as a whole.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ElasticSearch cluster</title>
      <link>https://blog.monsterxx03.com/2017/03/22/elasticsearch-cluster/</link>
      <pubDate>Wed, 22 Mar 2017 16:22:32 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/03/22/elasticsearch-cluster/</guid>
      <description>&lt;p&gt;In this article, let&amp;#8217;s talk about ElasticSearch&amp;#8217;s cluster mode, which means multi nodes ElasticSearch.&lt;/p&gt;

&lt;h2 id=&#34;basic-concepts&#34;&gt;Basic concepts&lt;/h2&gt;

&lt;p&gt;cluster: A collection of server nodes with same &lt;code&gt;cluster.name&lt;/code&gt; settings in elasticsearch.yaml&lt;/p&gt;

&lt;p&gt;primary shards: Divide a index into multi parts(by default 5), shards of an index can be distributed over multi nodes. It enables scale index horizontally and make access to index parallelly(accross multi nodes).&lt;/p&gt;

&lt;p&gt;replicas: backup for shards, also replicas can handle search requests, which means you can scale your search capacity horizontally via replicas.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bigtable notes</title>
      <link>https://blog.monsterxx03.com/2016/12/11/bigtable-notes/</link>
      <pubDate>Sun, 11 Dec 2016 16:20:24 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/12/11/bigtable-notes/</guid>
      <description>&lt;p&gt;æ‚ä¹±ç¬”è®°ï¼Œè¾…åŠ©è¯»paper.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GFS notes</title>
      <link>https://blog.monsterxx03.com/2016/11/19/gfs-notes/</link>
      <pubDate>Sat, 19 Nov 2016 16:18:41 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/11/19/gfs-notes/</guid>
      <description>&lt;p&gt;çœ‹äº†ä¸‹å¾ˆä¹…å‰ google çš„ GFS è®ºæ–‡ï¼Œ åšç‚¹ç¬”è®°ã€‚&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Migrate to encrypted RDS</title>
      <link>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</link>
      <pubDate>Fri, 28 Oct 2016 16:17:30 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</guid>
      <description>&lt;p&gt;æœ€è¿‘å…¬å¸åœ¨åš HIPAA Compliance ç›¸å…³çš„äº‹æƒ…ï¼Œå…¶ä¸­è¦æ±‚ä¹‹ä¸€æ˜¯æ‰€æœ‰dbéœ€è¦å¼€å¯encryption.&lt;/p&gt;

&lt;p&gt;æ¯”è¾ƒéº»çƒ¦çš„æ˜¯rds çš„encryption åªèƒ½åœ¨åˆ›å»ºçš„æ—¶å€™è®¾å®šï¼Œæ— æ³•ä¹‹åä¿®æ”¹, æ‰€ä»¥å¿…é¡»å¯¹çº¿ä¸Šçš„db åšä¸€æ¬¡ migration.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL ç´¢å¼•ä¼˜åŒ–</title>
      <link>https://blog.monsterxx03.com/2016/07/26/mysql-index-optimization/</link>
      <pubDate>Tue, 26 Jul 2016 16:13:54 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/07/26/mysql-index-optimization/</guid>
      <description>&lt;p&gt;ä»€ä¹ˆæ˜¯ç´¢å¼•,ç´¢å¼•æ€ä¹ˆå»ºè¿™äº›åŸºæœ¬çš„å°±è·³è¿‡ä¸è°ˆäº†,æ•´ç†ä¸€äº›å‰æ®µæ—¶é—´ä¼˜åŒ–çº¿ä¸Š SQL æŸ¥è¯¢æ—¶ç¢°åˆ°çš„ä¸€äº›é—®é¢˜. ä¸»è¦è§£å†³ä¸‹é¢å‡ ä¸ªé—®é¢˜:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;å»ºç«‹ç´¢å¼•æ€æ ·é€‰æ‹©åˆé€‚çš„åˆ—.&lt;/li&gt;
&lt;li&gt;æ€æ ·è®© SQL èƒ½æœ‰æ•ˆåˆ©ç”¨ç´¢å¼•.&lt;/li&gt;
&lt;li&gt;å¦‚æœå¯¹ SQL æ•ˆç‡è¿›è¡Œè¯„ä¼°(å³è®¾ç½®ç´¢å¼•å‰åæ˜¯å¦çœŸçš„æœ‰æ€§èƒ½æå‡).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Redshift as data warehouse</title>
      <link>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</link>
      <pubDate>Sat, 16 Jul 2016 16:11:39 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</guid>
      <description>&lt;p&gt;Glow çš„ server infrastructure å…¨éƒ¨æ­å»ºåœ¨ AWS ä¸Šï¼Œä¸€èˆ¬è¦é€‰æ‹©ä¸€äº›åŸºç¡€æœåŠ¡çš„æ—¶å€™ï¼Œæ€»æ˜¯å…ˆçœ‹ AWS, åªè¦åŠŸèƒ½å’Œæˆæœ¬ç¬¦åˆè¦æ±‚ï¼Œä¸ä¼šç‰¹æ„é€‰æ‹©å¼€æºæ–¹æ¡ˆã€‚&lt;/p&gt;

&lt;p&gt;æ•°æ®ä»“åº“æˆ‘ä»¬é€‰æ‹©äº† AWS çš„ Redshift.&lt;/p&gt;

&lt;p&gt;åœ¨ä¸€å¹´å¤šçš„ä½¿ç”¨è¿‡ç¨‹ä¸­ Redshift çš„æ€§èƒ½å’Œç¨³å®šæ€§éƒ½ä¸é”™, å½“ç„¶ä¹Ÿæœ‰ä¸€äº›å‘, è¿™é‡Œæ•´ç†ä¸‹åœ¨ä½¿ç”¨ redshift çš„è¿‡ç¨‹ä¸­çš„ä¸€äº›ç»éªŒå’Œé‡åˆ°çš„å‘.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL innodb buffer pool</title>
      <link>https://blog.monsterxx03.com/2016/07/16/mysql-innodb-buffer-pool/</link>
      <pubDate>Sat, 16 Jul 2016 16:07:14 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/07/16/mysql-innodb-buffer-pool/</guid>
      <description>&lt;p&gt;æœ€è¿‘åœ¨å¯¹å…¬å¸çš„ MySQL æœåŠ¡å™¨åšæ€§èƒ½ä¼˜åŒ–, ä¸€ç›´å¯¹ innodb çš„å†…å­˜ä½¿ç”¨æ–¹å¼ä¸æ˜¯å¾ˆæ¸…æ¥š, ä¹˜è¿™æœºä¼šåšç‚¹æ€»ç»“.&lt;/p&gt;

&lt;p&gt;åœ¨é…ç½® MySQL çš„æ—¶å€™, ä¸€èˆ¬éƒ½ä¼šéœ€è¦è®¾ç½® _innodb_buffer_pool&lt;em&gt;size&lt;/em&gt;, åœ¨å°† MySQL è®¾ç½®åœ¨å•ç‹¬çš„æœåŠ¡å™¨ä¸Šæ—¶, ä¸€èˆ¬ä¼šè®¾ç½®ä¸ºç‰©ç†å†…å­˜çš„80%.&lt;/p&gt;

&lt;p&gt;ä¹‹å‰ä¸€ç›´ç–‘æƒ‘ MySQL æ˜¯æ€ä¹ˆç¼“å­˜æ•°æ®çš„(ä¸æ˜¯æŒ‡query cache), ç›´è§‰åº”è¯¥æ˜¯LRU, ä½†å¦‚æœ query ä¸€ä¸‹ä»ç£ç›˜ä¸Šè¯»å–å¤§é‡çš„æ•°æ®çš„è¯(å…¨è¡¨æ‰«ææˆ–æ˜¯ mysqldump), æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“å°±ä¼šæŠŠçƒ­æ•°æ®ç»™è¸¢å‡ºå»?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>