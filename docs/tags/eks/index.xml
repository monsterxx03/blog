<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Eks on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/eks/</link>
    <description>Recent content in Eks on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Thu, 01 Apr 2021 14:40:30 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/eks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rolling Upgrade Worker Nodes in EKS</title>
      <link>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</link>
      <pubDate>Thu, 01 Apr 2021 14:40:30 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</guid>
      <description>&lt;p&gt;EKS control plane 的升级是比较简单的, 直接在 aws console 上点下就可以了, 但 worker node 是自己用 asg(autoscaling group) 管理的, 升级 worker node 又不想影响业务是有讲究的.&lt;/p&gt;&#xA;&lt;p&gt;跑在 EKS 里, 且希望不被中断 traffic 的有:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;stateless 的 api server, queue consumer&lt;/li&gt;&#xA;&lt;li&gt;被 redis sentinel 监控着的 redis master/slave&lt;/li&gt;&#xA;&lt;li&gt;用于 cache 的 redis cluster&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;写了个内部工具, 把下面的流程全部自动化了. 这样升级 eks 版本, 需要更换 worker node 时候就轻松多了. 因为这个工具对部署情况做了很多假设和限制, 开源的价值不是很大.&lt;/p&gt;&#xA;&lt;h2 id=&#34;stateless-application&#34;&gt;Stateless application&lt;/h2&gt;&#xA;&lt;p&gt;stateless 的应用全部用 deployment 部署.&lt;/p&gt;&#xA;&lt;p&gt;一般建议的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;修改 asg 的 launch configuration, 指向新版本的 ami&lt;/li&gt;&#xA;&lt;li&gt;把所有老的 worker node 用 kubectl cordon 标记成 unschedulable&lt;/li&gt;&#xA;&lt;li&gt;关闭 cluster-autoscaler&lt;/li&gt;&#xA;&lt;li&gt;修改 asg 的 desired count, 让 asg 用新 ami 启动新的 worker node&lt;/li&gt;&#xA;&lt;li&gt;用 kubectl drain 把老 worker node 上的 pod evict 掉, 让它们 schedule 到新的 worker node 上.&lt;/li&gt;&#xA;&lt;li&gt;重新开启 cluster autoscaler, 等它把老的闲置 worker node 关闭.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这里有些问题:&lt;/p&gt;</description>
    </item>
    <item>
      <title>解决 k8s 1.16 apiVersion deprecation 造成的 helm revision 冲突</title>
      <link>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</link>
      <pubDate>Tue, 16 Jun 2020 16:02:58 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</guid>
      <description>&lt;p&gt;最近开始把线上的 k8s 从 1.15 升级到 1.16, 1.16 里有一些 api verison 被彻底废弃, 需要迁移到新的 api version, 具体有: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;有两个问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;集群中使用的一些第三方 controller(nginx-ingress, external-dns-controller&amp;hellip;), 调用的 apiVersion 需要升级.&lt;/li&gt;&#xA;&lt;li&gt;已存在集群中的 objects(Deployment/ReplicaSet&amp;hellip;), 是否需要处理, eg: Deployment: extensions/v1beta1 -&amp;gt; apps/v1.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;第一个问题好解决, 升级一下对应的 image 版本就行了, 只要还在维护的 controller, 都已经升级到支持 1.16. 自己写的工具链也排查下是否有还在使用老版本 api 的, 因为我用的是 aws eks, 开下 &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html&#34;&gt;control-plane-logs&lt;/a&gt; 里的 audit log, 可以看还有什么在调用老的 api.&lt;/p&gt;&#xA;&lt;p&gt;第二个问题是不需要改, 已存在的 objects 无法修改 apiVersion, 集群升级到 1.16 后， 从新的 apiVersion 里能 pull 到之前的数据, apiVersion 字段自动就升级了.&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 eks 中正确设置 IAM 权限</title>
      <link>https://blog.monsterxx03.com/2020/04/16/%E5%9C%A8-eks-%E4%B8%AD%E6%AD%A3%E7%A1%AE%E8%AE%BE%E7%BD%AE-iam-%E6%9D%83%E9%99%90/</link>
      <pubDate>Thu, 16 Apr 2020 11:03:39 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/04/16/%E5%9C%A8-eks-%E4%B8%AD%E6%AD%A3%E7%A1%AE%E8%AE%BE%E7%BD%AE-iam-%E6%9D%83%E9%99%90/</guid>
      <description>&lt;p&gt;在代码中调用 aws api 的时候常用两种方法:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接传入 aws accessKey/secretKey&lt;/li&gt;&#xA;&lt;li&gt;使用 instance profile&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;前者一般是创建一个 IAM 用户, 绑定对应权限, 生成 keypair, 在 k8s 环境里, 把 keypair 放在 Secrets 里, 或通过环境变量注入. 好处是可以每个应用单独设置,&#xA;但需要自己管理 keypair.&lt;/p&gt;&#xA;&lt;p&gt;后者创建一个 IAM role, 绑定对应权限, 创建 ec2 的时候选择对应的 role. 跑在该 ec2 instance 上的程序自动能拿到对应的 IAM 权限. 好处是不必自己管理 keypair,&#xA;缺点是跑在同一 server 上的程序权限都一样.&lt;/p&gt;&#xA;&lt;p&gt;eks 1.14 里有个两全齐美的办法: serviceaccount role: &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html&#34;&gt;https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;可以把 IAM role 绑定在 pod 使用的 ServiceAccount 上, 既避免了管理 keypair 的麻烦, 也可以 per 应用得设置权限.&lt;/p&gt;</description>
    </item>
    <item>
      <title> 迁移到 k8s 过程中碰到的问题</title>
      <link>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Tue, 23 Jul 2019 12:32:08 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &amp;hellip;&amp;hellip; 记录一下(大多都不是 k8s 的问题).&lt;/p&gt;&#xA;&lt;h2 id=&#34;nginx-ingress-controller-的问题&#34;&gt;nginx ingress controller 的问题&lt;/h2&gt;&#xA;&lt;h3 id=&#34;zero-downtime-pods-upgrade&#34;&gt;zero-downtime pods upgrade&lt;/h3&gt;&#xA;&lt;p&gt;默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip),&#xA;和直接使用 service cluster ip 比, 好处是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以支持 sticky session&lt;/li&gt;&#xA;&lt;li&gt;可以用 round robin 之外的负载均衡算法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip&#xA;list(nginx 内置的 lua 来修改内存中的 ip list)&lt;/p&gt;&#xA;&lt;p&gt;这样的问题是, 从 pod 被干掉到 nginx 更新之间有个时间差, 部分请求会挂掉, 解决方法可以给 pod 设置一个 preStop hook, sleep 几秒, 等 nginx ingress controller&#xA;更新完成.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8S: 剩下的问题</title>
      <link>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 30 Jun 2019 16:11:06 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程,&#xA;但需要持续关注.&lt;/p&gt;&#xA;&lt;h2 id=&#34;dns-timeout-and-conntrack&#34;&gt;DNS timeout and conntrack&lt;/h2&gt;&#xA;&lt;p&gt;看到有个关于 DNS 的issue: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/56903&#34;&gt;#56903&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候,&#xA;要通过 iptables 把　coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在&#xA;conntrack 内记录映射关系.&lt;/p&gt;&#xA;&lt;p&gt;这可能会带来两个问题:&lt;/p&gt;&#xA;&lt;h3 id=&#34;conntrack-table-被-udp-的-dns-查询填满&#34;&gt;conntrack table 被 udp 的 dns 查询填满&lt;/h3&gt;&#xA;&lt;p&gt;udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默认 30s(&lt;code&gt;net.netfilter.nf_conntrack_udp_timeout&lt;/code&gt;)&#xA;短时间内大量 udp 查询可能填满 conntrack, 导致丢包.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Centralized Logging on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</link>
      <pubDate>Sun, 26 May 2019 14:27:38 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</guid>
      <description>&lt;p&gt;搞定了监控, 下一步在 k8s 上要做的是中心化日志, 大体看了下, 感兴趣的有两个选择: ELK 套件, 或fluent-bit + fluentd.&lt;/p&gt;&#xA;&lt;p&gt;ELK 那套好处是, 可以把监控和日志一体化, filebeat 收集日志, metricbeat 收集 metrics, 统一存储在 ElasticSearch 里, 通过第三方项目&lt;a href=&#34;https://github.com/Yelp/elastalert&#34;&gt;elastalert&lt;/a&gt;&#xA;可以做报警，也能在 kibana 里集成界面. 坏处是 ElasticSearch 存储成本高, 吃资源. 我们对存储的日志使用需求基本就是 debug, 没有特别复杂的BI需求, 上一整套 ELK 还是太重了.&lt;/p&gt;&#xA;&lt;p&gt;选择 fluent-bit + fluentd 还有个好处是, 之前内部有套收集 metrics(用于统计 DAU, retention 之类指标) 的系统本来就是基于 fluentd 的, 用这套就不用改 metrics 那边的 ETL 了.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prometheus on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</link>
      <pubDate>Tue, 14 May 2019 13:52:02 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</guid>
      <description>&lt;h1 id=&#34;why-move-to-prometheus&#34;&gt;Why move to prometheus?&lt;/h1&gt;&#xA;&lt;p&gt;把生产环境迁移到 k8s 的第一步是要搞定监控, 目前线上监控用的是商业的 datadog, 在 container 环境下&#xA;datadog 监控还要按 container 数目收费, 单 host 只有 10 个的额度, 超过要加钱, 高密度部署下很不划算.&#xA;一个 server 跑 20 个以上 container 是很正常的事情, 单台 server 的监控费用立马翻倍.&lt;/p&gt;&#xA;&lt;p&gt;tracing 这块之前用的也是 datadog, 但太贵了,一直也想换开源实现, 索性监控报警也换了, 踩一把坑吧.&lt;/p&gt;&#xA;&lt;p&gt;vendor lock 总是不爽的&amp;hellip;&lt;/p&gt;&#xA;&lt;h1 id=&#34;metrics-in-k8s&#34;&gt;Metrics in k8s&lt;/h1&gt;&#xA;&lt;p&gt;先不提 prometheus, k8s 中 metrics 来源有那么几个:&lt;/p&gt;&#xA;&lt;h2 id=&#34;metrics-serever&#34;&gt;metrics-serever&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/metrics-server&#34;&gt;metrics-server&lt;/a&gt; (取代 heapster), 从 node&#xA;上 kubelet 的 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/stats/v1alpha1/types.go&#34;&gt;summary api&lt;/a&gt; 抓取数据(node/pod 的 cpu/memory 信息), kubectl top 和 kube-dashboard 的 metrics 数据来源就是它, horizontal pod&#xA;autoscaler 做 scale up/down 决策的数据来源也是它, metrics-server 只在内存里保留 node 和 pod 的最新值.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8s Volume Resize on EKS</title>
      <link>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</link>
      <pubDate>Fri, 12 Apr 2019 13:23:54 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</guid>
      <description>&lt;p&gt;从 k8s 1.8 开始支持 &lt;a href=&#34;https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/&#34;&gt;PersistentVolumeClaimResize&lt;/a&gt;. 但 api 是 alpha 状态, 默认不开启, eks launch&#xA;的时候版本是 1.10, 因为没法改 control plane, 所以没法直接在 k8s 内做 ebs 扩容. 后来升级到了&#xA;1.11, 这个 feature 默认被打开了, 尝试了下直接在 EKS 内做 ebs 的扩容.&lt;/p&gt;&#xA;&lt;p&gt;注意:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这个 feature 只能对通过 pvc 管理的 volume 做扩容, 如果直接挂的是 pv, 只能自己按传统的 ebs 扩容流程在 eks 之外做.&lt;/li&gt;&#xA;&lt;li&gt;用来创建 pvc 的 storageclass 上必须设置 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 为 true&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在 eks 上使用 pv/pvc,　对于需要 retain 的 volume, 我一般的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在 eks 之外手工创建 ebs volume.&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pv, 指向 ebs 的 volume id&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pvc, 指向 pv&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;示例 yaml:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---&#xA;kind: PersistentVolume&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  storageClassName: gp2&#xA;  persistentVolumeReclaimPolicy: Retain&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  capacity:&#xA;    storage: 5Gi&#xA;  awsElasticBlockStore:&#xA;    fsType: ext4&#xA;    volumeID: vol-xxxx   # create in aws manually&#xA;---&#xA;kind: PersistentVolumeClaim&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test-claim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  volumeName: test&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;假如现在要扩容到 10Gi, 流程是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>EkS 评测 part-3</title>
      <link>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</link>
      <pubDate>Wed, 26 Sep 2018 10:16:42 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</guid>
      <description>&lt;p&gt;这篇记录对 ingress 的测试.&lt;/p&gt;&#xA;&lt;p&gt;ingress 用来将外部流量导入　k8s 内的　service. 将 service 的类型设置为 LoadBalancer / NodePort 也可以将单个 service 暴露到公网, 但用 ingress 可以只使用一个公网入口,根据　host name 或　url path 来将请求分发到不同的 service.&lt;/p&gt;&#xA;&lt;p&gt;一般　k8s 内的资源都会由一个 controller 来负责它的状态管理, 都由 kube-controller-manager 负责，　但 ingress controller 不是它的一部分，需要是视情况自己选择合适的 ingress controller.&lt;/p&gt;&#xA;&lt;p&gt;在 eks 上我主要需要 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;ingress-nginx&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&#34;&gt;aws-alb-ingress-controller&lt;/a&gt;. 注意, nginx inc 还维护一个 &lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress&#34;&gt;kubernetes-ingress&lt;/a&gt;, 和官方那个不是一个东西， 没测试过.&lt;/p&gt;&#xA;&lt;p&gt;这里主要只测试了 ingress-nginx, 看了下内部实现, 数据的转发真扭曲&amp;hellip;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>eks 评测 part-2</title>
      <link>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</link>
      <pubDate>Fri, 21 Sep 2018 10:28:17 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</guid>
      <description>&lt;p&gt;上文测试了一下 EKS 和 cluster autoscaler, 本文记录对 persisten volume 的测试.&lt;/p&gt;&#xA;&lt;h1 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h1&gt;&#xA;&lt;p&gt;创建 gp2 类型的 storageclass, 并用 annotations 设置为默认 sc, dynamic volume provision 会用到:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kind: StorageClass&#xA;apiVersion: storage.k8s.io/v1&#xA;metadata:&#xA;    name: gp2&#xA;    annotations:&#xA;        storageclass.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&#xA;provisioner: kubernetes.io/aws-ebs&#xA;reclaimPolicy: Retain&#xA;parameters:&#xA;    type: gp2&#xA;    fsType: ext4&#xA;    encrypted: &amp;quot;true&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;因为 eks 是基于 1.10.3 的, volume expansion 还是 alpha 状态, 没法自动开启(没法改 api server 配置), 所以 storageclass 的 allowVolumeExpansion, 设置了也没用.&#xA;这里 &lt;code&gt;encrypted&lt;/code&gt; 的值必须是字符串, 否则会创建失败, 而且报错莫名其妙.&lt;/p&gt;&#xA;&lt;h2 id=&#34;创建-pod-的时候指定一个已存在的-ebs-volume&#34;&gt;创建 pod 的时候指定一个已存在的 ebs volume&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;    name: test&#xA;spec:&#xA;    volumes:&#xA;        - name: test&#xA;          awsElasticBlockStore:&#xA;              fsType: ext4&#xA;              volumeID: vol-03670d6294ccf29fd&#xA;    containers:&#xA;        - image: nginx&#xA;          name: nginx&#xA;          volumeMounts:&#xA;              - name: test&#xA;                mountPath: /mnt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;kubectl -it test -- /bin/bash&lt;/code&gt;  进去看一下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@test:/# df -h&#xA;Filesystem      Size  Used Avail Use% Mounted on&#xA;overlay          20G  2.2G   18G  11% /&#xA;tmpfs           3.9G     0  3.9G   0% /dev&#xA;tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup&#xA;/dev/xvdcz      976M  2.6M  907M   1% /mnt&#xA;/dev/xvda1       20G  2.2G   18G  11% /etc/hosts&#xA;shm              64M     0   64M   0% /dev/shm&#xA;tmpfs           3.9G   12K  3.9G   1% /run/secrets/kubernetes.io/serviceaccount&#xA;tmpfs           3.9G     0  3.9G   0% /sys/firmware&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;那块 volume 的确绑定在 &lt;code&gt;/mnt&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>EKS 评测</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>&lt;p&gt;EKS 正式 launch 还没有正经用过, 最近总算试了一把, 记录一点.&lt;/p&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;p&gt;AWS 官方的 Guide 只提供了一个 cloudformation template 来设置 worker node, 我喜欢用 terraform, 可以跟着这个文档尝试:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html 来设置完整的 eks cluster 和管理 worker node 的 autoscaling  group.&lt;/p&gt;&#xA;&lt;p&gt;设置完 EKS 后需要添加一条 ConfigMap:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: aws-auth&#xA;  namespace: kube-system&#xA;data:&#xA;  mapRoles: |&#xA;    - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole&#xA;      username: system:node:{{EC2PrivateDNSName}}&#xA;      groups:&#xA;        - system:bootstrappers&#xA;        - system:nodes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;这样 worker node 节点才能加入集群.&lt;/p&gt;&#xA;&lt;h2 id=&#34;网络&#34;&gt;网络&lt;/h2&gt;&#xA;&lt;p&gt;之前一直没有在 AWS 上尝试构建 k8s 的一个原因, 就是不喜欢 overlay 网络, 给系统带来了额外的复杂度和管理开销, VPC flowlog 看不到 pod 之间流量, 封包后 tcpdump 不好 debug 应用层流量.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
