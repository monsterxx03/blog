<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/aws/</link>
    <description>Recent content in AWS on Shining Moon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Sat, 18 Jan 2020 15:20:47 +0800</lastBuildDate>
    
	<atom:link href="https://blog.monsterxx03.com/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ç¼–å†™ postmortem</title>
      <link>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</link>
      <pubDate>Sat, 18 Jan 2020 15:20:47 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</guid>
      <description>æˆåŠŸçš„ç»éªŒæ€»æ˜¯å¸¦æœ‰ç‚¹è¿æ°”æˆä»½, å¤±è´¥åˆ™æ˜¯å¿…ç„¶çš„:). å·¥ä½œä¸­ï¼Œ çº¿ä¸Šç¯å¢ƒçš„é—®é¢˜åƒå¥‡ç™¾æ€ª, æœ‰çš„æ¥è‡ªè‡ªå·±ä»£ç  bug, æœ‰çš„æ˜¯é…ç½®é”™è¯¯, æœ‰æ—¶å€™æ˜¯ç¬¬ä¸‰æ–¹çš„ vendor æˆäº†çŒªé˜Ÿå‹. å¯¹äºä¸€äº›æ’æŸ¥è¿‡ç¨‹æ¯”è¾ƒå›°éš¾æˆ–å…·æœ‰ä»£è¡¨æ€§çš„é—®é¢˜, éœ€è¦è®°å½•ä¸‹æ¥, ä¸€èˆ¬æŠŠè¿™ä¸ªè¿‡ç¨‹å«åš postmortem(éªŒå°¸). è¿™ç¯‡å†™ä¸€ä¸‹è‡ªå·±åš postmortem çš„è¿‡ç¨‹, å¹¶è®°å½•ä¸€ä¸ªæœ€è¿‘å¤„ç†çš„æ•…éšœ. Postmortem process æˆ‘å¤§ä½“åˆ†ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†: ç”¨å°½é‡ç®€ç»ƒçš„è¯­å¥æè¿°æ¸…æ¥šåœ¨ä»€ä¹ˆæ—¶é—´å‘ç”Ÿäº†ä»€ä¹ˆ,è°å‚ä¸äº†é—®é¢˜çš„å¤„ç†(wh</description>
    </item>
    
    <item>
      <title>èŠèŠ AWS çš„è®¡è´¹æ¨¡å¼</title>
      <link>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Mon, 30 Dec 2019 12:08:47 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</guid>
      <description>ç½‘ä¸Šç»å¸¸æœ‰äººè¯Ÿç—… AWS çš„è®¡è´¹æ¨¡å¼å¤æ‚, å–œæ¬¢å›½å†…é‚£ç§æ‰“åŒ…å¼çš„å”®å–æ–¹å¼, è¿™ä¸ªå¯èƒ½å—é™äºæ¯ä¸ªå…¬å¸çš„è´¢åŠ¡æµç¨‹, é¢„ç®—åˆ¶å®šæ–¹å¼, åˆä¸åˆå›½æƒ…,æœ¬æ–‡ä¸è®¨è®º. ä»…ä»å¼€å‘è€…çš„è§’åº¦ä»‹ç»ä¸‹ AWS éƒ¨åˆ†å¸¸ç”¨ service çš„è®¡è´¹æ–¹å¼. PS: é‚£äº›ä¸ºäº†è¹­ä¸€å¹´ free plan ç„¶åæŠ±æ€¨ä»€ä¹ˆå·è·‘æµé‡, å·å·æ‰£è´¹çš„å¤§å“¥å°±çœçœå§, AWS æ ¹æœ¬ä¸æ˜¯ç»™ä¸ªäººç”¨çš„, è€è€å®å®ç”¨ lightsail å¾—äº†. EC2 EC2 çš„ä»·æ ¼æ˜¯æœ€å¤æ‚çš„, ä¸€å° EC2 instance çš„ä»·æ ¼ç»„æˆ: instance fee, å®é™…æ”¯ä»˜çš„æ˜¯ CPU+RAM çš„è´¹ç”¨ EBS fee, server çš„æ ¹åˆ†åŒºéƒ½æ˜¯ EBS volume, æŒ‰ EBS è®¡è´¹(31GB çš„ volume ä½¿</description>
    </item>
    
    <item>
      <title>K8s Volume Resize on EKS</title>
      <link>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</link>
      <pubDate>Fri, 12 Apr 2019 13:23:54 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</guid>
      <description>ä» k8s 1.8 å¼€å§‹æ”¯æŒ PersistentVolumeClaimResize. ä½† api æ˜¯ alpha çŠ¶æ€, é»˜è®¤ä¸å¼€å¯, eks launch çš„æ—¶å€™ç‰ˆæœ¬æ˜¯ 1.10, å› ä¸ºæ²¡æ³•æ”¹ control plane, æ‰€ä»¥æ²¡æ³•ç›´æ¥åœ¨ k8s å†…åš ebs æ‰©å®¹. åæ¥å‡çº§åˆ°äº† 1.11, è¿™ä¸ª feature é»˜è®¤è¢«æ‰“å¼€äº†, å°è¯•äº†ä¸‹ç›´æ¥åœ¨ EKS å†…åš ebs çš„æ‰©å®¹. æ³¨æ„: è¿™ä¸ª feature åªèƒ½å¯¹é€šè¿‡ pvc ç®¡ç†çš„ volume åšæ‰©å®¹, å¦‚æœç›´æ¥æŒ‚çš„æ˜¯ pv, åªèƒ½è‡ªå·±æŒ‰ä¼ ç»Ÿçš„ ebs æ‰©å®¹æµç¨‹åœ¨ eks ä¹‹å¤–åš. ç”¨æ¥åˆ›å»º pvc çš„ storageclass ä¸Šå¿…é¡»è®¾ç½® allowVolumeExpansion ä¸º true åœ¨ eks ä¸Šä½¿ç”¨ pv/pvc, å¯¹äºéœ€è¦ retain çš„ volume, æˆ‘ä¸€èˆ¬çš„æµç¨‹æ˜¯: åœ¨ eks ä¹‹å¤–æ‰‹å·¥åˆ›å»º ebs volume. åœ¨ eks ä¸­åˆ›å»º pv, æŒ‡å‘ ebs çš„ volume id åœ¨ eks ä¸­åˆ›å»º pvc, æŒ‡å‘ pv ç¤ºä¾‹ yaml:</description>
    </item>
    
    <item>
      <title>ç®¡ç†è´Ÿè½½</title>
      <link>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</link>
      <pubDate>Tue, 12 Feb 2019 13:08:29 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</guid>
      <description>æœ€è¿‘åœ¨çœ‹ google çš„ &amp;lt;The Site Reliablity Workbook&amp;gt;, å…¶ä¸­æœ‰ä¸€ç« æ˜¯&amp;quot;Manage load&amp;rdquo;, å†…å®¹è¿˜æŒºè¯¦ç»†çš„, ç»“åˆåœ¨ aws ä¸Šçš„ç»éªŒåšç‚¹ç¬”è®°. Load Balancing æµé‡çš„å…¥å£æ˜¯è´Ÿè½½å‡è¡¡, æœ€æœ€ç®€å•çš„åšæ³•æ˜¯åœ¨ DNS ä¸Šåš round robin, ä½†è¿™æ ·å¾ˆä¾èµ– client, ä¸åŒçš„ client å¯èƒ½ä¸å®Œå…¨éµå®ˆ DNS çš„ TTL, å½“åœ°çš„ ISP ä¹Ÿä¼šæœ‰ç¼“å­˜. google ç”¨ anycast æŠ€æœ¯åœ¨è‡ªå·±çš„ç½‘ç»œä¸­é€šè¿‡ BGP ç»™ä¸€ä¸ªåŸŸåå‘å¸ƒå¤šä¸ª endpoint, å…±äº«ä¸€ä¸ª vip(virtual ip), é€šè¿‡ BGP routing æ¥å°†ç”¨æˆ·çš„æ•°æ®åŒ…å‘é€åˆ°æœ€è¿‘çš„ frontend server, ä»¥æ­¤æ¥å‡å°‘ latency. ä½†åªä¾èµ– BGP ä¼šå¸¦æ¥ä¸¤ä¸ªé—®é¢˜: æŸä¸ªåœ°åŒºçš„ç”¨æˆ·è¿‡å¤šä¼šç»™æœ€è¿‘çš„ frontend server å¸¦æ¥è¿‡é«˜çš„è´Ÿ</description>
    </item>
    
    <item>
      <title>AWS Aurora DB</title>
      <link>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</link>
      <pubDate>Wed, 31 Oct 2018 15:23:45 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</guid>
      <description>æœ€è¿‘åœ¨æŠŠéƒ¨åˆ†ç”¨ RDS çš„ MySQL è¿ç§»åˆ° aurora ä¸Šå», è¯»äº†ä¸‹ aurora çš„ paper, é¡ºä¾¿å’Œ RDS çš„æ¶æ„åšäº›å¯¹æ¯”. Paper notes å­˜å‚¨è®¡ç®—åˆ†ç¦» redo log ä¸‹æ¨åˆ°å­˜å‚¨å±‚ å‰¯æœ¬: 6 å‰¯æœ¬ 3 AZ(2 per az), å¤±å»ä¸€ä¸ª AZ + 1 additoinal node ä¸ä¼šä¸¢æ•°æ®(å¯è¯»ä¸å¯å†™). å¤±å»ä¸€ä¸ª AZ (æˆ–ä»»æ„2 node) ä¸å½±å“æ•°æ®å†™å…¥. 10GB ä¸€ä¸ª segment, æ¯ä¸ª segment 6 å‰¯æœ¬ä¸€ä¸ª PG (protection group), ä¸€ AZ ä¸¤å‰¯æœ¬. åœ¨ 10Gbps çš„ç½‘ç»œä¸Š, ä¿®å¤ä¸€ä¸ª 10GB çš„segment éœ€è¦ 10s. MySQL ä¸€ä¸ªåº”ç”¨å±‚çš„å†™ä¼šåœ¨åº•å±‚äº§ç”Ÿå¾ˆå¤šé¢å¤–çš„å†™æ“ä½œï¼Œä¼šå¸¦æ¥å†™æ”¾å¤§é—®é¢˜: redo log ç”¨æ¥ crash recovery, binlog ä¼šä¸Šä¼  s3 ç”¨äº point in time restore. åœ¨ aurora é‡Œï¼Œåª</description>
    </item>
    
    <item>
      <title>åœ¨ redshift ä¸­è®¡ç®— p95 latency</title>
      <link>https://blog.monsterxx03.com/2018/10/12/%E5%9C%A8-redshift-%E4%B8%AD%E8%AE%A1%E7%AE%97-p95-latency/</link>
      <pubDate>Fri, 12 Oct 2018 14:49:13 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/10/12/%E5%9C%A8-redshift-%E4%B8%AD%E8%AE%A1%E7%AE%97-p95-latency/</guid>
      <description>p95 latency çš„å®šä¹‰: æŠŠä¸€æ®µæ—¶é—´çš„ latency æŒ‰ç…§ä»å°åˆ°å¤§æ’åº, ç æ‰æœ€é«˜çš„ %5, å‰©ä¸‹æœ€å¤§çš„å€¼å°±æ˜¯ p95 latency. p99, p90 åŒç†. p95 latency è¡¨ç¤ºè¯¥æ—¶é—´æ®µå†… 95% çš„ reqeust éƒ½æ¯”è¿™ä¸ªå€¼å¿«. ä¸€èˆ¬æˆ‘ç›´æ¥çœ‹ CloudWatch, å’Œ datadog ç®—å¥½çš„ p95 å€¼. è¿™æ¬¡çœ‹çœ‹æ€ä¹ˆä» access log é‡Œç›´æ¥è®¡ç®— p95 latency. å‡è®¾åœ¨ redshift ä¸­æœ‰ä¸€å¼ è¡¨å­˜å‚¨äº†åº”ç”¨çš„ access log, ç»“æ„å¦‚ä¸‹: CREATE TABLE access_log ( url string, time string, resp_time real ); url time resp_time /test1 2018-10-11T00:10:00.418480Z 0.123 /test2 2018-10-11T00:12:00.512340Z 0.321 è¦ç®— p95 å¾ˆç®€å•, æŠŠ log æŒ‰åˆ†é’Ÿæ•°åˆ†ç»„, ç”¨ percentile_cont åœ¨ç»„å†…æŒ‰ resp_time æ’åºè®¡ç®— å°±èƒ½å¾—åˆ°: select date_trunc(&#39;minute&#39;, time::timestamp) as ts, percentile_cont(0.95) within group(order by resp_time) as p95 from access_log group by 1 order by 1; å¾—åˆ°: ts | p95 ---------------------+------------------- 2018-10-11 00:00:00 | 0.71904999999995 2018-10-11 00:01:00</description>
    </item>
    
    <item>
      <title>EkS è¯„æµ‹ part-3</title>
      <link>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</link>
      <pubDate>Wed, 26 Sep 2018 10:16:42 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</guid>
      <description>è¿™ç¯‡è®°å½•å¯¹ ingress çš„æµ‹è¯•. ingress ç”¨æ¥å°†å¤–éƒ¨æµé‡å¯¼å…¥ k8s å†…çš„ service. å°† service çš„ç±»å‹è®¾ç½®ä¸º LoadBalancer / NodePort ä¹Ÿå¯ä»¥å°†å•ä¸ª service æš´éœ²åˆ°å…¬ç½‘, ä½†ç”¨ ingress å¯ä»¥åªä½¿ç”¨ä¸€ä¸ªå…¬ç½‘å…¥å£,æ ¹æ® host name æˆ– url path æ¥å°†è¯·æ±‚åˆ†å‘åˆ°ä¸åŒçš„ service. ä¸€èˆ¬ k8s å†…çš„èµ„æºéƒ½ä¼šç”±ä¸€ä¸ª controller æ¥è´Ÿè´£å®ƒçš„çŠ¶æ€ç®¡ç†, éƒ½ç”± kube-controller-manager è´Ÿè´£ï¼Œ ä½† ingress controller ä¸æ˜¯å®ƒçš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ˜¯è§†æƒ…å†µè‡ªå·±é€‰æ‹©åˆé€‚çš„ ingress controller. åœ¨ eks ä¸Šæˆ‘ä¸»è¦éœ€è¦ ingress-nginx å’Œ aws-alb-ingress-controller. æ³¨æ„, nginx inc è¿˜ç»´æŠ¤ä¸€ä¸ª kubernetes-ingress, å’Œå®˜æ–¹é‚£ä¸ªä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œ æ²¡æµ‹è¯•è¿‡. è¿™é‡Œä¸»è¦åªæµ‹è¯•äº† ingress-nginx, çœ‹äº†ä¸‹å†…éƒ¨å®ç°, æ•°æ®çš„è½¬å‘çœŸæ‰­æ›²</description>
    </item>
    
    <item>
      <title>eks è¯„æµ‹ part-2</title>
      <link>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</link>
      <pubDate>Fri, 21 Sep 2018 10:28:17 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</guid>
      <description>ä¸Šæ–‡æµ‹è¯•äº†ä¸€ä¸‹ EKS å’Œ cluster autoscaler, æœ¬æ–‡è®°å½•å¯¹ persisten volume çš„æµ‹è¯•. PersistentVolume åˆ›å»º gp2 ç±»å‹çš„ storageclass, å¹¶ç”¨ annotations è®¾ç½®ä¸ºé»˜è®¤ sc, dynamic volume provision ä¼šç”¨åˆ°: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 annotations: storageclass.kubernetes.io/is-default-class: &amp;quot;true&amp;quot; provisioner: kubernetes.io/aws-ebs reclaimPolicy: Retain parameters: type: gp2 fsType: ext4 encrypted: &amp;quot;true&amp;quot; å› ä¸º eks æ˜¯åŸºäº 1.10.3 çš„, volume expansion è¿˜æ˜¯ alpha çŠ¶æ€, æ²¡æ³•è‡ªåŠ¨å¼€å¯(æ²¡æ³•æ”¹ api server é…ç½®), æ‰€ä»¥ storageclass çš„ allowVolumeExpansion, è®¾ç½®äº†ä¹Ÿæ²¡ç”¨. è¿™é‡Œ encrypted çš„å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸², å¦åˆ™ä¼šåˆ›å»ºå¤±è´¥, è€Œä¸”æŠ¥é”™è«åå…¶å¦™. åˆ›å»º pod çš„æ—¶å€™æŒ‡å®šä¸€ä¸ªå·²å­˜åœ¨çš„ ebs volume apiVersion: v1 kind: Pod metadata: name: test spec: volumes: - name: test awsElasticBlockStore: fsType: ext4 volumeID: vol-03670d6294ccf29fd containers: - image: nginx name: nginx volumeMounts: - name: test mountPath: /mnt kubectl -it test -- /bin/bash è¿›å»çœ‹ä¸€ä¸‹: root@test:/# df -h</description>
    </item>
    
    <item>
      <title>EKS è¯„æµ‹</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>EKS æ­£å¼ launch è¿˜æ²¡æœ‰æ­£ç»ç”¨è¿‡, æœ€è¿‘æ€»ç®—è¯•äº†ä¸€æŠŠ, è®°å½•ä¸€ç‚¹. Setup AWS å®˜æ–¹çš„ Guide åªæä¾›äº†ä¸€ä¸ª cloudformation template æ¥è®¾ç½® worker node, æˆ‘å–œæ¬¢ç”¨ terraform, å¯ä»¥è·Ÿç€è¿™ä¸ªæ–‡æ¡£å°è¯•:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html æ¥è®¾ç½®å®Œæ•´çš„ eks cluster å’Œç®¡ç† worker node çš„ autoscaling group. è®¾ç½®å®Œ EKS åéœ€è¦æ·»åŠ ä¸€æ¡ ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes è¿™æ · worker node èŠ‚ç‚¹æ‰èƒ½åŠ å…¥é›†ç¾¤. ç½‘</description>
    </item>
    
    <item>
      <title>Use SNS &amp; SQS to build Pub/Sub System</title>
      <link>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</link>
      <pubDate>Wed, 23 May 2018 18:05:28 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</guid>
      <description>Recently, we build pub/sub system based on AWS&amp;rsquo;s SNS &amp;amp; SQS service, take some notes.
Originally, we have an pub/sub system based on redis(use BLPOP to listen to a redis list). It&amp;rsquo;s really simple, and mainly for cross app operations. Now we have needs to enhance it to support more complex pubsub logic, eg: topic based distribution. It don&amp;rsquo;t support redelivery as well, if subscribers failed to process the message, message will be dropped.
There&amp;rsquo;re three obvious choices in my mind:
 kafka AMQP based system (rabbitmq,activemq &amp;hellip;) SNS + SQS  My demands for this system are:
 Support message persistence. Support topic based message distribution. Easy to manage.  The data volume won&amp;rsquo;t be very large, so performance and throughput won&amp;rsquo;t be critical concerns.
I choose SNS + SQS, main concerns are from operation side:
 kafka need zookeeper to support cluster. rabbitmq need extra configuration for HA, and AMQP model is relatively complex for programming.  So my decision is:
 application publish message to SNS topic Setup multi SQS queues to subscribe SNS topic Let different application processes to subscribe to different queues to finish its logic.  SQS and SNS is very simple, not too much to say, just some notes:</description>
    </item>
    
    <item>
      <title>AWS çš„ K8S CNI Plugin</title>
      <link>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</link>
      <pubDate>Mon, 09 Apr 2018 15:28:38 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</guid>
      <description>EKS è¿˜æ²¡æœ‰ launch, ä½† AWS å…ˆå¼€æºäº†è‡ªå·±çš„ CNI æ’ä»¶, ç®€å•çœ‹äº†ä¸‹, è¯´è¯´å®ƒçš„å®ç°å’Œå…¶ä»– K8S ç½‘ç»œæ–¹æ¡ˆçš„å·®åˆ«. K8S é›†ç¾¤å¯¹ç½‘ç»œæœ‰å‡ ä¸ªåŸºæœ¬è¦æ±‚: container ä¹‹é—´ç½‘ç»œå¿…é¡»å¯è¾¾ï¼Œä¸”ä¸é€šè¿‡ NAT æ‰€æœ‰ node å¿…é¡»å¯ä»¥å’Œæ‰€æœ‰ container é€šä¿¡, ä¸”ä¸é€šè¿‡ NAT container è‡ªå·±çœ‹åˆ°çš„ IP, å¿…é¡»å’Œå…¶ä»– container çœ‹åˆ°çš„å®ƒçš„ ip ç›¸åŒ. Flannel in VPC flannel æ˜¯ K8S çš„ä¸€ä¸ª CNI æ’ä»¶, åœ¨ VPC é‡Œä½¿ç”¨ flannel çš„è¯, æœ‰å‡ ä¸ªé€‰æ‹©: é€šè¿‡ VXLAN/UDP è¿›è¡Œå°åŒ…, å°åŒ…å½±å“ç½‘ç»œæ€§èƒ½, è€Œä¸”ä¸å¥½ debug ç”¨ aws vpc backend, è¿™ç§æ–¹å¼ä¼šæŠŠæ¯å°ä¸»æœºçš„ docker ç½‘æ®µæ·»åŠ è¿› vpc routing table, ä½†é»˜è®¤ routing table é‡Œåªèƒ½æœ‰50æ¡è§„åˆ™</description>
    </item>
    
    <item>
      <title>AWS lambda çš„ä¸€äº›åº”ç”¨åœºæ™¯</title>
      <link>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</link>
      <pubDate>Fri, 23 Mar 2018 17:40:54 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</guid>
      <description>è¿™å‡ å¹´å¹ serverless çš„æ¯”è¾ƒå¤š, åœ¨å…¬å¸å†…éƒ¨ä¹Ÿç”¨ lambda , è®°å½•ä¸€ä¸‹, è¿™ä¸œè¥¿æŒºæœ‰ç”¨, ä½†è¿œä¸åˆ°ä¸‡èƒ½, åœºæ™¯æ¯”è¾ƒæœ‰é™. lambda çš„ä»£ç çš„éƒ¨ç½²ç”¨çš„ serverless æ¡†æ¶, æœ¬èº«æ”¯æŒå¤šç§ cloud å¹³å°, æˆ‘ä»¬å°±åªåœ¨ aws lambda ä¸Šäº†. æˆ‘åŸºæœ¬ä¸Šå°±æŠŠ lambda å½“æˆ trigger å’Œ web hook ç”¨. å’Œ auto scaling group ä¸€èµ·ç”¨ çº¿ä¸Šæ‰€æœ‰åˆ†ç»„çš„æœºå™¨éƒ½æ˜¯ç”¨ auto scaling group ç®¡ç†çš„, åªä¸è¿‡ stateless çš„ server å¼€äº†è‡ªåŠ¨ä¼¸ç¼©, å¸¦çŠ¶æ€çš„ (ElasticSearch cluster, redis cache cluster) åªç”¨æ¥ç»´æŠ¤å›ºå®š size. åœ¨å¾€ä¸€ä¸ª group é‡ŒåŠ  server çš„æ—¶å€™, è¦åšçš„äº‹æƒ…æŒºå¤šçš„, ç»™æ–° server æ·»åŠ ç»„å†…ç¼–å· tag, æ·»åŠ å†…ç½‘åŸŸå, provision, éƒ¨ç½²æœ€æ–°ä»£ç . è¿™äº›äº‹éƒ½ç”¨</description>
    </item>
    
    <item>
      <title>Access sensitive variables on AWS lambda</title>
      <link>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</link>
      <pubDate>Wed, 28 Feb 2018 21:45:23 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</guid>
      <description>AWS lambda is convenient to run simple serverless application, but how to access sensitive data in code? like password,token&amp;hellip;
Usually, we inject secrets as environment variables, but they&amp;rsquo;re still visable on lambda console. I don&amp;rsquo;t use it in aws lambda.
The better way is use aws parameter store as configuration center. It can work with KMS to encrypt your data.
Code example:
client = boto3.client(&#39;ssm&#39;) resp = client.get_parameter( Name=&#39;/redshift/admin/password&#39;, WithDecryption=True ) resp: { &amp;quot;Parameter&amp;quot;: { &amp;quot;Name&amp;quot;: &amp;quot;/redshift/admin/password&amp;quot;, &amp;quot;Type&amp;quot;: &amp;quot;SecureString&amp;quot;, &amp;quot;Value&amp;quot;: &amp;quot;password value&amp;quot;, &amp;quot;Version&amp;quot;: 1 } }  Things you need to do to make it work:
 Create a new KMS key Use new created KMS key to encrypt your data in parameter store. Set a execution role for your lambda function. In the KMS key&amp;rsquo;s setting page, add the lambda execution role to the list which can read this KMS key.  Then your lambda code can access encrypted data at runtime, and you needn&amp;rsquo;t set aws access_key/secret_key, lambda execution role enable access to data in parameter store.
BTW, parameter store support hierarchy(at most 15 levels), splitted by /. You can retrive data under same level in one call, deltails can be found in doc, eg: http://boto3.readthedocs.io/en/latest/reference/services/ssm.html#SSM.Client.get_parameters_by_path</description>
    </item>
    
    <item>
      <title>Get Real Client Ip on AWS</title>
      <link>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</link>
      <pubDate>Thu, 01 Feb 2018 15:20:37 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</guid>
      <description>If you run a webserver on AWS, get real client ip will be tricky if you didn&amp;rsquo;t configure server right and write code correctly.
Things related to client real ip:
 CloudFront (cdn) ALB (loadbalancer) nginx (on ec2) webserver (maybe a python flask application).  Request sequence diagram will be like following:
User&amp;rsquo;s real client ip is forwarded by front proxies one by one in head X-Forwarded-For.
For CloudFront:
 If user&amp;rsquo;s req header don&amp;rsquo;t have X-Forwarded-For, it will set user&amp;rsquo;s ip(from tcp connection) in X-Forwarded-For If user&amp;rsquo;s req already have X-Forwarded-For, it will append user&amp;rsquo;s ip(from tcp connection) to the end of X-Forwarded-For  For ALB, rule is same as CloudFront, so the X-Forwarded-For header pass to nginx will be the value received from CloudFront + CloudFront&amp;rsquo;s ip.
For nginx, things will be tricky depends on your config.
Things maybe involved in nginx:
 real ip module proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  If you didn&amp;rsquo;t use real ip module, you need to pass X-Forwarded-For head explictly.
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; will append ALB&amp;rsquo;s ip to the end of X-Forwarded-For header received from ALB.
So X-Forwarded-For header your webserver received will be user ip,cloudfront ip, alb ip
Or you can use real ip module to trust the value passed from ALB.</description>
    </item>
    
    <item>
      <title>DynamoDB</title>
      <link>https://blog.monsterxx03.com/2017/12/15/dynamodb/</link>
      <pubDate>Fri, 15 Dec 2017 22:24:36 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/12/15/dynamodb/</guid>
      <description>DynamoDB æ˜¯ AWS çš„æ‰˜ç®¡ NoSQL æ•°æ®åº“ï¼Œå¯ä»¥å½“ä½œç®€å•çš„ KV æ•°æ®åº“ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ–‡æ¡£æ•°æ®åº“ä½¿ç”¨. Data model ç»„ç»‡æ•°æ®çš„å•ä½æ˜¯ table, æ¯å¼  table å¿…é¡»è®¾ç½® primary key, å¯ä»¥è®¾ç½®å¯é€‰çš„ sort key æ¥åšç´¢å¼•. æ¯æ¡æ•°æ®è®°ä½œä¸€ä¸ª item, æ¯ä¸ª item å«æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª attribute, å…¶ä¸­å¿…é¡»åŒ…æ‹¬ primary key. attribute å¯¹åº”çš„ value æ”¯æŒä»¥ä¸‹å‡ ç§ç±»å‹: Number, ç”±äº DynamoDB çš„ä¼ è¾“åè®®æ˜¯ http + json, ä¸ºäº†è·¨è¯­è¨€çš„å…¼å®¹æ€§, number ä¸€å¾‹ä¼šè¢«è½¬æˆ string ä¼ è¾“. Binary, ç”¨æ¥è¡¨ç¤ºä»»æ„çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œä¼šç”¨ base64 encode åä¼ è¾“. Boolean, true or false Null Document ç±»å‹åŒ…å« List å’Œ Map, å¯ä»¥äº’ç›¸åµŒå¥—. List, ä¸ªæ•°æ— é™åˆ¶, æ€»å¤§å°</description>
    </item>
    
    <item>
      <title>AWS DMS notes</title>
      <link>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</link>
      <pubDate>Sat, 14 Oct 2017 22:33:36 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</guid>
      <description>AWS&amp;rsquo;s DMS (Data migration service) can be used to do incremental ETL between databases. I use it to load data from RDS (MySQL) to Redshift.
It works, but have some concerns. Take some notes when doing this project.
Prerequisites Source RDS must:
 Enable automatic backups Increase binlog remain time, call mysql.rds_set_configuration(&#39;binlog retention hours&#39;, 24); Set binlog_format to ROW. Privileges on source RDS: REPLICATION CLIENT , REPLICATION SLAVE , SELECT on replication target tables  DDL on source table Redshift has some limits on change columns:
 New column only must be added in the end Can&amp;rsquo;t rename columns  So for DDL on source MySQL, you can&amp;rsquo;t add columns at non end postition, otherwise data in target table will corrupt. I disabled ddl changes target db:
 &amp;quot;ChangeProcessingDdlHandlingPolicy&amp;quot;:{ &amp;quot;HandleSourceTableDropped&amp;quot;:false, &amp;quot;HandleSourceTableTruncated&amp;quot;:false, &amp;quot;HandleSourceTableAltered&amp;quot;:false },  If source table schema changed, I just drop and reload target table on console.
Control write speed on Redshift Since Redshift is an OLAP database, write operation is slow and concurrency is low, streaming data directly will have big impact on it.
And we have may analysis jobs running on redshift all the time, directly streaming will lock target table and make my analysis jobs timeout.</description>
    </item>
    
    <item>
      <title>Get all invalid PTR record on  Route53</title>
      <link>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</link>
      <pubDate>Fri, 29 Sep 2017 08:55:18 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</guid>
      <description>I use autoscaling group to manage stateless servers. Servers go up and down every day.
Once server is up, I will add a PTR record for itâ€™s internal ip. But when itâ€™s down, I didnâ€™t cleanup the PTR record. As times fly, a lot of invalid PTR records left in Route53.
To cleanup those PTR records realtime, you can write a lambda function, use server termination event as trigger. But how to cleanup the old records at once?
Straightforward way is write a script to call AWS API to get a PTR list, get ip from record, test whether the ip is live, if not, delete it.
Since use awscli to delete a Route53 record is very troublesome (involve json format), youâ€™d better write a python script to delete them. I just demo some ideas to collect those records via shell.
You can do it in a single line, but make things clear and easy to debug, I split it into several steps.
Get PTR record list aws route53 list-resource-record-sets --hosted-zone-id xxxxx --query &amp;quot;ResourceRecordSets[?Type==&#39;PTR&#39;].Name&amp;quot; | grep -Po &#39;&amp;quot;(.+?)&amp;quot;&#39; | tr -d \&amp;quot; &amp;gt; ptr.txt  ptr.txt will contain lines like:
1.0.0.10.in-addr.arpa. 2.0.0.10.in-addr.arpa.  Get ip list from PTR records cat ptr.</description>
    </item>
    
    <item>
      <title>Build private static website on S3</title>
      <link>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</link>
      <pubDate>Sat, 19 Aug 2017 07:28:16 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</guid>
      <description>Build static website on S3 is very easy, but by default, it can be accessed by open internet.It will be super helpful if we can build website only available in VPC. Then we can use it to host internal deb repo, doc siteâ€¦
Steps are very easy, you only need VPC endpoints and S3 bucket policy.
AWS api is open to internet, if you need to access S3 in VPC, your requests will pass through VPCâ€™s internet gateway or NAT gateway. With VPC endpoints(can be found in VPC console), your requests to S3 will go through AWSâ€™s internal network. Currently, VPC endpoints only support S3, support for dynamodb is in test.
To restrict S3 bucket only available in your VPC, need to set bucket policy (to host static website, enable static website support first). At first, I didnâ€™t check doc, try to restrict access by my VPC ip cidr, but it didnâ€™t work, I need to restrict by VPC endpoint id:
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Id&amp;quot;: &amp;quot;Policy1415115909152&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Sid&amp;quot;: &amp;quot;Access-to-specific-VPCE-only&amp;quot;, &amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;Action&amp;quot;: &amp;quot;s3:GetObject&amp;quot;, &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Resource&amp;quot;: [&amp;quot;arn:aws:s3:::my_secure_bucket&amp;quot;, &amp;quot;arn:aws:s3:::my_secure_bucket/*&amp;quot;], &amp;quot;Condition&amp;quot;: { &amp;quot;StringEquals&amp;quot;: { &amp;quot;aws:sourceVpce&amp;quot;: &amp;quot;vpce-1a2b3c4d&amp;quot; } } } ] }  BTW, if you can config bucket policy restrict on VPC directly, with VPC endpoint you can limit to subnets.</description>
    </item>
    
    <item>
      <title>Use redshift spectrum to do query on s3</title>
      <link>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</link>
      <pubDate>Fri, 21 Jul 2017 03:10:58 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</guid>
      <description>ä½¿ç”¨ redshift spectrum æŸ¥è¯¢ S3 æ•°æ® é€šå¸¸ä½¿ç”¨ redshift åšæ•°æ®ä»“åº“çš„æ—¶å€™è¦åšå¤§é‡çš„ ETL å·¥ä½œï¼Œä¸€èˆ¬æµç¨‹æ˜¯æŠŠå„ç§æ¥æºçš„æ•°æ®æ£é¼“æ£é¼“ä¸¢åˆ° S3 ä¸Šå»ï¼Œå†ä» S3 å€’è…¾è¿› redshift. å¦‚æœä½ æœ‰å¤§é‡çš„å†å²æ•°æ®è¦å¯¼è¿› redshiftï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ä¼šå¾ˆç—›è‹¦ï¼Œredshift å¯¹ä¸€æ¬¡å€’å…¥å¤§é‡æ•°æ®å¹¶ä¸å‹å¥½ï¼Œä½ è¦åˆ†æ‰¹æ¥åšã€‚ ä»Šå¹´4æœˆçš„æ—¶å€™ï¼Œ redshift å‘å¸ƒäº†ä¸€ä¸ªæ–°åŠŸèƒ½ spectrum, å¯ä»¥ä» redshift é‡Œç›´æ¥æŸ¥è¯¢ s3 ä¸Šçš„ç»“æ„åŒ–æ•°æ®ã€‚æœ€è¿‘æŠŠéƒ¨åˆ†æ•°æ®ä»“åº“ç›´æ¥è¿ç§»åˆ°äº† spectrum, æ­£å¥½æ¥è®²è®²ã€‚ åŠ¨æœº Glow çš„æ•°æ®ä»“åº“å»ºåœ¨ redshift ä¸Šï¼Œ åˆåˆ†æˆäº†ä¸¤ä¸ª</description>
    </item>
    
    <item>
      <title>Infrastructure as Code</title>
      <link>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</link>
      <pubDate>Fri, 21 Apr 2017 16:25:07 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</guid>
      <description>Create virtual resource on AWS is very convenient, but how to manage them will be a problem when your size grow.
You will come to:
 How to explain the detail online settings for your colleagues (like: how our prod vpc is setup?whatâ€™s the DHCP option set?), navigate around AWS console is okay, but not convenient. Who did what to which resource at when? AWS have a service called Config, can be used to track this change, but if you want to make things as clear as viewing git log, still a lot of works to do.  Ideally, we should manage AWS resources like code, all changes kept in VCS, so called Infrastructure as Code.
Iâ€™ve tried three ways to do it:
 ansible CloudFormation terraform  In this article, I&amp;rsquo;ll compare them, however, the conclusion is to use terraform ğŸ™‚
Ansible Provision tools, like ansible/chef/puppet, all can be used to create aws resources, but they have some common problems:
 Hard to track changes after bootstrap. No confident what it will do to existing resources.  For example, I define a security group in ansibble:
ec2_group: name: &amp;quot;web&amp;quot; description: &amp;quot;security group in web&amp;quot; vpc_id: &amp;quot;vpc-xxx&amp;quot; region: &amp;quot;us-east-1&amp;quot; rules: - proto: tcp from_port: 80 to_port: 80 cidr_ip: 0.</description>
    </item>
    
    <item>
      <title>Migrate to encrypted RDS</title>
      <link>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</link>
      <pubDate>Fri, 28 Oct 2016 16:17:30 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</guid>
      <description>&lt;p&gt;æœ€è¿‘å…¬å¸åœ¨åš HIPAA Compliance ç›¸å…³çš„äº‹æƒ…ï¼Œå…¶ä¸­è¦æ±‚ä¹‹ä¸€æ˜¯æ‰€æœ‰dbéœ€è¦å¼€å¯encryption.&lt;/p&gt;
&lt;p&gt;æ¯”è¾ƒéº»çƒ¦çš„æ˜¯rds çš„encryption åªèƒ½åœ¨åˆ›å»ºçš„æ—¶å€™è®¾å®šï¼Œæ— æ³•ä¹‹åä¿®æ”¹, æ‰€ä»¥å¿…é¡»å¯¹çº¿ä¸Šçš„db åšä¸€æ¬¡ migration.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Redshift as data warehouse</title>
      <link>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</link>
      <pubDate>Sat, 16 Jul 2016 16:11:39 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</guid>
      <description>&lt;p&gt;Glow çš„ server infrastructure å…¨éƒ¨æ­å»ºåœ¨ AWS ä¸Šï¼Œä¸€èˆ¬è¦é€‰æ‹©ä¸€äº›åŸºç¡€æœåŠ¡çš„æ—¶å€™ï¼Œæ€»æ˜¯å…ˆçœ‹ AWS, åªè¦åŠŸèƒ½å’Œæˆæœ¬ç¬¦åˆè¦æ±‚ï¼Œä¸ä¼šç‰¹æ„é€‰æ‹©å¼€æºæ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æ•°æ®ä»“åº“æˆ‘ä»¬é€‰æ‹©äº† AWS çš„ Redshift.&lt;/p&gt;
&lt;p&gt;åœ¨ä¸€å¹´å¤šçš„ä½¿ç”¨è¿‡ç¨‹ä¸­ Redshift çš„æ€§èƒ½å’Œç¨³å®šæ€§éƒ½ä¸é”™, å½“ç„¶ä¹Ÿæœ‰ä¸€äº›å‘, è¿™é‡Œæ•´ç†ä¸‹åœ¨ä½¿ç”¨ redshift çš„è¿‡ç¨‹ä¸­çš„ä¸€äº›ç»éªŒå’Œé‡åˆ°çš„å‘.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>