<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/aws/</link>
    <description>Recent content in Aws on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Thu, 01 Apr 2021 14:40:30 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rolling Upgrade Worker Nodes in EKS</title>
      <link>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</link>
      <pubDate>Thu, 01 Apr 2021 14:40:30 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</guid>
      <description>&lt;p&gt;EKS control plane 的升级是比较简单的, 直接在 aws console 上点下就可以了, 但 worker node 是自己用 asg(autoscaling group) 管理的, 升级 worker node 又不想影响业务是有讲究的.&lt;/p&gt;&#xA;&lt;p&gt;跑在 EKS 里, 且希望不被中断 traffic 的有:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;stateless 的 api server, queue consumer&lt;/li&gt;&#xA;&lt;li&gt;被 redis sentinel 监控着的 redis master/slave&lt;/li&gt;&#xA;&lt;li&gt;用于 cache 的 redis cluster&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;写了个内部工具, 把下面的流程全部自动化了. 这样升级 eks 版本, 需要更换 worker node 时候就轻松多了. 因为这个工具对部署情况做了很多假设和限制, 开源的价值不是很大.&lt;/p&gt;&#xA;&lt;h2 id=&#34;stateless-application&#34;&gt;Stateless application&lt;/h2&gt;&#xA;&lt;p&gt;stateless 的应用全部用 deployment 部署.&lt;/p&gt;&#xA;&lt;p&gt;一般建议的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;修改 asg 的 launch configuration, 指向新版本的 ami&lt;/li&gt;&#xA;&lt;li&gt;把所有老的 worker node 用 kubectl cordon 标记成 unschedulable&lt;/li&gt;&#xA;&lt;li&gt;关闭 cluster-autoscaler&lt;/li&gt;&#xA;&lt;li&gt;修改 asg 的 desired count, 让 asg 用新 ami 启动新的 worker node&lt;/li&gt;&#xA;&lt;li&gt;用 kubectl drain 把老 worker node 上的 pod evict 掉, 让它们 schedule 到新的 worker node 上.&lt;/li&gt;&#xA;&lt;li&gt;重新开启 cluster autoscaler, 等它把老的闲置 worker node 关闭.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这里有些问题:&lt;/p&gt;</description>
    </item>
    <item>
      <title>解决 k8s 1.16 apiVersion deprecation 造成的 helm revision 冲突</title>
      <link>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</link>
      <pubDate>Tue, 16 Jun 2020 16:02:58 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</guid>
      <description>&lt;p&gt;最近开始把线上的 k8s 从 1.15 升级到 1.16, 1.16 里有一些 api verison 被彻底废弃, 需要迁移到新的 api version, 具体有: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;有两个问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;集群中使用的一些第三方 controller(nginx-ingress, external-dns-controller&amp;hellip;), 调用的 apiVersion 需要升级.&lt;/li&gt;&#xA;&lt;li&gt;已存在集群中的 objects(Deployment/ReplicaSet&amp;hellip;), 是否需要处理, eg: Deployment: extensions/v1beta1 -&amp;gt; apps/v1.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;第一个问题好解决, 升级一下对应的 image 版本就行了, 只要还在维护的 controller, 都已经升级到支持 1.16. 自己写的工具链也排查下是否有还在使用老版本 api 的, 因为我用的是 aws eks, 开下 &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html&#34;&gt;control-plane-logs&lt;/a&gt; 里的 audit log, 可以看还有什么在调用老的 api.&lt;/p&gt;&#xA;&lt;p&gt;第二个问题是不需要改, 已存在的 objects 无法修改 apiVersion, 集群升级到 1.16 后， 从新的 apiVersion 里能 pull 到之前的数据, apiVersion 字段自动就升级了.&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 eks 中正确设置 IAM 权限</title>
      <link>https://blog.monsterxx03.com/2020/04/16/%E5%9C%A8-eks-%E4%B8%AD%E6%AD%A3%E7%A1%AE%E8%AE%BE%E7%BD%AE-iam-%E6%9D%83%E9%99%90/</link>
      <pubDate>Thu, 16 Apr 2020 11:03:39 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/04/16/%E5%9C%A8-eks-%E4%B8%AD%E6%AD%A3%E7%A1%AE%E8%AE%BE%E7%BD%AE-iam-%E6%9D%83%E9%99%90/</guid>
      <description>&lt;p&gt;在代码中调用 aws api 的时候常用两种方法:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接传入 aws accessKey/secretKey&lt;/li&gt;&#xA;&lt;li&gt;使用 instance profile&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;前者一般是创建一个 IAM 用户, 绑定对应权限, 生成 keypair, 在 k8s 环境里, 把 keypair 放在 Secrets 里, 或通过环境变量注入. 好处是可以每个应用单独设置,&#xA;但需要自己管理 keypair.&lt;/p&gt;&#xA;&lt;p&gt;后者创建一个 IAM role, 绑定对应权限, 创建 ec2 的时候选择对应的 role. 跑在该 ec2 instance 上的程序自动能拿到对应的 IAM 权限. 好处是不必自己管理 keypair,&#xA;缺点是跑在同一 server 上的程序权限都一样.&lt;/p&gt;&#xA;&lt;p&gt;eks 1.14 里有个两全齐美的办法: serviceaccount role: &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html&#34;&gt;https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;可以把 IAM role 绑定在 pod 使用的 ServiceAccount 上, 既避免了管理 keypair 的麻烦, 也可以 per 应用得设置权限.&lt;/p&gt;</description>
    </item>
    <item>
      <title>用 AWS Personalize 做推荐系统</title>
      <link>https://blog.monsterxx03.com/2020/02/18/%E7%94%A8-aws-personalize-%E5%81%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Tue, 18 Feb 2020 14:38:56 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/02/18/%E7%94%A8-aws-personalize-%E5%81%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</guid>
      <description>&lt;p&gt;这几天测试了下 aws 的 personalize service, 看看能不能替换掉产品里现有的一些推荐逻辑.&lt;/p&gt;&#xA;&lt;p&gt;大致的流程:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;导入数据&lt;/li&gt;&#xA;&lt;li&gt;选择 recipe 进行 training, 得到一个 solution version&lt;/li&gt;&#xA;&lt;li&gt;选择最佳 solution version 创建 compaign&lt;/li&gt;&#xA;&lt;li&gt;调用 api, 根据 compaign 得到 recommendations&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;一些 iam 权限相关的设置就不写了, 具体看文档吧, 这里只记录下主要步骤.&lt;/p&gt;&#xA;&lt;h2 id=&#34;导入数据&#34;&gt;导入数据&lt;/h2&gt;&#xA;&lt;p&gt;首先需要准备用来 training 的数据, 分成三种数据集:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;User&lt;/li&gt;&#xA;&lt;li&gt;Item&lt;/li&gt;&#xA;&lt;li&gt;User-Item interaction&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;其中 User-Item interaction 是必须的 dataset, 所有 recipe 都会用到, User 和 Item 被称作 metadata dataset, 只有个别 recipe 会用到.&lt;/p&gt;&#xA;&lt;p&gt;每个 dataset 创建的时候需要建立一个 schema, 来描述各自的结构(avro 格式).&lt;/p&gt;&#xA;&lt;p&gt;example User schema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &amp;quot;type&amp;quot;: &amp;quot;record&amp;quot;,&#xA;    &amp;quot;name&amp;quot;: &amp;quot;Users&amp;quot;,与与&#xA;    &amp;quot;namespace&amp;quot;: &amp;quot;com.amazonaws.personalize.schema&amp;quot;,&#xA;    &amp;quot;fields&amp;quot;: [&#xA;        {&#xA;            &amp;quot;name&amp;quot;: &amp;quot;user_id&amp;quot;,&#xA;            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;&#xA;        },&#xA;        {&#xA;            &amp;quot;name&amp;quot;: &amp;quot;birthday&amp;quot;,&#xA;            &amp;quot;type&amp;quot;: &amp;quot;int与&#xA;        },&#xA;        {&#xA;            &amp;quot;name&amp;quot;: &amp;quot;gender&amp;quot;,&#xA;            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,&#xA;            &amp;quot;categorical&amp;quot;: true&#xA;        },&#xA;        {&#xA;            &amp;quot;name&amp;quot;: &amp;quot;location&amp;quot;,&#xA;            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,&#xA;            &amp;quot;categorical&amp;quot;: true&#xA;        }&#xA;    ],&#xA;    &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;其中 &lt;code&gt;user_id&lt;/code&gt; 是必填字段, 其他都是可选的自定义字段, 其他字段如果是 string, 必须加上&lt;code&gt;&amp;quot;categorical&amp;quot;: true&lt;/code&gt;, 表示它是用来分类的.&lt;/p&gt;</description>
    </item>
    <item>
      <title>编写 postmortem</title>
      <link>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</link>
      <pubDate>Sat, 18 Jan 2020 15:20:47 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</guid>
      <description>&lt;p&gt;成功的经验总是带有点运气成份, 失败则是必然的:). 工作中， 线上环境的问题千奇百怪, 有的来自自己代码 bug, 有的是配置错误, 有时候是第三方的 vendor 成了猪队友. 对于一些排查过程比较困难或具有代表性的问题, 需要记录下来, 一般把这个过程叫做 postmortem(验尸).&lt;/p&gt;&#xA;&lt;p&gt;这篇写一下自己做 postmortem 的过程, 并记录一个最近处理的故障.&lt;/p&gt;&#xA;&lt;h2 id=&#34;postmortem-process&#34;&gt;Postmortem process&lt;/h2&gt;&#xA;&lt;p&gt;我大体分以下几个部分:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用尽量简练的语句描述清楚在什么时间发生了什么,谁参与了问题的处理(when, what, who)?&lt;/li&gt;&#xA;&lt;li&gt;详细描述解决问题的过程, 包括但不限于:  debug 的过程, 中间的推测, 用到的工具&amp;hellip; (How)&lt;/li&gt;&#xA;&lt;li&gt;如果找到了 root case, 记录下来, 没找到, 记录下当时的 workaround, 有什么副作用. (Why)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;postmortem-case&#34;&gt;Postmortem case&lt;/h2&gt;&#xA;&lt;p&gt;实际的 postmorten 写得更简单一点, 这里把过程中的一些思考也记下来.&lt;/p&gt;</description>
    </item>
    <item>
      <title>聊聊 AWS 的计费模式</title>
      <link>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Mon, 30 Dec 2019 12:08:47 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</guid>
      <description>&lt;p&gt;网上经常有人诟病 AWS 的计费模式复杂, 喜欢国内那种打包式的售卖方式, 这个可能受限于每个公司的财务流程, 预算制定方式, 合不合国情,本文不讨论.&#xA;仅从开发者的角度介绍下 AWS 部分常用 service 的计费方式.&lt;/p&gt;&#xA;&lt;p&gt;PS: 那些为了蹭一年 free plan 然后抱怨什么偷跑流量, 偷偷扣费的大哥就省省吧, AWS 根本不是给个人用的, 老老实实用 lightsail 得了.&lt;/p&gt;&#xA;&lt;h2 id=&#34;ec2&#34;&gt;EC2&lt;/h2&gt;&#xA;&lt;p&gt;EC2 的价格是最复杂的, 一台 EC2 instance 的价格组成:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;instance fee, 实际支付的是 CPU+RAM 的费用&lt;/li&gt;&#xA;&lt;li&gt;EBS fee, server 的根分区都是 EBS volume, 按 EBS 计费(31GB 的 volume 使用 1 小时, 和 1GB 的volume 使用 744 小时价格相同).&lt;/li&gt;&#xA;&lt;li&gt;data transfer fee, 这部分组成比较复杂，简单讲, 入流量免费, 出流量按 GiB 计费, 如果出流量是到 AWS 其他 region, 价格比一般公网便宜, 在内网传输流量, 同一个 available zone 是免费的,&#xA;跨 az 收费.&lt;/li&gt;&#xA;&lt;li&gt;EIP fee, 每台 instance 挂一个 EIP 是免费的(eip 使用状态不收费, 闲置收费), 超过一个 eip, 每个按小时再收费.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;instance fee 又分 on-demand/resersed/spot instance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8s Volume Resize on EKS</title>
      <link>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</link>
      <pubDate>Fri, 12 Apr 2019 13:23:54 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</guid>
      <description>&lt;p&gt;从 k8s 1.8 开始支持 &lt;a href=&#34;https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/&#34;&gt;PersistentVolumeClaimResize&lt;/a&gt;. 但 api 是 alpha 状态, 默认不开启, eks launch&#xA;的时候版本是 1.10, 因为没法改 control plane, 所以没法直接在 k8s 内做 ebs 扩容. 后来升级到了&#xA;1.11, 这个 feature 默认被打开了, 尝试了下直接在 EKS 内做 ebs 的扩容.&lt;/p&gt;&#xA;&lt;p&gt;注意:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这个 feature 只能对通过 pvc 管理的 volume 做扩容, 如果直接挂的是 pv, 只能自己按传统的 ebs 扩容流程在 eks 之外做.&lt;/li&gt;&#xA;&lt;li&gt;用来创建 pvc 的 storageclass 上必须设置 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 为 true&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在 eks 上使用 pv/pvc,　对于需要 retain 的 volume, 我一般的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在 eks 之外手工创建 ebs volume.&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pv, 指向 ebs 的 volume id&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pvc, 指向 pv&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;示例 yaml:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---&#xA;kind: PersistentVolume&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  storageClassName: gp2&#xA;  persistentVolumeReclaimPolicy: Retain&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  capacity:&#xA;    storage: 5Gi&#xA;  awsElasticBlockStore:&#xA;    fsType: ext4&#xA;    volumeID: vol-xxxx   # create in aws manually&#xA;---&#xA;kind: PersistentVolumeClaim&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test-claim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  volumeName: test&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;假如现在要扩容到 10Gi, 流程是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>管理负载</title>
      <link>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</link>
      <pubDate>Tue, 12 Feb 2019 13:08:29 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</guid>
      <description>&lt;p&gt;最近在看 google 的 &lt;code&gt;&amp;lt;The Site Reliablity Workbook&amp;gt;&lt;/code&gt;, 其中有一章是&amp;quot;Manage load&amp;quot;, 内容还挺详细的, 结合在 aws 上的经验做点笔记.&lt;/p&gt;&#xA;&lt;h2 id=&#34;load-balancing&#34;&gt;Load Balancing&lt;/h2&gt;&#xA;&lt;p&gt;流量的入口是负载均衡, 最最简单的做法是在 DNS 上做 round robin, 但这样很依赖 client, 不同的 client 可能不完全遵守 DNS 的 TTL, 当地的 ISP 也会有缓存.&lt;/p&gt;&#xA;&lt;p&gt;google 用 anycast 技术在自己的网络中通过 BGP 给一个域名发布多个 endpoint, 共享一个 vip(virtual ip), 通过 BGP routing 来将用户的数据包发送到最近的 frontend server, 以此来减少 latency.&lt;/p&gt;&#xA;&lt;p&gt;但只依赖 BGP 会带来两个问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;某个地区的用户过多会给最近的 frontend server 带来过高的负载&lt;/li&gt;&#xA;&lt;li&gt;ISP 的 BGP 路由会重计算, 当 BGP routing 变化后, 进行中的 tcp connection 会被 reset(同一个 connection 上的后续数据包被发送到不同的 server, tcp session 不存在于新 server 上)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;为了解决原生 BGP anycast 的问题, google 开发了 Maglev, 即使路由发生了变化(routing flap), connection 也不断开, 把这种方式叫做 stablized anycast.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS Aurora DB</title>
      <link>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</link>
      <pubDate>Wed, 31 Oct 2018 15:23:45 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</guid>
      <description>&lt;p&gt;最近在把部分用 RDS 的 MySQL 迁移到 aurora 上去, 读了下 aurora 的 paper, 顺便和 RDS 的架构做些对比.&lt;/p&gt;&#xA;&lt;h2 id=&#34;paper-notes&#34;&gt;Paper notes&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;存储计算分离&lt;/li&gt;&#xA;&lt;li&gt;redo log 下推到存储层&lt;/li&gt;&#xA;&lt;li&gt;副本: 6 副本 3 AZ(2 per az), 失去一个 AZ + 1 additoinal node 不会丢数据(可读不可写). 失去一个 AZ (或任意2 node) 不影响数据写入.&lt;/li&gt;&#xA;&lt;li&gt;10GB 一个 segment, 每个 segment 6 副本一个 PG (protection group), 一 AZ　两副本.&lt;/li&gt;&#xA;&lt;li&gt;在 10Gbps 的网络上, 修复一个 10GB 的segment 需要 10s.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;MySQL 一个应用层的写会在底层产生很多额外的写操作，会带来写放大问题:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.monsterxx03.com/posts/images/aurora-mysql-replication.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;redo log 用来 crash recovery, binlog 会上传 s3　用于 point in time restore.&lt;/p&gt;&#xA;&lt;p&gt;在 aurora 里，只有 redo log 会通过网络复制到各个 replica, master 会等待 4/6 replicas 完成 redo log 的写入就认为写入成功 (所以失去3副本就无法写入数据了). 其他副本会根据 redo log 重建数据(单独的 redo log applicator 进程).&lt;/p&gt;</description>
    </item>
    <item>
      <title>在 redshift 中计算 p95 latency</title>
      <link>https://blog.monsterxx03.com/2018/10/12/%E5%9C%A8-redshift-%E4%B8%AD%E8%AE%A1%E7%AE%97-p95-latency/</link>
      <pubDate>Fri, 12 Oct 2018 14:49:13 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/10/12/%E5%9C%A8-redshift-%E4%B8%AD%E8%AE%A1%E7%AE%97-p95-latency/</guid>
      <description>&lt;p&gt;p95 latency 的定义: 把一段时间的 latency 按照从小到大排序, 砍掉最高的 %5, 剩下最大的值就是 p95 latency. p99, p90 同理.&lt;/p&gt;&#xA;&lt;p&gt;p95 latency 表示该时间段内 95% 的 reqeust 都比这个值快.&lt;/p&gt;&#xA;&lt;p&gt;一般我直接看 CloudWatch, 和 datadog 算好的 p95 值. 这次看看怎么从 access log 里直接计算 p95 latency.&lt;/p&gt;&#xA;&lt;p&gt;假设在 redshift 中有一张表存储了应用的 access log, 结构如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;CREATE TABLE access_log (&#xA;    url         string,&#xA;    time        string,&#xA;    resp_time   real&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;url&lt;/th&gt;&#xA;          &lt;th&gt;time&lt;/th&gt;&#xA;          &lt;th&gt;resp_time&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;/test1&lt;/td&gt;&#xA;          &lt;td&gt;2018-10-11T00:10:00.418480Z&lt;/td&gt;&#xA;          &lt;td&gt;0.123&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;/test2&lt;/td&gt;&#xA;          &lt;td&gt;2018-10-11T00:12:00.512340Z&lt;/td&gt;&#xA;          &lt;td&gt;0.321&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;要算 p95 很简单, 把 log 按分钟数分组, 用 &lt;code&gt;percentile_cont&lt;/code&gt; 在组内按 &lt;code&gt;resp_time&lt;/code&gt; 排序计算 就能得到:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;select date_trunc(&#39;minute&#39;, time::timestamp) as ts,&#xA;      percentile_cont(0.95) within group(order by resp_time) as p95&#xA;from access_log &#xA;group by 1&#xA;order by 1;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;得到:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;     ts          |        p95&#xA;---------------------+-------------------&#xA; 2018-10-11 00:00:00 |  0.71904999999995&#xA; 2018-10-11 00:01:00 | 0.555550000000034&#xA; 2018-10-11 00:02:00 | 0.478999999999939&#xA; 2018-10-11 00:03:00 | 0.507250000000081&#xA; 2018-10-11 00:04:00 | 0.456000000000025&#xA; 2018-10-11 00:05:00 | 0.458999999999949&#xA; 2018-10-11 00:06:00 | 0.581000000000054&#xA; 2018-10-11 00:07:00 | 0.585099999999937&#xA; 2018-10-11 00:08:00 | 0.527999999999908&#xA; 2018-10-11 00:09:00 | 0.570999999999936&#xA; 2018-10-11 00:10:00 | 0.587950000000069&#xA; 2018-10-11 00:11:00 | 0.648900000000077&#xA; 2018-10-11 00:12:00 | 0.570000000000024&#xA; 2018-10-11 00:13:00 | 0.592649999999954&#xA; 2018-10-11 00:14:00 | 0.584149999999998&#xA; 2018-10-11 00:15:00 |  3.00854999999952&#xA; 2018-10-11 00:16:00 | 0.832999999999871&#xA; 2018-10-11 00:17:00 |  1.07154999999991&#xA; 2018-10-11 00:18:00 | 0.553600000000092&#xA; 2018-10-11 00:19:00 | 0.605799999999997&#xA; 2018-10-11 00:20:00 | 0.832000000000137&#xA; ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;PERCENTILE_CONT&lt;/code&gt; 是逆分布函数, 给定一个百分比, 在一个连续分布模型上计算该百分比处的数值, 如果在该点处没有数据, 会根据最接近的前后值进行插值计算出实际值.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EkS 评测 part-3</title>
      <link>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</link>
      <pubDate>Wed, 26 Sep 2018 10:16:42 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</guid>
      <description>&lt;p&gt;这篇记录对 ingress 的测试.&lt;/p&gt;&#xA;&lt;p&gt;ingress 用来将外部流量导入　k8s 内的　service. 将 service 的类型设置为 LoadBalancer / NodePort 也可以将单个 service 暴露到公网, 但用 ingress 可以只使用一个公网入口,根据　host name 或　url path 来将请求分发到不同的 service.&lt;/p&gt;&#xA;&lt;p&gt;一般　k8s 内的资源都会由一个 controller 来负责它的状态管理, 都由 kube-controller-manager 负责，　但 ingress controller 不是它的一部分，需要是视情况自己选择合适的 ingress controller.&lt;/p&gt;&#xA;&lt;p&gt;在 eks 上我主要需要 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;ingress-nginx&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&#34;&gt;aws-alb-ingress-controller&lt;/a&gt;. 注意, nginx inc 还维护一个 &lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress&#34;&gt;kubernetes-ingress&lt;/a&gt;, 和官方那个不是一个东西， 没测试过.&lt;/p&gt;&#xA;&lt;p&gt;这里主要只测试了 ingress-nginx, 看了下内部实现, 数据的转发真扭曲&amp;hellip;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>eks 评测 part-2</title>
      <link>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</link>
      <pubDate>Fri, 21 Sep 2018 10:28:17 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</guid>
      <description>&lt;p&gt;上文测试了一下 EKS 和 cluster autoscaler, 本文记录对 persisten volume 的测试.&lt;/p&gt;&#xA;&lt;h1 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h1&gt;&#xA;&lt;p&gt;创建 gp2 类型的 storageclass, 并用 annotations 设置为默认 sc, dynamic volume provision 会用到:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kind: StorageClass&#xA;apiVersion: storage.k8s.io/v1&#xA;metadata:&#xA;    name: gp2&#xA;    annotations:&#xA;        storageclass.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&#xA;provisioner: kubernetes.io/aws-ebs&#xA;reclaimPolicy: Retain&#xA;parameters:&#xA;    type: gp2&#xA;    fsType: ext4&#xA;    encrypted: &amp;quot;true&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;因为 eks 是基于 1.10.3 的, volume expansion 还是 alpha 状态, 没法自动开启(没法改 api server 配置), 所以 storageclass 的 allowVolumeExpansion, 设置了也没用.&#xA;这里 &lt;code&gt;encrypted&lt;/code&gt; 的值必须是字符串, 否则会创建失败, 而且报错莫名其妙.&lt;/p&gt;&#xA;&lt;h2 id=&#34;创建-pod-的时候指定一个已存在的-ebs-volume&#34;&gt;创建 pod 的时候指定一个已存在的 ebs volume&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;    name: test&#xA;spec:&#xA;    volumes:&#xA;        - name: test&#xA;          awsElasticBlockStore:&#xA;              fsType: ext4&#xA;              volumeID: vol-03670d6294ccf29fd&#xA;    containers:&#xA;        - image: nginx&#xA;          name: nginx&#xA;          volumeMounts:&#xA;              - name: test&#xA;                mountPath: /mnt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;kubectl -it test -- /bin/bash&lt;/code&gt;  进去看一下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@test:/# df -h&#xA;Filesystem      Size  Used Avail Use% Mounted on&#xA;overlay          20G  2.2G   18G  11% /&#xA;tmpfs           3.9G     0  3.9G   0% /dev&#xA;tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup&#xA;/dev/xvdcz      976M  2.6M  907M   1% /mnt&#xA;/dev/xvda1       20G  2.2G   18G  11% /etc/hosts&#xA;shm              64M     0   64M   0% /dev/shm&#xA;tmpfs           3.9G   12K  3.9G   1% /run/secrets/kubernetes.io/serviceaccount&#xA;tmpfs           3.9G     0  3.9G   0% /sys/firmware&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;那块 volume 的确绑定在 &lt;code&gt;/mnt&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>EKS 评测</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>&lt;p&gt;EKS 正式 launch 还没有正经用过, 最近总算试了一把, 记录一点.&lt;/p&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;p&gt;AWS 官方的 Guide 只提供了一个 cloudformation template 来设置 worker node, 我喜欢用 terraform, 可以跟着这个文档尝试:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html 来设置完整的 eks cluster 和管理 worker node 的 autoscaling  group.&lt;/p&gt;&#xA;&lt;p&gt;设置完 EKS 后需要添加一条 ConfigMap:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: aws-auth&#xA;  namespace: kube-system&#xA;data:&#xA;  mapRoles: |&#xA;    - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole&#xA;      username: system:node:{{EC2PrivateDNSName}}&#xA;      groups:&#xA;        - system:bootstrappers&#xA;        - system:nodes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;这样 worker node 节点才能加入集群.&lt;/p&gt;&#xA;&lt;h2 id=&#34;网络&#34;&gt;网络&lt;/h2&gt;&#xA;&lt;p&gt;之前一直没有在 AWS 上尝试构建 k8s 的一个原因, 就是不喜欢 overlay 网络, 给系统带来了额外的复杂度和管理开销, VPC flowlog 看不到 pod 之间流量, 封包后 tcpdump 不好 debug 应用层流量.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Use SNS &amp; SQS to build Pub/Sub System</title>
      <link>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</link>
      <pubDate>Wed, 23 May 2018 18:05:28 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</guid>
      <description>&lt;p&gt;Recently, we build pub/sub system based on AWS&amp;rsquo;s SNS &amp;amp; SQS service, take some notes.&lt;/p&gt;&#xA;&lt;p&gt;Originally, we have an pub/sub system based on redis(use BLPOP to listen to a redis list). It&amp;rsquo;s&#xA;really simple, and mainly for cross app operations. Now we have needs to enhance it to support more complex&#xA;pubsub logic, eg: topic based distribution. It don&amp;rsquo;t support redelivery as well, if subscribers failed to process&#xA;the message, message will be dropped.&lt;/p&gt;&#xA;&lt;p&gt;There&amp;rsquo;re three obvious choices in my mind:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;kafka&lt;/li&gt;&#xA;&lt;li&gt;AMQP based system (rabbitmq,activemq &amp;hellip;)&lt;/li&gt;&#xA;&lt;li&gt;SNS + SQS&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My demands for this system are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support message persistence.&lt;/li&gt;&#xA;&lt;li&gt;Support topic based message distribution.&lt;/li&gt;&#xA;&lt;li&gt;Easy to manage.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The data volume won&amp;rsquo;t be very large, so performance and throughput won&amp;rsquo;t be critical concerns.&lt;/p&gt;&#xA;&lt;p&gt;I choose SNS + SQS, main concerns are from operation side:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;kafka need zookeeper to support cluster.&lt;/li&gt;&#xA;&lt;li&gt;rabbitmq need extra configuration for HA, and AMQP model is relatively complex for programming.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So my decision is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;application publish message to SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Setup multi SQS queues to subscribe SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Let different application processes to subscribe to different queues to finish its logic.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;SQS and SNS is very simple, not too much to say, just some notes:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;SQS queue have two types, FIFO queue and standard queue. FIFO queue will ensure message order, and ensure exactly once delivery, tps is limited(3000/s)&#xA;standard queue is at least once delivery, message order is not ensured, tps is unlimited. In my case, I use standard queue, order is not very important.&lt;/li&gt;&#xA;&lt;li&gt;SQS message size limit is 256KB.&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;a href=&#34;https://github.com/p4tin/goaws&#34;&gt;goaws&lt;/a&gt; for local development, it has problem on processing message attributes, but I just use message body. messages only store in ram,&#xA;will be cleared after restarted.&lt;/li&gt;&#xA;&lt;li&gt;If you failed to deliver message to sqs from sns, can setup topic&amp;rsquo;s &lt;code&gt;sqs failure feedback role&lt;/code&gt; to log to cloudwatch, in most case it&amp;rsquo;s caused by iam permission.&lt;/li&gt;&#xA;&lt;li&gt;Message in sqs can retain at most 14 days.&lt;/li&gt;&#xA;&lt;li&gt;Once a message is received by a client, it will be invisible to other clients in &lt;code&gt;visibility_timeout_seconds&lt;/code&gt;(default 30s). It means if your client failed to process&#xA;the message, it will be redelivered after 30s.&lt;/li&gt;&#xA;&lt;li&gt;SQS client use long polling to receive message, set &lt;code&gt;receive_wait_time_seconds&lt;/code&gt; to reduce api call to reduce fee.&lt;/li&gt;&#xA;&lt;li&gt;If your client failed to process a message due to bug, the message will be redelivered looply, set &lt;code&gt;redrive_policy&lt;/code&gt; for the queue to limit retry count, and set a dead letter&#xA;queue to store those messages. You can decide how to handle them late.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I setup SNS and SQS via terraform, used following resources:&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS 的 K8S CNI Plugin</title>
      <link>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</link>
      <pubDate>Mon, 09 Apr 2018 15:28:38 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</guid>
      <description>&lt;p&gt;EKS 还没有 launch, 但 AWS 先开源了自己的 CNI 插件, 简单看了下, 说说它的实现和其他 K8S 网络方案的差别.&lt;/p&gt;&#xA;&lt;p&gt;K8S 集群对网络有几个基本要求:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;container 之间网络必须可达，且不通过 NAT&lt;/li&gt;&#xA;&lt;li&gt;所有 node 必须可以和所有 container 通信, 且不通过 NAT&lt;/li&gt;&#xA;&lt;li&gt;container 自己看到的 IP, 必须和其他 container 看到的它的 ip 相同.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;flannel-in-vpc&#34;&gt;Flannel in VPC&lt;/h2&gt;&#xA;&lt;p&gt;flannel 是 K8S 的一个 CNI 插件, 在 VPC 里使用 flannel 的话, 有几个选择:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过 VXLAN/UDP 进行封包, 封包影响网络性能, 而且不好 debug&lt;/li&gt;&#xA;&lt;li&gt;用 aws vpc backend, 这种方式会把每台主机的 docker 网段添加进 vpc routing table, 但默认 routing table 里只能有50条规则, 所以只能 50 个 node, 可以发 ticket 提升, 但数量太多会影响 vpc 性能.&lt;/li&gt;&#xA;&lt;li&gt;host-gw, 在每个 node 上直接维护集群中所有节点的路由, 没测试过, 感觉出问题也很难 debug, 假如用 autoscaling group 管理 node 集群, 能否让 K8S 在 scale in/out 的时候修改所有节点的路由?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;以上方式都只能利用 EC2 上的单网卡, security group 也没法作用在 pod 上.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS lambda 的一些应用场景</title>
      <link>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</link>
      <pubDate>Fri, 23 Mar 2018 17:40:54 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</guid>
      <description>&lt;p&gt;这几年吹 serverless 的比较多,  在公司内部也用 lambda , 记录一下, 这东西挺有用, 但远不到万能, 场景比较有限.&lt;/p&gt;&#xA;&lt;p&gt;lambda 的代码的部署用的 &lt;a href=&#34;https://serverless.com&#34;&gt;serverless&lt;/a&gt; 框架, 本身支持多种 cloud 平台, 我们就只在 aws lambda 上了.&lt;/p&gt;&#xA;&lt;p&gt;我基本上就把 lambda 当成 trigger 和 web hook 用.&lt;/p&gt;&#xA;&lt;h2 id=&#34;和--auto-scaling-group-一起用&#34;&gt;和  auto scaling group 一起用&lt;/h2&gt;&#xA;&lt;p&gt;线上所有分组的机器都是用 auto scaling group 管理的, 只不过 stateless 的 server 开了自动伸缩, 带状态的 (ElasticSearch cluster, redis cache cluster) 只用来维护固定 size.&lt;/p&gt;&#xA;&lt;p&gt;在往一个 group 里加 server 的时候, 要做的事情挺多的, 给新 server 添加组内编号 tag, 添加内网域名, provision, 部署最新代码.&lt;/p&gt;&#xA;&lt;p&gt;这些事都用 jenkins 来做, 但怎么触发 jenkins job 呢?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Access sensitive variables on AWS lambda</title>
      <link>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</link>
      <pubDate>Wed, 28 Feb 2018 21:45:23 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</guid>
      <description>&lt;p&gt;AWS lambda is convenient to run simple serverless application, but how to access sensitive data in code? like password,token&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Usually, we inject secrets as environment variables, but they&amp;rsquo;re still visable on lambda console. I don&amp;rsquo;t use it in aws lambda.&lt;/p&gt;&#xA;&lt;p&gt;The better way is use &lt;a href=&#34;https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html&#34;&gt;aws parameter store&lt;/a&gt; as configuration center. It can work with KMS to encrypt your data.&lt;/p&gt;&#xA;&lt;p&gt;Code example:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;client = boto3.client(&#39;ssm&#39;)&#xA;resp = client.get_parameter(&#xA;    Name=&#39;/redshift/admin/password&#39;,&#xA;    WithDecryption=True&#xA;)&#xA;resp:&#xA;&#xA;    {&#xA;        &amp;quot;Parameter&amp;quot;: {&#xA;            &amp;quot;Name&amp;quot;: &amp;quot;/redshift/admin/password&amp;quot;,&#xA;            &amp;quot;Type&amp;quot;: &amp;quot;SecureString&amp;quot;,&#xA;            &amp;quot;Value&amp;quot;: &amp;quot;password value&amp;quot;,&#xA;            &amp;quot;Version&amp;quot;: 1&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Things you need to do to make it work:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create a new KMS key&lt;/li&gt;&#xA;&lt;li&gt;Use new created KMS key to encrypt your data in parameter store.&lt;/li&gt;&#xA;&lt;li&gt;Set a execution role for your lambda function.&lt;/li&gt;&#xA;&lt;li&gt;In the KMS key&amp;rsquo;s setting page, add the lambda execution role to the list which can read this KMS key.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Then your lambda code can access encrypted data at runtime, and you needn&amp;rsquo;t set aws access_key/secret_key, lambda execution role enable access to data in parameter store.&lt;/p&gt;&#xA;&lt;p&gt;BTW, parameter store support hierarchy(at most 15 levels), splitted by &lt;code&gt;/&lt;/code&gt;. You can retrive data under same level in one call, deltails can be found in doc, eg: &lt;a href=&#34;http://boto3.readthedocs.io/en/latest/reference/services/ssm.html#SSM.Client.get_parameters_by_path&#34;&gt;http://boto3.readthedocs.io/en/latest/reference/services/ssm.html#SSM.Client.get_parameters_by_path&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get Real Client Ip on AWS</title>
      <link>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</link>
      <pubDate>Thu, 01 Feb 2018 15:20:37 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</guid>
      <description>&lt;p&gt;If you run a webserver on AWS, get real client ip will be tricky if you didn&amp;rsquo;t configure server right and write code correctly.&lt;/p&gt;&#xA;&lt;p&gt;Things related to client real ip:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CloudFront (cdn)&lt;/li&gt;&#xA;&lt;li&gt;ALB (loadbalancer)&lt;/li&gt;&#xA;&lt;li&gt;nginx (on ec2)&lt;/li&gt;&#xA;&lt;li&gt;webserver (maybe a python flask application).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Request sequence diagram will be like following:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.monsterxx03.com/posts/images/cf-alb-nginx.png&#34; alt=&#34;req&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;User&amp;rsquo;s real client ip is forwarded by front proxies one by one in head &lt;code&gt;X-Forwarded-For&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;For CloudFront:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If user&amp;rsquo;s req header don&amp;rsquo;t  have &lt;code&gt;X-Forwarded-For&lt;/code&gt;, it will set user&amp;rsquo;s ip(from tcp connection) in &lt;code&gt;X-Forwarded-For&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;If user&amp;rsquo;s req already have &lt;code&gt;X-Forwarded-For&lt;/code&gt;, it will append user&amp;rsquo;s ip(from tcp connection) to the end of &lt;code&gt;X-Forwarded-For&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For ALB, rule is same as CloudFront, so the &lt;code&gt;X-Forwarded-For&lt;/code&gt; header pass to nginx will be the value received from CloudFront + CloudFront&amp;rsquo;s ip.&lt;/p&gt;&#xA;&lt;p&gt;For nginx, things will be tricky depends on your config.&lt;/p&gt;&#xA;&lt;p&gt;Things maybe involved in nginx:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;real ip module&lt;/li&gt;&#xA;&lt;li&gt;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If you didn&amp;rsquo;t use real ip module, you need to pass X-Forwarded-For head explictly.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&lt;/code&gt; will append ALB&amp;rsquo;s ip to the end of &lt;code&gt;X-Forwarded-For&lt;/code&gt; header received from ALB.&lt;/p&gt;&#xA;&lt;p&gt;So &lt;code&gt;X-Forwarded-For&lt;/code&gt; header your webserver received will be &lt;code&gt;user ip,cloudfront ip, alb ip&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Or you can use real ip module to trust the value passed from ALB.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DynamoDB</title>
      <link>https://blog.monsterxx03.com/2017/12/15/dynamodb/</link>
      <pubDate>Fri, 15 Dec 2017 22:24:36 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2017/12/15/dynamodb/</guid>
      <description>&lt;p&gt;DynamoDB 是 AWS 的托管 NoSQL 数据库，可以当作简单的 KV 数据库使用，也可以作为文档数据库使用.&lt;/p&gt;&#xA;&lt;h2 id=&#34;data-model&#34;&gt;Data model&lt;/h2&gt;&#xA;&lt;p&gt;组织数据的单位是 table, 每张 table 必须设置 primary key, 可以设置可选的 sort key 来做索引.&lt;/p&gt;&#xA;&lt;p&gt;每条数据记作一个 item, 每个 item 含有一个或多个 attribute, 其中必须包括 primary key.&lt;/p&gt;&#xA;&lt;p&gt;attribute 对应的 value 支持以下几种类型:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Number, 由于 DynamoDB 的传输协议是 http + json, 为了跨语言的兼容性, number 一律会被转成 string 传输.&lt;/li&gt;&#xA;&lt;li&gt;Binary, 用来表示任意的二进制数据，会用 base64 encode 后传输.&lt;/li&gt;&#xA;&lt;li&gt;Boolean, true or false&lt;/li&gt;&#xA;&lt;li&gt;Null&lt;/li&gt;&#xA;&lt;li&gt;Document 类型包含 List 和 Map, 可以互相嵌套.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;List, 个数无限制, 总大小不超过 400KB&lt;/li&gt;&#xA;&lt;li&gt;Map, 属性个数无限制，总大小不超过 400 KB, 嵌套层级不超过 32 级.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Set,  一个 set 内元素数目无限制, 无序，不超过 400KB, 但必须属于同一类型, 支持 number set, binary set, string set.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;选择-primary-key&#34;&gt;选择 primary key&lt;/h2&gt;&#xA;&lt;p&gt;Table 的 primary key 支持单一的 partition key 或复合的 partition key + sort key. 不管哪种，最后的组成的primary key 在一张表中必须唯一.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS DMS notes</title>
      <link>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</link>
      <pubDate>Sat, 14 Oct 2017 22:33:36 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</guid>
      <description>&lt;p&gt;AWS&amp;rsquo;s DMS (Data migration service) can be used to do incremental ETL between databases. I use it to load data from RDS (MySQL) to Redshift.&lt;/p&gt;&#xA;&lt;p&gt;It works, but have some concerns. Take some notes when doing this project.&lt;/p&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;Source RDS must:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Enable automatic backups&lt;/li&gt;&#xA;&lt;li&gt;Increase binlog remain time, &lt;code&gt;call mysql.rds_set_configuration(&#39;binlog retention hours&#39;, 24);&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;binlog_format&lt;/code&gt; to &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Privileges on source RDS: &lt;code&gt;REPLICATION CLIENT &lt;/code&gt;, &lt;code&gt;REPLICATION SLAVE &lt;/code&gt;, &lt;code&gt;SELECT&lt;/code&gt; on replication target tables&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;ddl-on-source-table&#34;&gt;DDL on source table&lt;/h2&gt;&#xA;&lt;p&gt;Redshift has some limits on change columns:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;New column only must be added in the end&lt;/li&gt;&#xA;&lt;li&gt;Can&amp;rsquo;t rename columns&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So for DDL on source MySQL, you can&amp;rsquo;t add columns at non end postition, otherwise data in target table will corrupt. I disabled ddl changes target db:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    &amp;quot;ChangeProcessingDdlHandlingPolicy&amp;quot;:{  &#xA;        &amp;quot;HandleSourceTableDropped&amp;quot;:false,&#xA;        &amp;quot;HandleSourceTableTruncated&amp;quot;:false,&#xA;        &amp;quot;HandleSourceTableAltered&amp;quot;:false&#xA;    },&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;If source table schema changed, I just drop and reload target table on console.&lt;/p&gt;&#xA;&lt;h2 id=&#34;control-write-speed-on-redshift&#34;&gt;Control write speed on Redshift&lt;/h2&gt;&#xA;&lt;p&gt;Since Redshift is an OLAP database, write operation is slow and concurrency is low, streaming data directly will have big impact on it.&lt;/p&gt;&#xA;&lt;p&gt;And we have may analysis jobs running on redshift all the time, directly streaming will lock target table and make my analysis jobs timeout.&lt;/p&gt;&#xA;&lt;p&gt;So I need to batch apply changes on DMS. Follow settings need to tweak in task settings json:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get all invalid PTR record on  Route53</title>
      <link>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</link>
      <pubDate>Fri, 29 Sep 2017 08:55:18 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</guid>
      <description>&lt;p&gt;I use autoscaling group to manage stateless servers. Servers go up and down every day.&lt;/p&gt;&#xA;&lt;p&gt;Once server is up, I will add a PTR record for it’s internal ip. But when it’s down, I didn’t cleanup the PTR record. As times fly, a lot of invalid PTR records left in Route53.&lt;/p&gt;&#xA;&lt;p&gt;To cleanup those PTR records realtime, you can write a lambda function, use server termination event as trigger. But how to cleanup the old records at once?&lt;/p&gt;&#xA;&lt;p&gt;Straightforward way is write a script to call AWS API to get a PTR list, get ip from record, test whether the ip is live, if not, delete it.&lt;/p&gt;&#xA;&lt;p&gt;Since use awscli to delete a Route53 record is very troublesome (involve json format), you’d better write a python script to delete them. I just demo some ideas to collect those records via shell.&lt;/p&gt;&#xA;&lt;p&gt;You can do it in a single line, but make things clear and easy to debug, I split it into several steps.&lt;/p&gt;&#xA;&lt;h2 id=&#34;get-ptr-record-list&#34;&gt;Get PTR record list&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;aws route53 list-resource-record-sets  --hosted-zone-id xxxxx --query &amp;quot;ResourceRecordSets[?Type==&#39;PTR&#39;].Name&amp;quot; |  grep -Po &#39;&amp;quot;(.+?)&amp;quot;&#39; | tr -d \&amp;quot; &amp;gt; ptr.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;ptr.txt will contain lines like:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;1.0.0.10.in-addr.arpa.&#xA;2.0.0.10.in-addr.arpa.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;get-ip-list-from-ptr-records&#34;&gt;Get ip list from PTR records&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;cat ptr.txt | while read -r line ; do echo -n $line | tac -s. | cut -d. -f3- | sed &#39;s/.$//&#39; ; done &amp;gt; ip.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;ip.txt:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Build private static website on S3</title>
      <link>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</link>
      <pubDate>Sat, 19 Aug 2017 07:28:16 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</guid>
      <description>&lt;p&gt;Build static website on S3 is very easy, but by default, it can be accessed by open internet.It will be super helpful if we can build website only available in VPC. Then we can use it to host internal deb repo, doc site…&lt;/p&gt;&#xA;&lt;p&gt;Steps are very easy, you only need VPC endpoints and S3 bucket policy.&lt;/p&gt;&#xA;&lt;p&gt;AWS api is open to internet, if you need to access S3 in VPC, your requests will pass through VPC’s internet gateway or NAT gateway. With VPC endpoints(can be found in VPC console), your requests to S3 will go through AWS’s internal network. Currently, VPC endpoints only support S3, support for dynamodb is in test.&lt;/p&gt;&#xA;&lt;p&gt;To restrict S3 bucket only available in your VPC, need to set bucket policy (to host static website, enable static website support first). At first, I didn’t check doc, try to restrict access by my VPC ip cidr, but it didn’t work, I need to restrict by VPC endpoint id:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,&#xA;  &amp;quot;Id&amp;quot;: &amp;quot;Policy1415115909152&amp;quot;,&#xA;  &amp;quot;Statement&amp;quot;: [&#xA;    {&#xA;      &amp;quot;Sid&amp;quot;: &amp;quot;Access-to-specific-VPCE-only&amp;quot;,&#xA;      &amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;,&#xA;      &amp;quot;Action&amp;quot;: &amp;quot;s3:GetObject&amp;quot;,&#xA;      &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,&#xA;      &amp;quot;Resource&amp;quot;: [&amp;quot;arn:aws:s3:::my_secure_bucket&amp;quot;,&#xA;                   &amp;quot;arn:aws:s3:::my_secure_bucket/*&amp;quot;],&#xA;      &amp;quot;Condition&amp;quot;: {&#xA;        &amp;quot;StringEquals&amp;quot;: {&#xA;          &amp;quot;aws:sourceVpce&amp;quot;: &amp;quot;vpce-1a2b3c4d&amp;quot;&#xA;        }&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;BTW, if you can config bucket policy restrict on VPC directly, with VPC endpoint you can limit to subnets. Details can be found in doc: &lt;a href=&#34;http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html&#34;&gt;http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Use redshift spectrum to do query on s3</title>
      <link>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</link>
      <pubDate>Fri, 21 Jul 2017 03:10:58 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/07/21/use-redshift-spectrum-to-do-query-on-s3/</guid>
      <description>&lt;h1 id=&#34;使用-redshift-spectrum-查询-s3-数据&#34;&gt;使用 redshift spectrum 查询 S3 数据&lt;/h1&gt;&#xA;&lt;p&gt;通常使用 redshift 做数据仓库的时候要做大量的 ETL 工作，一般流程是把各种来源的数据捣鼓捣鼓丢到 S3 上去，再从 S3 倒腾进 redshift. 如果你有大量的历史数据要导进 redshift，这个过程就会很痛苦，redshift 对一次倒入大量数据并不友好，你要分批来做。&lt;/p&gt;&#xA;&lt;p&gt;今年4月的时候， redshift 发布了一个新功能 spectrum, 可以从 redshift 里直接查询 s3 上的结构化数据。最近把部分数据仓库直接迁移到了 spectrum, 正好来讲讲。&lt;/p&gt;&#xA;&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;&#xA;&lt;p&gt;Glow 的数据仓库建在 redshift 上， 又分成了两个集群，一个 ssd 的集群存放最近 4 个月的数据，供产品分析，metrics report, debug 等等 adhoc 的查询。4个月之前的数据存放在一个 hdd 的集群里，便宜容量大，查询慢。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Infrastructure as Code</title>
      <link>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</link>
      <pubDate>Fri, 21 Apr 2017 16:25:07 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</guid>
      <description>&lt;p&gt;Create virtual resource on AWS is very convenient, but how to manage them will be a problem when your size grow.&lt;/p&gt;&#xA;&lt;p&gt;You will come to:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to explain the detail online settings for your colleagues (like: how our prod vpc is setup?what’s the DHCP option set?), navigate around AWS console is okay, but not convenient.&lt;/li&gt;&#xA;&lt;li&gt;Who did what to which resource at when? AWS have a service called &lt;code&gt;Config&lt;/code&gt;, can be used to track this change, but if you want to make things as clear as viewing git log, still a lot of works to do.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Ideally, we should manage AWS resources like code, all changes kept in VCS, so called &lt;code&gt;Infrastructure as Code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;I’ve tried three ways to do it:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ansible&lt;/li&gt;&#xA;&lt;li&gt;CloudFormation&lt;/li&gt;&#xA;&lt;li&gt;terraform&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In this article, I&amp;rsquo;ll compare them, however, the conclusion is to use terraform 🙂&lt;/p&gt;&#xA;&lt;h2 id=&#34;ansible&#34;&gt;Ansible&lt;/h2&gt;&#xA;&lt;p&gt;Provision tools, like ansible/chef/puppet, all can be used to create aws resources, but they have some common problems:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hard to track changes after bootstrap.&lt;/li&gt;&#xA;&lt;li&gt;No confident what it will do to existing resources.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For example, I define a security group in ansibble:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;ec2_group:&#xA;  name: &amp;quot;web&amp;quot;&#xA;  description: &amp;quot;security group in web&amp;quot;&#xA;  vpc_id: &amp;quot;vpc-xxx&amp;quot;&#xA;  region: &amp;quot;us-east-1&amp;quot;&#xA;  rules:&#xA;    - proto: tcp&#xA;      from_port: 80&#xA;      to_port: 80&#xA;      cidr_ip: 0.0.0.0/0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;It will create a security group named “web” in vpc-xxx. At first glance, it’s convenient and straightforward.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Migrate to encrypted RDS</title>
      <link>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</link>
      <pubDate>Fri, 28 Oct 2016 16:17:30 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</guid>
      <description>&lt;p&gt;最近公司在做 HIPAA Compliance 相关的事情，其中要求之一是所有db需要开启encryption.&lt;/p&gt;&#xA;&lt;p&gt;比较麻烦的是rds 的encryption 只能在创建的时候设定，无法之后修改, 所以必须对线上的db 做一次 migration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Redshift as data warehouse</title>
      <link>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</link>
      <pubDate>Sat, 16 Jul 2016 16:11:39 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2016/07/16/redshift-as-data-warehouse/</guid>
      <description>&lt;p&gt;Glow 的 server infrastructure 全部搭建在 AWS 上，一般要选择一些基础服务的时候，总是先看 AWS, 只要功能和成本符合要求，不会特意选择开源方案。&lt;/p&gt;&#xA;&lt;p&gt;数据仓库我们选择了 AWS 的 Redshift.&lt;/p&gt;&#xA;&lt;p&gt;在一年多的使用过程中 Redshift 的性能和稳定性都不错, 当然也有一些坑, 这里整理下在使用 redshift 的过程中的一些经验和遇到的坑.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
