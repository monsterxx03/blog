<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/k8s/</link>
    <description>Recent content in K8s on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Thu, 01 Apr 2021 14:40:30 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rolling Upgrade Worker Nodes in EKS</title>
      <link>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</link>
      <pubDate>Thu, 01 Apr 2021 14:40:30 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</guid>
      <description>&lt;p&gt;EKS control plane 的升级是比较简单的, 直接在 aws console 上点下就可以了, 但 worker node 是自己用 asg(autoscaling group) 管理的, 升级 worker node 又不想影响业务是有讲究的.&lt;/p&gt;&#xA;&lt;p&gt;跑在 EKS 里, 且希望不被中断 traffic 的有:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;stateless 的 api server, queue consumer&lt;/li&gt;&#xA;&lt;li&gt;被 redis sentinel 监控着的 redis master/slave&lt;/li&gt;&#xA;&lt;li&gt;用于 cache 的 redis cluster&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;写了个内部工具, 把下面的流程全部自动化了. 这样升级 eks 版本, 需要更换 worker node 时候就轻松多了. 因为这个工具对部署情况做了很多假设和限制, 开源的价值不是很大.&lt;/p&gt;&#xA;&lt;h2 id=&#34;stateless-application&#34;&gt;Stateless application&lt;/h2&gt;&#xA;&lt;p&gt;stateless 的应用全部用 deployment 部署.&lt;/p&gt;&#xA;&lt;p&gt;一般建议的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;修改 asg 的 launch configuration, 指向新版本的 ami&lt;/li&gt;&#xA;&lt;li&gt;把所有老的 worker node 用 kubectl cordon 标记成 unschedulable&lt;/li&gt;&#xA;&lt;li&gt;关闭 cluster-autoscaler&lt;/li&gt;&#xA;&lt;li&gt;修改 asg 的 desired count, 让 asg 用新 ami 启动新的 worker node&lt;/li&gt;&#xA;&lt;li&gt;用 kubectl drain 把老 worker node 上的 pod evict 掉, 让它们 schedule 到新的 worker node 上.&lt;/li&gt;&#xA;&lt;li&gt;重新开启 cluster autoscaler, 等它把老的闲置 worker node 关闭.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这里有些问题:&lt;/p&gt;</description>
    </item>
    <item>
      <title>从k8s deprecating docker 说起</title>
      <link>https://blog.monsterxx03.com/2020/12/04/%E4%BB%8Ek8s-deprecating-docker-%E8%AF%B4%E8%B5%B7/</link>
      <pubDate>Fri, 04 Dec 2020 10:49:40 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/12/04/%E4%BB%8Ek8s-deprecating-docker-%E8%AF%B4%E8%B5%B7/</guid>
      <description>&lt;p&gt;k8s 1.20 的 release note 里说 deprecated docker: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;对 docker 和 k8s 关系比较了解的人一看就知道是废弃 dockershim, 正常操作, 具体有什么影响, 建议阅读:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/&#34;&gt;https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2020/12/02/dockershim-faq/&#34;&gt;https://kubernetes.io/blog/2020/12/02/dockershim-faq/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;但容器圈里那堆名词的确很让人困惑, docker, dockershim, containerd, containerd-shim, runc, cri, oci, csi, cni, cri-o, 或许曾经还听说过 runv, rkt, clear containers, 后来又来了 kata containers, firecracker, gvisor. 论造名词, 造项目, 都快赶上前端圈的脚趾头了.&lt;/p&gt;&#xA;&lt;p&gt;这篇比较水, 就解释下它们大致的关系, 前提是你大概知道 docker, namespace, cgroup, container, k8s pod 之间的关系, 不做额外解释.&lt;/p&gt;&#xA;&lt;h2 id=&#34;从-docker-说起&#34;&gt;从 docker 说起&lt;/h2&gt;&#xA;&lt;p&gt;在我现在这台机器上的 ubuntu 18.04, 安装 docker, 添加官方源之后, 安装的 deb package(version 19.03) 是 docker-ce&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;apt show docker-ce&lt;/code&gt;  看依赖:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubectl Plugin for Redis Cluster</title>
      <link>https://blog.monsterxx03.com/2020/08/04/kubectl-plugin-for-redis-cluster/</link>
      <pubDate>Tue, 04 Aug 2020 22:44:40 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/08/04/kubectl-plugin-for-redis-cluster/</guid>
      <description>&lt;p&gt;在 k8s 上部署 redis cluster 后, 感觉 redis-cli 管理 redis cluster 非常别扭, 写了个 kubectl 的插件 &lt;a href=&#34;https://github.com/monsterxx03/kubectl-rc&#34;&gt;kubectl-rc&lt;/a&gt; 来辅助管理 redis-cluster.&lt;/p&gt;&#xA;&lt;h2 id=&#34;redis-cli-难用在哪&#34;&gt;redis-cli 难用在哪&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;不直观 &amp;amp; 不统一&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;部分 cluster 信息是直接通过 redis protocol 获得的, 比如 &lt;code&gt;cluster nodes&lt;/code&gt;, &lt;code&gt;cluster slots&lt;/code&gt;, 但部分管理命令又是通过 &lt;code&gt;redis-cli --cluster&lt;/code&gt; 执行的.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;cluster nodes&lt;/code&gt;, &lt;code&gt;cluster slots&lt;/code&gt; 这些命令输出的又是 ip 和 node id, k8s 环境下我更关心实际的 pod name.&lt;/p&gt;&#xA;&lt;p&gt;做 failover 的时候又不是通过 &lt;code&gt;--cluster&lt;/code&gt; 执行的, 必须连到 slave 上通过 &lt;code&gt;cluster failover&lt;/code&gt; 来执行&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;传参在 k8s 环境下特别麻烦&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;举个例子, 添加节点:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;redis-cli --cluster add-node new_host:new_port existing_host:existing_port&#xA;    --cluster-slave --cluster-master-id &amp;lt;arg&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;需要知道操作 pod 的 ip, 如果要变成某个指定 pod 的 slave, 又要传 node id.&lt;/p&gt;&#xA;&lt;p&gt;在 k8s 环境下实际操作的时候流程就会变成:&lt;/p&gt;</description>
    </item>
    <item>
      <title>从 twemproxy 迁移到 redis cluster</title>
      <link>https://blog.monsterxx03.com/2020/07/08/%E4%BB%8E-twemproxy-%E8%BF%81%E7%A7%BB%E5%88%B0-redis-cluster/</link>
      <pubDate>Wed, 08 Jul 2020 17:08:40 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/07/08/%E4%BB%8E-twemproxy-%E8%BF%81%E7%A7%BB%E5%88%B0-redis-cluster/</guid>
      <description>&lt;p&gt;线上有个 redis 的缓存集群, 跑的还是 3.0, 前面套 twemproxy 做 sharding. 跑了好几年了都很稳定, 但一直有些很不爽的地方, 最近有点时间,决定&#xA;升级到redis 6, 并迁移到 redis cluster 方案.&lt;/p&gt;&#xA;&lt;h3 id=&#34;twemproxy-的工作模式&#34;&gt;twemproxy 的工作模式&lt;/h3&gt;&#xA;&lt;p&gt;twemproxy 的原理很简单, 后面运行 N 个 redis 实例, 应用连到 twemproxy, twemproxy 解析应用发过来的 redis protocol, 根据 key 做 hash, 打散到后面 N 个 redis 实例上.&lt;/p&gt;&#xA;&lt;p&gt;具体打散的方式可以是简单的 hash%N, 也可以用一致性 hash 算法. hash%N 的问题是, 增减节点的时候所有 cache 必然 miss.&lt;/p&gt;&#xA;&lt;p&gt;一致性 hash, 在实现的时候会先弄一个 size 很大的 hash ring(eg: 2^32),这里每个节点被叫做一个虚拟节点, 把虚拟节点的数目叫做 X, 然后将 N 个 redis 实例均匀分布到这个环上, 每个 redis 把它叫做实节点吧, 分配 key&#xA;的时候 hash%X, 得到虚拟节点, 然后顺时针找下一个最近的实节点, 就找到了相应的 redis. 因为虚拟节点的数目是不变的, 增减 redis 实例的数目是改变了实节点的分布, 顺时针找下个实节点的时候还是有一定几率落在以前的&#xA;redis 实例上的, 这在一定程度上减少了 cache 的 miss. 可以看作 hash%N 的优化版本,但不解决本质问题.&lt;/p&gt;</description>
    </item>
    <item>
      <title>解决 k8s 1.16 apiVersion deprecation 造成的 helm revision 冲突</title>
      <link>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</link>
      <pubDate>Tue, 16 Jun 2020 16:02:58 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/06/16/%E8%A7%A3%E5%86%B3-k8s-1.16-apiversion-deprecation-%E9%80%A0%E6%88%90%E7%9A%84-helm-revision-%E5%86%B2%E7%AA%81/</guid>
      <description>&lt;p&gt;最近开始把线上的 k8s 从 1.15 升级到 1.16, 1.16 里有一些 api verison 被彻底废弃, 需要迁移到新的 api version, 具体有: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md#deprecations-and-removals&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;有两个问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;集群中使用的一些第三方 controller(nginx-ingress, external-dns-controller&amp;hellip;), 调用的 apiVersion 需要升级.&lt;/li&gt;&#xA;&lt;li&gt;已存在集群中的 objects(Deployment/ReplicaSet&amp;hellip;), 是否需要处理, eg: Deployment: extensions/v1beta1 -&amp;gt; apps/v1.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;第一个问题好解决, 升级一下对应的 image 版本就行了, 只要还在维护的 controller, 都已经升级到支持 1.16. 自己写的工具链也排查下是否有还在使用老版本 api 的, 因为我用的是 aws eks, 开下 &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html&#34;&gt;control-plane-logs&lt;/a&gt; 里的 audit log, 可以看还有什么在调用老的 api.&lt;/p&gt;&#xA;&lt;p&gt;第二个问题是不需要改, 已存在的 objects 无法修改 apiVersion, 集群升级到 1.16 后， 从新的 apiVersion 里能 pull 到之前的数据, apiVersion 字段自动就升级了.&lt;/p&gt;</description>
    </item>
    <item>
      <title>kube-scheduler internal</title>
      <link>https://blog.monsterxx03.com/2019/08/02/kube-scheduler-internal/</link>
      <pubDate>Fri, 02 Aug 2019 16:30:10 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/08/02/kube-scheduler-internal/</guid>
      <description>&lt;p&gt;追了一下 kube-scheduler 的源码, 记录一点, 基于 tag &lt;code&gt;v1.16.0-alpha.2&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;一句话概括 kube-scheduler 的职责是: 找到 pending 的 pod, 挑选一个合适的 node, 将 pod bind 上去.&lt;/p&gt;&#xA;&lt;h2 id=&#34;get-pending-pod&#34;&gt;Get pending pod&lt;/h2&gt;&#xA;&lt;p&gt;在 scheduler 的初始化过程中给 &lt;code&gt;pod/node/pv/pvc/service/storageClassInformer&lt;/code&gt; 添加回调函数, 功能大致都是在这些资源发生变化时更新本地的 cache 和 ScheduleQueue &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.0-alpha.2/pkg/scheduler/scheduler.go#L207&#34;&gt;scheduler.go:New&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;ScheduleQueue 是关键, 内部实现是一个 PriorityQueue, 它有三个 sub queue:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;activeQ 用来存放等待 schedule 的 pods, kube-schedule 实际工作时候从这个 queue 中取 pod, 实现上是一个 heap, 如果 pod 定义了 priority, 则按照 priority 由高到低排序, 否则按 pod 到达的时间排序: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.0-alpha.2/pkg/scheduler/internal/queue/scheduling_queue.go#L154&#34;&gt;activeQComp&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;podBackoffQ 存放正在经历 backoff 的 pod, 也是 heap, 按 pod 上次 backoff 的时间排序: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/v1.16.0-alpha.2/pkg/scheduler/internal/queue/scheduling_queue.go#L651&#34;&gt;podsCompareBackoffCompleted&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;unschedulableQ 存放 schedule 失败的 pod, 只需要能根据 pod 的 identity (&lt;code&gt;podName_namespace&lt;/code&gt;) 找到 pod 就行, 不需要排序, 内部是个 map.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;由 podInformer 的 eventHandler 将新的 pod 加到 activeQ 中.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pyflame 的 kubectl plugin</title>
      <link>https://blog.monsterxx03.com/2019/07/28/pyflame-%E7%9A%84-kubectl-plugin/</link>
      <pubDate>Sun, 28 Jul 2019 12:47:23 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/07/28/pyflame-%E7%9A%84-kubectl-plugin/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/uber/pyflame&#34;&gt;pyflame&lt;/a&gt; 可以比较方便得生成 python 进程的调用函数栈火焰图, 来 debug&#xA;一些性能瓶颈, 做了个 kubectl 的小插件, 来方便得对 k8s pod 中的 python 进程进行 debug: &lt;a href=&#34;https://github.com/monsterxx03/kube-pyflame&#34;&gt;https://github.com/monsterxx03/kube-pyflame&lt;/a&gt;&#xA;直接把 svg 文件下载到本地.&lt;/p&gt;&#xA;&lt;p&gt;要对 pod 中的 python 进程进行 profiling, 大致思路有两种:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;直接在 container 内使用 pyflame, 但这样要把 pyflame 做到所有的 base 镜像里去, 而且目标 container要在 SecurityContext 加上 &lt;code&gt;SYS_PTRACE&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;在 host 上用 pyflame debug 对应进程, pyflame 自身是能识别跑在 container 里的进程, 自动执行 &lt;code&gt;setns&lt;/code&gt; 的.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;我希望保持线上环境干净, 最后的做法是, 把 pyflame 单独做一个镜像, 先用 kubectl 找到目标 pod 所在节点, 然后用 nodeSelector 在对应节点上起一个&#xA;pyflame 的 pod, 因为要能看到其他 namespace 的进程, 需要设置 &lt;code&gt;hostPID: true&lt;/code&gt;, pyflame 要能执行 &lt;code&gt;setns&lt;/code&gt;, 这个 debug pod 要设置 &lt;code&gt;privileged: true&lt;/code&gt;.&#xA;执行完成后把 svg 下载下来, 并删除 debug pod.&lt;/p&gt;</description>
    </item>
    <item>
      <title> 迁移到 k8s 过程中碰到的问题</title>
      <link>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Tue, 23 Jul 2019 12:32:08 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &amp;hellip;&amp;hellip; 记录一下(大多都不是 k8s 的问题).&lt;/p&gt;&#xA;&lt;h2 id=&#34;nginx-ingress-controller-的问题&#34;&gt;nginx ingress controller 的问题&lt;/h2&gt;&#xA;&lt;h3 id=&#34;zero-downtime-pods-upgrade&#34;&gt;zero-downtime pods upgrade&lt;/h3&gt;&#xA;&lt;p&gt;默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip),&#xA;和直接使用 service cluster ip 比, 好处是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可以支持 sticky session&lt;/li&gt;&#xA;&lt;li&gt;可以用 round robin 之外的负载均衡算法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip&#xA;list(nginx 内置的 lua 来修改内存中的 ip list)&lt;/p&gt;&#xA;&lt;p&gt;这样的问题是, 从 pod 被干掉到 nginx 更新之间有个时间差, 部分请求会挂掉, 解决方法可以给 pod 设置一个 preStop hook, sleep 几秒, 等 nginx ingress controller&#xA;更新完成.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8S: 剩下的问题</title>
      <link>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 30 Jun 2019 16:11:06 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>&lt;p&gt;准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程,&#xA;但需要持续关注.&lt;/p&gt;&#xA;&lt;h2 id=&#34;dns-timeout-and-conntrack&#34;&gt;DNS timeout and conntrack&lt;/h2&gt;&#xA;&lt;p&gt;看到有个关于 DNS 的issue: &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/56903&#34;&gt;#56903&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候,&#xA;要通过 iptables 把　coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在&#xA;conntrack 内记录映射关系.&lt;/p&gt;&#xA;&lt;p&gt;这可能会带来两个问题:&lt;/p&gt;&#xA;&lt;h3 id=&#34;conntrack-table-被-udp-的-dns-查询填满&#34;&gt;conntrack table 被 udp 的 dns 查询填满&lt;/h3&gt;&#xA;&lt;p&gt;udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默认 30s(&lt;code&gt;net.netfilter.nf_conntrack_udp_timeout&lt;/code&gt;)&#xA;短时间内大量 udp 查询可能填满 conntrack, 导致丢包.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Centralized Logging on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</link>
      <pubDate>Sun, 26 May 2019 14:27:38 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</guid>
      <description>&lt;p&gt;搞定了监控, 下一步在 k8s 上要做的是中心化日志, 大体看了下, 感兴趣的有两个选择: ELK 套件, 或fluent-bit + fluentd.&lt;/p&gt;&#xA;&lt;p&gt;ELK 那套好处是, 可以把监控和日志一体化, filebeat 收集日志, metricbeat 收集 metrics, 统一存储在 ElasticSearch 里, 通过第三方项目&lt;a href=&#34;https://github.com/Yelp/elastalert&#34;&gt;elastalert&lt;/a&gt;&#xA;可以做报警，也能在 kibana 里集成界面. 坏处是 ElasticSearch 存储成本高, 吃资源. 我们对存储的日志使用需求基本就是 debug, 没有特别复杂的BI需求, 上一整套 ELK 还是太重了.&lt;/p&gt;&#xA;&lt;p&gt;选择 fluent-bit + fluentd 还有个好处是, 之前内部有套收集 metrics(用于统计 DAU, retention 之类指标) 的系统本来就是基于 fluentd 的, 用这套就不用改 metrics 那边的 ETL 了.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prometheus on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</link>
      <pubDate>Tue, 14 May 2019 13:52:02 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</guid>
      <description>&lt;h1 id=&#34;why-move-to-prometheus&#34;&gt;Why move to prometheus?&lt;/h1&gt;&#xA;&lt;p&gt;把生产环境迁移到 k8s 的第一步是要搞定监控, 目前线上监控用的是商业的 datadog, 在 container 环境下&#xA;datadog 监控还要按 container 数目收费, 单 host 只有 10 个的额度, 超过要加钱, 高密度部署下很不划算.&#xA;一个 server 跑 20 个以上 container 是很正常的事情, 单台 server 的监控费用立马翻倍.&lt;/p&gt;&#xA;&lt;p&gt;tracing 这块之前用的也是 datadog, 但太贵了,一直也想换开源实现, 索性监控报警也换了, 踩一把坑吧.&lt;/p&gt;&#xA;&lt;p&gt;vendor lock 总是不爽的&amp;hellip;&lt;/p&gt;&#xA;&lt;h1 id=&#34;metrics-in-k8s&#34;&gt;Metrics in k8s&lt;/h1&gt;&#xA;&lt;p&gt;先不提 prometheus, k8s 中 metrics 来源有那么几个:&lt;/p&gt;&#xA;&lt;h2 id=&#34;metrics-serever&#34;&gt;metrics-serever&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/metrics-server&#34;&gt;metrics-server&lt;/a&gt; (取代 heapster), 从 node&#xA;上 kubelet 的 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/stats/v1alpha1/types.go&#34;&gt;summary api&lt;/a&gt; 抓取数据(node/pod 的 cpu/memory 信息), kubectl top 和 kube-dashboard 的 metrics 数据来源就是它, horizontal pod&#xA;autoscaler 做 scale up/down 决策的数据来源也是它, metrics-server 只在内存里保留 node 和 pod 的最新值.&lt;/p&gt;</description>
    </item>
    <item>
      <title>kubeconfig 和 aws named profile 管理的 tips</title>
      <link>https://blog.monsterxx03.com/2019/05/07/kubeconfig-%E5%92%8C-aws-named-profile-%E7%AE%A1%E7%90%86%E7%9A%84-tips/</link>
      <pubDate>Tue, 07 May 2019 18:36:38 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/07/kubeconfig-%E5%92%8C-aws-named-profile-%E7%AE%A1%E7%90%86%E7%9A%84-tips/</guid>
      <description>&lt;p&gt;我有两个 EKS 集群 (sandbox + production), 这两个集群分处两个 aws 帐号中.&#xA;所以管理的时候也需要两套 aws credential.&lt;/p&gt;&#xA;&lt;p&gt;同时我用 &lt;a href=&#34;https://github.com/futuresimple/helm-secrets&#34;&gt;helm-secrets&lt;/a&gt; 来管理 helm charts&#xA;中需要加密的一些配置. helm-secrets 只是 &lt;a href=&#34;https://github.com/mozilla/sops&#34;&gt;sops&lt;/a&gt; 的一个 shell&#xA;wrapper, 实际加密是通过 sops 进行的.&lt;/p&gt;&#xA;&lt;p&gt;sops 支持 aws KMS, gcp KMS, azure key vault.. 等加密服务. 我用的是 aws KMS, 在 KMS 里创建一个 key,&#xA;授权允许我这个 iam 帐号能用它来进行加解密.&lt;/p&gt;&#xA;&lt;p&gt;这带来了一个问题, kubectl 和 helm-secrets 都需要 aws credential, 如果两边用的不一样就会执行失败.&lt;/p&gt;&#xA;&lt;p&gt;我统一使用 aws 的 &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html&#34;&gt;named profiles&lt;/a&gt;&#xA;来管理 credential. 不在环境变量里设 aws 的 access key/secret key(如果设置了, 优先级比 named profile 高)&lt;/p&gt;&#xA;&lt;p&gt;目录结构:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jenkins on K8S</title>
      <link>https://blog.monsterxx03.com/2019/04/29/jenkins-on-k8s/</link>
      <pubDate>Mon, 29 Apr 2019 15:56:12 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/04/29/jenkins-on-k8s/</guid>
      <description>&lt;p&gt;最近在把 jenkins 迁移到 k8s, 具体怎么 setup 的不赘述了(helm chart, jenkins home 目录挂pvc, jenkins kubernetes-plugin).&lt;/p&gt;&#xA;&lt;p&gt;jenkins 跑 k8s 好处是可以方便得做分布式 build, 每次 trigger 一个 job 的时候自动起一个 pod 作为 jenkins slave agent, 结束了自动删掉.&#xA;在 aws 上结合 cluster-autoscaler 可以极大得扩展 ci 的并行能力, 降低成本.&lt;/p&gt;&#xA;&lt;p&gt;记录一点过程中的坑.&lt;/p&gt;&#xA;&lt;p&gt;装上 &lt;a href=&#34;https://github.com/jenkinsci/kubernetes-plugin&#34;&gt;kubernetes-plugin&lt;/a&gt; 后,要想让 jenkins 的 job 在 pod 中跑, 必须用 pipeline 的方式编写 job 定义. script 和 declarative&#xA;两种方式都支持启动 pod. 如果用 shell 的方式编写, 不会跑在 pod 里，会直接在　master 的　workspace 里 build.&lt;/p&gt;&#xA;&lt;p&gt;declarative 方式的例子:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;pipeline {&#xA;    agent {&#xA;        kubernetes  {&#xA;            label &#39;test-deploy&#39;&#xA;            yamlFile &#39;test-deploy.yaml&#39;&#xA;        }&#xA;    }&#xA;    stages {&#xA;        stage(&#39;stage test&#39;) {&#xA;            steps(&#39;tests&#39;) {&#xA;                container(&#39;test&#39;) {&#xA;                    sh &#39;ls&#39;&#xA;                }&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;ls&lt;/code&gt; 命令就会在 test container 里执行, 如果不用 &lt;code&gt;container()&lt;/code&gt; step 的话，默认会在 pod 的 default container 里执行.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K8s Volume Resize on EKS</title>
      <link>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</link>
      <pubDate>Fri, 12 Apr 2019 13:23:54 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/04/12/k8s-volume-resize-on-eks/</guid>
      <description>&lt;p&gt;从 k8s 1.8 开始支持 &lt;a href=&#34;https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/&#34;&gt;PersistentVolumeClaimResize&lt;/a&gt;. 但 api 是 alpha 状态, 默认不开启, eks launch&#xA;的时候版本是 1.10, 因为没法改 control plane, 所以没法直接在 k8s 内做 ebs 扩容. 后来升级到了&#xA;1.11, 这个 feature 默认被打开了, 尝试了下直接在 EKS 内做 ebs 的扩容.&lt;/p&gt;&#xA;&lt;p&gt;注意:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这个 feature 只能对通过 pvc 管理的 volume 做扩容, 如果直接挂的是 pv, 只能自己按传统的 ebs 扩容流程在 eks 之外做.&lt;/li&gt;&#xA;&lt;li&gt;用来创建 pvc 的 storageclass 上必须设置 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 为 true&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在 eks 上使用 pv/pvc,　对于需要 retain 的 volume, 我一般的流程是:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在 eks 之外手工创建 ebs volume.&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pv, 指向 ebs 的 volume id&lt;/li&gt;&#xA;&lt;li&gt;在 eks 中创建 pvc, 指向 pv&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;示例 yaml:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---&#xA;kind: PersistentVolume&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test&#xA;spec:&#xA;  storageClassName: gp2&#xA;  persistentVolumeReclaimPolicy: Retain&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  capacity:&#xA;    storage: 5Gi&#xA;  awsElasticBlockStore:&#xA;    fsType: ext4&#xA;    volumeID: vol-xxxx   # create in aws manually&#xA;---&#xA;kind: PersistentVolumeClaim&#xA;apiVersion: v1&#xA;metadata:&#xA;  name: test-claim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  volumeName: test&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;假如现在要扩容到 10Gi, 流程是:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes 中的 pod 调度</title>
      <link>https://blog.monsterxx03.com/2018/12/16/kubernetes-%E4%B8%AD%E7%9A%84-pod-%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Sun, 16 Dec 2018 13:09:20 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/12/16/kubernetes-%E4%B8%AD%E7%9A%84-pod-%E8%B0%83%E5%BA%A6/</guid>
      <description>&lt;p&gt;定义 pod 的时候通过添加 node selector 可以让 pod 调度到有特定 label 的 node 上去, 这是最简单的调度方式.&#xA;其他还有更复杂的调度方式: node-taints/tolerations, node-affinity, pod-affinity, 来达到让某些类型的 pod 调度到一起,&#xA;让某些类型的 pod 不跑一起的效果.&lt;/p&gt;&#xA;&lt;h2 id=&#34;taints-and-tolerations&#34;&gt;Taints and Tolerations&lt;/h2&gt;&#xA;&lt;p&gt;如果 node 有 taints, 那只有能 tolerate 这些 taints 的 pod 才能调度到上面.&lt;/p&gt;&#xA;&lt;p&gt;taint 的基本格式是: &lt;code&gt;&amp;lt;key&amp;gt;&amp;lt;operator&amp;gt;&amp;lt;value&amp;gt;:&amp;lt;effect&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;kubectl describe node xxx&lt;/code&gt; 可以看到节点的 taints, 比如 master 节点上会有:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Taints:       node-role.kubernetes.io/master:NoSchedule &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;这里 key 是 &lt;code&gt;node-role.kubernetes.io/master&lt;/code&gt;, 没有等号和 value, operator 就是 &lt;code&gt;Exists&lt;/code&gt; , effect 是 &lt;code&gt;NoSchedule&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;master 节点上的这条 taint 就定义了只有能 tolerate 它的 pod 能调度到上面, 一般都是些系统 pod.&lt;/p&gt;&#xA;&lt;p&gt;比如看下 kube-proxy: &lt;code&gt;kubectl describe pod kube-proxy-efiv -n kube-system&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>EkS 评测 part-3</title>
      <link>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</link>
      <pubDate>Wed, 26 Sep 2018 10:16:42 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</guid>
      <description>&lt;p&gt;这篇记录对 ingress 的测试.&lt;/p&gt;&#xA;&lt;p&gt;ingress 用来将外部流量导入　k8s 内的　service. 将 service 的类型设置为 LoadBalancer / NodePort 也可以将单个 service 暴露到公网, 但用 ingress 可以只使用一个公网入口,根据　host name 或　url path 来将请求分发到不同的 service.&lt;/p&gt;&#xA;&lt;p&gt;一般　k8s 内的资源都会由一个 controller 来负责它的状态管理, 都由 kube-controller-manager 负责，　但 ingress controller 不是它的一部分，需要是视情况自己选择合适的 ingress controller.&lt;/p&gt;&#xA;&lt;p&gt;在 eks 上我主要需要 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;ingress-nginx&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&#34;&gt;aws-alb-ingress-controller&lt;/a&gt;. 注意, nginx inc 还维护一个 &lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress&#34;&gt;kubernetes-ingress&lt;/a&gt;, 和官方那个不是一个东西， 没测试过.&lt;/p&gt;&#xA;&lt;p&gt;这里主要只测试了 ingress-nginx, 看了下内部实现, 数据的转发真扭曲&amp;hellip;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>eks 评测 part-2</title>
      <link>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</link>
      <pubDate>Fri, 21 Sep 2018 10:28:17 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</guid>
      <description>&lt;p&gt;上文测试了一下 EKS 和 cluster autoscaler, 本文记录对 persisten volume 的测试.&lt;/p&gt;&#xA;&lt;h1 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h1&gt;&#xA;&lt;p&gt;创建 gp2 类型的 storageclass, 并用 annotations 设置为默认 sc, dynamic volume provision 会用到:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kind: StorageClass&#xA;apiVersion: storage.k8s.io/v1&#xA;metadata:&#xA;    name: gp2&#xA;    annotations:&#xA;        storageclass.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&#xA;provisioner: kubernetes.io/aws-ebs&#xA;reclaimPolicy: Retain&#xA;parameters:&#xA;    type: gp2&#xA;    fsType: ext4&#xA;    encrypted: &amp;quot;true&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;因为 eks 是基于 1.10.3 的, volume expansion 还是 alpha 状态, 没法自动开启(没法改 api server 配置), 所以 storageclass 的 allowVolumeExpansion, 设置了也没用.&#xA;这里 &lt;code&gt;encrypted&lt;/code&gt; 的值必须是字符串, 否则会创建失败, 而且报错莫名其妙.&lt;/p&gt;&#xA;&lt;h2 id=&#34;创建-pod-的时候指定一个已存在的-ebs-volume&#34;&gt;创建 pod 的时候指定一个已存在的 ebs volume&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;    name: test&#xA;spec:&#xA;    volumes:&#xA;        - name: test&#xA;          awsElasticBlockStore:&#xA;              fsType: ext4&#xA;              volumeID: vol-03670d6294ccf29fd&#xA;    containers:&#xA;        - image: nginx&#xA;          name: nginx&#xA;          volumeMounts:&#xA;              - name: test&#xA;                mountPath: /mnt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;kubectl -it test -- /bin/bash&lt;/code&gt;  进去看一下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@test:/# df -h&#xA;Filesystem      Size  Used Avail Use% Mounted on&#xA;overlay          20G  2.2G   18G  11% /&#xA;tmpfs           3.9G     0  3.9G   0% /dev&#xA;tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup&#xA;/dev/xvdcz      976M  2.6M  907M   1% /mnt&#xA;/dev/xvda1       20G  2.2G   18G  11% /etc/hosts&#xA;shm              64M     0   64M   0% /dev/shm&#xA;tmpfs           3.9G   12K  3.9G   1% /run/secrets/kubernetes.io/serviceaccount&#xA;tmpfs           3.9G     0  3.9G   0% /sys/firmware&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;那块 volume 的确绑定在 &lt;code&gt;/mnt&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>EKS 评测</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>&lt;p&gt;EKS 正式 launch 还没有正经用过, 最近总算试了一把, 记录一点.&lt;/p&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;p&gt;AWS 官方的 Guide 只提供了一个 cloudformation template 来设置 worker node, 我喜欢用 terraform, 可以跟着这个文档尝试:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html 来设置完整的 eks cluster 和管理 worker node 的 autoscaling  group.&lt;/p&gt;&#xA;&lt;p&gt;设置完 EKS 后需要添加一条 ConfigMap:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: aws-auth&#xA;  namespace: kube-system&#xA;data:&#xA;  mapRoles: |&#xA;    - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole&#xA;      username: system:node:{{EC2PrivateDNSName}}&#xA;      groups:&#xA;        - system:bootstrappers&#xA;        - system:nodes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;这样 worker node 节点才能加入集群.&lt;/p&gt;&#xA;&lt;h2 id=&#34;网络&#34;&gt;网络&lt;/h2&gt;&#xA;&lt;p&gt;之前一直没有在 AWS 上尝试构建 k8s 的一个原因, 就是不喜欢 overlay 网络, 给系统带来了额外的复杂度和管理开销, VPC flowlog 看不到 pod 之间流量, 封包后 tcpdump 不好 debug 应用层流量.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes in Action Notes</title>
      <link>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</link>
      <pubDate>Mon, 03 Sep 2018 18:20:46 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</guid>
      <description>&lt;p&gt;Miscellaneous notes when reading &lt;code&gt;&amp;lt;Kubernetes in Action&amp;gt;&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;api-group-and-api-version&#34;&gt;api group and api version&lt;/h2&gt;&#xA;&lt;p&gt;core api group need&amp;rsquo;t specified in &lt;code&gt;apiVersion&lt;/code&gt; field.&lt;/p&gt;&#xA;&lt;p&gt;For example, &lt;code&gt;ReplicationController&lt;/code&gt; is on core api group, so only:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ReplicationController&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;ReplicationSet&lt;/code&gt; is added later in &lt;code&gt;app&lt;/code&gt; group, &lt;code&gt;v1beta2&lt;/code&gt; version (k8s v1.8):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta2            1&#xA;kind: ReplicaSet           &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&#34;&gt;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;replicationcontroller-vs-replicationset&#34;&gt;ReplicationController VS ReplicationSet&lt;/h3&gt;&#xA;&lt;p&gt;ReplicationController is replaced by ReplicationSet, which has more expressive pod selectors.&lt;/p&gt;&#xA;&lt;p&gt;ReplicationController&amp;rsquo;s label selector only allows matching pods that include a certain label, ReplicationSet can&#xA;meet multi labels at same time.&lt;/p&gt;&#xA;&lt;p&gt;rs also support operator on key value: &lt;code&gt;In, NotIn, Exists, DoesNotExist&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;If migrate from rc to rs, can delete rc with &lt;code&gt;--cascade=false&lt;/code&gt; option, it will delete&#xA;rc only, but left pods running, then we can create a rs with same selector to make pods under management.&lt;/p&gt;&#xA;&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;&#xA;&lt;p&gt;DaemonSet ensures pod run exact one copy on one node, useful for processes like monitor agent and log collector. Use &lt;code&gt;node-selector&lt;/code&gt;&#xA;to make ds only run on specific nodes.&lt;/p&gt;&#xA;&lt;p&gt;If node is made unschedulable, normal pods won&amp;rsquo;t be scheduled to deploy on them, but ds will still be deployed to it, since ds will bypass&#xA;scheduler.&lt;/p&gt;&#xA;&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;&#xA;&lt;p&gt;Job is used to run a single completable task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS 的 K8S CNI Plugin</title>
      <link>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</link>
      <pubDate>Mon, 09 Apr 2018 15:28:38 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</guid>
      <description>&lt;p&gt;EKS 还没有 launch, 但 AWS 先开源了自己的 CNI 插件, 简单看了下, 说说它的实现和其他 K8S 网络方案的差别.&lt;/p&gt;&#xA;&lt;p&gt;K8S 集群对网络有几个基本要求:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;container 之间网络必须可达，且不通过 NAT&lt;/li&gt;&#xA;&lt;li&gt;所有 node 必须可以和所有 container 通信, 且不通过 NAT&lt;/li&gt;&#xA;&lt;li&gt;container 自己看到的 IP, 必须和其他 container 看到的它的 ip 相同.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;flannel-in-vpc&#34;&gt;Flannel in VPC&lt;/h2&gt;&#xA;&lt;p&gt;flannel 是 K8S 的一个 CNI 插件, 在 VPC 里使用 flannel 的话, 有几个选择:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;通过 VXLAN/UDP 进行封包, 封包影响网络性能, 而且不好 debug&lt;/li&gt;&#xA;&lt;li&gt;用 aws vpc backend, 这种方式会把每台主机的 docker 网段添加进 vpc routing table, 但默认 routing table 里只能有50条规则, 所以只能 50 个 node, 可以发 ticket 提升, 但数量太多会影响 vpc 性能.&lt;/li&gt;&#xA;&lt;li&gt;host-gw, 在每个 node 上直接维护集群中所有节点的路由, 没测试过, 感觉出问题也很难 debug, 假如用 autoscaling group 管理 node 集群, 能否让 K8S 在 scale in/out 的时候修改所有节点的路由?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;以上方式都只能利用 EC2 上的单网卡, security group 也没法作用在 pod 上.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
