<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Book on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/book/</link>
    <description>Recent content in Book on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Tue, 12 Feb 2019 13:08:29 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/book/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>管理负载</title>
      <link>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</link>
      <pubDate>Tue, 12 Feb 2019 13:08:29 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/</guid>
      <description>&lt;p&gt;最近在看 google 的 &lt;code&gt;&amp;lt;The Site Reliablity Workbook&amp;gt;&lt;/code&gt;, 其中有一章是&amp;quot;Manage load&amp;quot;, 内容还挺详细的, 结合在 aws 上的经验做点笔记.&lt;/p&gt;&#xA;&lt;h2 id=&#34;load-balancing&#34;&gt;Load Balancing&lt;/h2&gt;&#xA;&lt;p&gt;流量的入口是负载均衡, 最最简单的做法是在 DNS 上做 round robin, 但这样很依赖 client, 不同的 client 可能不完全遵守 DNS 的 TTL, 当地的 ISP 也会有缓存.&lt;/p&gt;&#xA;&lt;p&gt;google 用 anycast 技术在自己的网络中通过 BGP 给一个域名发布多个 endpoint, 共享一个 vip(virtual ip), 通过 BGP routing 来将用户的数据包发送到最近的 frontend server, 以此来减少 latency.&lt;/p&gt;&#xA;&lt;p&gt;但只依赖 BGP 会带来两个问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;某个地区的用户过多会给最近的 frontend server 带来过高的负载&lt;/li&gt;&#xA;&lt;li&gt;ISP 的 BGP 路由会重计算, 当 BGP routing 变化后, 进行中的 tcp connection 会被 reset(同一个 connection 上的后续数据包被发送到不同的 server, tcp session 不存在于新 server 上)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;为了解决原生 BGP anycast 的问题, google 开发了 Maglev, 即使路由发生了变化(routing flap), connection 也不断开, 把这种方式叫做 stablized anycast.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes in Action Notes</title>
      <link>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</link>
      <pubDate>Mon, 03 Sep 2018 18:20:46 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</guid>
      <description>&lt;p&gt;Miscellaneous notes when reading &lt;code&gt;&amp;lt;Kubernetes in Action&amp;gt;&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;api-group-and-api-version&#34;&gt;api group and api version&lt;/h2&gt;&#xA;&lt;p&gt;core api group need&amp;rsquo;t specified in &lt;code&gt;apiVersion&lt;/code&gt; field.&lt;/p&gt;&#xA;&lt;p&gt;For example, &lt;code&gt;ReplicationController&lt;/code&gt; is on core api group, so only:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ReplicationController&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;ReplicationSet&lt;/code&gt; is added later in &lt;code&gt;app&lt;/code&gt; group, &lt;code&gt;v1beta2&lt;/code&gt; version (k8s v1.8):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta2            1&#xA;kind: ReplicaSet           &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&#34;&gt;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;replicationcontroller-vs-replicationset&#34;&gt;ReplicationController VS ReplicationSet&lt;/h3&gt;&#xA;&lt;p&gt;ReplicationController is replaced by ReplicationSet, which has more expressive pod selectors.&lt;/p&gt;&#xA;&lt;p&gt;ReplicationController&amp;rsquo;s label selector only allows matching pods that include a certain label, ReplicationSet can&#xA;meet multi labels at same time.&lt;/p&gt;&#xA;&lt;p&gt;rs also support operator on key value: &lt;code&gt;In, NotIn, Exists, DoesNotExist&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;If migrate from rc to rs, can delete rc with &lt;code&gt;--cascade=false&lt;/code&gt; option, it will delete&#xA;rc only, but left pods running, then we can create a rs with same selector to make pods under management.&lt;/p&gt;&#xA;&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;&#xA;&lt;p&gt;DaemonSet ensures pod run exact one copy on one node, useful for processes like monitor agent and log collector. Use &lt;code&gt;node-selector&lt;/code&gt;&#xA;to make ds only run on specific nodes.&lt;/p&gt;&#xA;&lt;p&gt;If node is made unschedulable, normal pods won&amp;rsquo;t be scheduled to deploy on them, but ds will still be deployed to it, since ds will bypass&#xA;scheduler.&lt;/p&gt;&#xA;&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;&#xA;&lt;p&gt;Job is used to run a single completable task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing data intensive application, reading notes, Part 2</title>
      <link>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</link>
      <pubDate>Wed, 17 May 2017 09:12:44 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</guid>
      <description>&lt;p&gt;Chapter 4, 5, 6&lt;/p&gt;&#xA;&lt;h2 id=&#34;encoding-formats&#34;&gt;Encoding formats&lt;/h2&gt;&#xA;&lt;p&gt;xml, json, msgpack are text based encoding format, they can’t carry binary bytes (useless you encode them in base64, size grows 33%). And they cary schema definition with data, wast a lot of space.&lt;/p&gt;&#xA;&lt;p&gt;thrift, protobuf are binary format, can take binary bytes, only carry data, the schema is defined with IDL(interface definition language). They have code generation tool to generate code to encode and decode data, along with check. Every field of data is binded with a tag(mapped to a field in IDL file). If a field is defined is required, it can’t by removed or change tag value, otherwise old code will not be able to decode it.&lt;/p&gt;&#xA;&lt;p&gt;avro (used in hadoop), have a write schema and a read schema, when store a large file in avro format(contain many records with same schema), the avro write schama file is appended to the data. If use avro in RPC, the avro schema is exchanged during connection setup. When decoding avro, the lib will look both write schema and read schema, and translate write schema into read schema. Forward compatibility means that you can have a new version of the schema as writer and an old version of the schema as reader, backward compatibility means that you can have a new version of the schema as reader and an old version as writer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing data intensive application, reading notes, Part 1</title>
      <link>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</link>
      <pubDate>Thu, 04 May 2017 16:27:52 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</guid>
      <description>&lt;p&gt;Notes when reading chapter 2 “Data models and query languages”, chapter 3 “Storage and retrieval”&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concurrency in Go, Reading Notes</title>
      <link>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</link>
      <pubDate>Wed, 19 Apr 2017 16:26:58 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</guid>
      <description>&lt;p&gt;A few notes taken when reading &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
