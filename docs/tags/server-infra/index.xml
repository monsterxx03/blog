<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>server-infra on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/server-infra/</link>
    <description>Recent content in server-infra on Shining Moon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Thu, 01 Apr 2021 14:40:30 +0800</lastBuildDate><atom:link href="https://blog.monsterxx03.com/tags/server-infra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rolling Upgrade Worker Nodes in EKS</title>
      <link>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</link>
      <pubDate>Thu, 01 Apr 2021 14:40:30 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2021/04/01/rolling-upgrade-worker-nodes-in-eks/</guid>
      <description>EKS control plane çš„å‡çº§æ˜¯æ¯”è¾ƒç®€å•çš„, ç›´æ¥åœ¨ aws console ä¸Šç‚¹ä¸‹å°±å¯ä»¥äº†, ä½† worker node æ˜¯è‡ªå·±ç”¨ asg(autoscaling group) ç®¡ç†çš„, å‡çº§ worker node åˆä¸æƒ³å½±å“ä¸šåŠ¡æ˜¯æœ‰è®²ç©¶çš„. è·‘åœ¨ EKS é‡Œ, ä¸”å¸Œæœ›ä¸è¢«ä¸­æ–­ traffic çš„æœ‰: stateless çš„ api server, queue consumer è¢« redis sentinel ç›‘æ§ç€çš„ redis master/slave ç”¨äº cache çš„ redis cluster å†™äº†ä¸ªå†…éƒ¨å·¥å…·, æŠŠä¸‹é¢çš„æµç¨‹å…¨éƒ¨è‡ªåŠ¨åŒ–äº†. è¿™æ ·å‡çº§ eks ç‰ˆæœ¬, éœ€è¦æ›´æ¢ worker node æ—¶å€™å°±è½»æ¾å¤šäº†. å› ä¸ºè¿™ä¸ªå·¥å…·å¯¹éƒ¨ç½²æƒ…å†µåšäº†å¾ˆå¤šå‡è®¾å’Œé™åˆ¶, å¼€æºçš„ä»·å€¼ä¸æ˜¯å¾ˆå¤§. Stateless application stateless çš„åº”ç”¨å…¨éƒ¨ç”¨ deployment éƒ¨ç½². ä¸€èˆ¬å»ºè®®çš„æµç¨‹æ˜¯: ä¿®æ”¹ asg çš„ launch configuration, æŒ‡å‘æ–°ç‰ˆæœ¬</description>
    </item>
    
    <item>
      <title>ä» twemproxy è¿ç§»åˆ° redis cluster</title>
      <link>https://blog.monsterxx03.com/2020/07/08/%E4%BB%8E-twemproxy-%E8%BF%81%E7%A7%BB%E5%88%B0-redis-cluster/</link>
      <pubDate>Wed, 08 Jul 2020 17:08:40 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2020/07/08/%E4%BB%8E-twemproxy-%E8%BF%81%E7%A7%BB%E5%88%B0-redis-cluster/</guid>
      <description>çº¿ä¸Šæœ‰ä¸ª redis çš„ç¼“å­˜é›†ç¾¤, è·‘çš„è¿˜æ˜¯ 3.0, å‰é¢å¥— twemproxy åš sharding. è·‘äº†å¥½å‡ å¹´äº†éƒ½å¾ˆç¨³å®š, ä½†ä¸€ç›´æœ‰äº›å¾ˆä¸çˆ½çš„åœ°æ–¹, æœ€è¿‘æœ‰ç‚¹æ—¶é—´,å†³å®š å‡çº§åˆ°redis 6, å¹¶è¿ç§»åˆ° redis cluster æ–¹æ¡ˆ. twemproxy çš„å·¥ä½œæ¨¡å¼ twemproxy çš„åŸç†å¾ˆç®€å•, åé¢è¿è¡Œ N ä¸ª redis å®ä¾‹, åº”ç”¨è¿åˆ° twemproxy, twemproxy è§£æåº”ç”¨å‘è¿‡æ¥çš„ redis protocol, æ ¹æ® key åš hash, æ‰“æ•£åˆ°åé¢ N ä¸ª redis å®ä¾‹ä¸Š. å…·ä½“æ‰“æ•£çš„æ–¹å¼å¯ä»¥æ˜¯ç®€å•çš„ hash%N, ä¹Ÿå¯ä»¥ç”¨ä¸€è‡´æ€§ hash ç®—æ³•. hash%N çš„é—®é¢˜æ˜¯, å¢å‡èŠ‚ç‚¹çš„æ—¶å€™æ‰€æœ‰ cache å¿…ç„¶ miss. ä¸€è‡´æ€§ hash, åœ¨å®ç°çš„æ—¶å€™ä¼šå…ˆå¼„ä¸€ä¸ª size å¾ˆå¤§çš„ hash ring(eg: 2^32),</description>
    </item>
    
    <item>
      <title>ç¼–å†™ postmortem</title>
      <link>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</link>
      <pubDate>Sat, 18 Jan 2020 15:20:47 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2020/01/18/%E7%BC%96%E5%86%99-postmortem/</guid>
      <description>æˆåŠŸçš„ç»éªŒæ€»æ˜¯å¸¦æœ‰ç‚¹è¿æ°”æˆä»½, å¤±è´¥åˆ™æ˜¯å¿…ç„¶çš„:). å·¥ä½œä¸­ï¼Œ çº¿ä¸Šç¯å¢ƒçš„é—®é¢˜åƒå¥‡ç™¾æ€ª, æœ‰çš„æ¥è‡ªè‡ªå·±ä»£ç  bug, æœ‰çš„æ˜¯é…ç½®é”™è¯¯, æœ‰æ—¶å€™æ˜¯ç¬¬ä¸‰æ–¹çš„ vendor æˆäº†çŒªé˜Ÿå‹. å¯¹äºä¸€äº›æ’æŸ¥è¿‡ç¨‹æ¯”è¾ƒå›°éš¾æˆ–å…·æœ‰ä»£è¡¨æ€§çš„é—®é¢˜, éœ€è¦è®°å½•ä¸‹æ¥, ä¸€èˆ¬æŠŠè¿™ä¸ªè¿‡ç¨‹å«åš postmortem(éªŒå°¸). è¿™ç¯‡å†™ä¸€ä¸‹è‡ªå·±åš postmortem çš„è¿‡ç¨‹, å¹¶è®°å½•ä¸€ä¸ªæœ€è¿‘å¤„ç†çš„æ•…éšœ. Postmortem process æˆ‘å¤§ä½“åˆ†ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†: ç”¨å°½é‡ç®€ç»ƒçš„è¯­å¥æè¿°æ¸…æ¥šåœ¨ä»€ä¹ˆæ—¶é—´å‘ç”Ÿäº†ä»€ä¹ˆ,è°å‚ä¸äº†é—®é¢˜çš„å¤„ç†(wh</description>
    </item>
    
    <item>
      <title>èŠèŠ AWS çš„è®¡è´¹æ¨¡å¼</title>
      <link>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Mon, 30 Dec 2019 12:08:47 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/12/30/%E8%81%8A%E8%81%8A-aws-%E7%9A%84%E8%AE%A1%E8%B4%B9%E6%A8%A1%E5%BC%8F/</guid>
      <description>ç½‘ä¸Šç»å¸¸æœ‰äººè¯Ÿç—… AWS çš„è®¡è´¹æ¨¡å¼å¤æ‚, å–œæ¬¢å›½å†…é‚£ç§æ‰“åŒ…å¼çš„å”®å–æ–¹å¼, è¿™ä¸ªå¯èƒ½å—é™äºæ¯ä¸ªå…¬å¸çš„è´¢åŠ¡æµç¨‹, é¢„ç®—åˆ¶å®šæ–¹å¼, åˆä¸åˆå›½æƒ…,æœ¬æ–‡ä¸è®¨è®º. ä»…ä»å¼€å‘è€…çš„è§’åº¦ä»‹ç»ä¸‹ AWS éƒ¨åˆ†å¸¸ç”¨ service çš„è®¡è´¹æ–¹å¼. PS: é‚£äº›ä¸ºäº†è¹­ä¸€å¹´ free plan ç„¶åæŠ±æ€¨ä»€ä¹ˆå·è·‘æµé‡, å·å·æ‰£è´¹çš„å¤§å“¥å°±çœçœå§, AWS æ ¹æœ¬ä¸æ˜¯ç»™ä¸ªäººç”¨çš„, è€è€å®å®ç”¨ lightsail å¾—äº†. EC2 EC2 çš„ä»·æ ¼æ˜¯æœ€å¤æ‚çš„, ä¸€å° EC2 instance çš„ä»·æ ¼ç»„æˆ: instance fee, å®é™…æ”¯ä»˜çš„æ˜¯ CPU+RAM çš„è´¹ç”¨ EBS fee, server çš„æ ¹åˆ†åŒºéƒ½æ˜¯ EBS volume, æŒ‰ EBS è®¡è´¹(31GB çš„ volume ä½¿</description>
    </item>
    
    <item>
      <title>é›†æˆ opentracing</title>
      <link>https://blog.monsterxx03.com/2019/11/15/%E9%9B%86%E6%88%90-opentracing/</link>
      <pubDate>Fri, 15 Nov 2019 14:05:59 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/11/15/%E9%9B%86%E6%88%90-opentracing/</guid>
      <description>ä¹‹å‰ç”¨è¿‡ datadog çš„ tracing åŠŸèƒ½, éå¸¸å¥½ç”¨, ä½†æ˜¯å¾ˆè´µ(å•å°30$), è¿ç§»åˆ° k8s å, ç›‘æ§è¿ç§»åˆ°äº† prometheus, ä¹ŸæŠŠ datadog çš„ tracing å»æ‰äº†.datadog çš„ tracing ä¹Ÿæ˜¯ opentracing çš„ä¸€ç§å®ç°, ç´¢æ€§å°±æ¢ä¸Šå¼€æºå®ç°. tracing ç³»ç»Ÿæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¾ˆå¥½ç”¨çš„ performance tuning å·¥å…·, opentracing åªæ˜¯ä¸€ä¸ªæ ‡å‡†ï¼Œé‡Œé¢å®šä¹‰äº† span, scope, tracer ç­‰æ¦‚å¿µï¼Œä½†ä¸è§„å®š tracing æ•°æ®åº”è¯¥æ€ä¹ˆ encoding, æ€ä¹ˆå­˜å‚¨, è·¨è¿›ç¨‹çš„ span æ•°æ®æ€ä¹ˆä¸²èµ·æ¥. é¦–å…ˆè¦æŒ‘é€‰ä¸€ä¸ªå¼€æºçš„ tracer å®ç°ï¼Œtracer ç”¨æ¥æ¥å—ä¸šåŠ¡ç³»ç»Ÿå‘å‡ºçš„ encode è¿‡çš„ span æ•°æ®,å¹¶å­˜å‚¨ï¼Œæä¾›ä¸€ä¸ªç•Œé¢ä¾›æŸ¥è¯¢. æˆ‘é€‰</description>
    </item>
    
    <item>
      <title> è¿ç§»åˆ° k8s è¿‡ç¨‹ä¸­ç¢°åˆ°çš„é—®é¢˜</title>
      <link>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Tue, 23 Jul 2019 12:32:08 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>å¼€å§‹æŠŠçº¿ä¸Šæµé‡å¾€ k8s é›†ç¾¤é‡Œé¢å¯¼äº†, ä¸­é—´ç¢°åˆ°äº†èŒ«èŒ«å¤šçš„é—®é¢˜ &amp;hellip;&amp;hellip; è®°å½•ä¸€ä¸‹(å¤§å¤šéƒ½ä¸æ˜¯ k8s çš„é—®é¢˜). nginx ingress controller çš„é—®é¢˜ zero-downtime pods upgrade é»˜è®¤é…ç½®ä¸‹, nginx ingress controller çš„ upstream æ˜¯ service çš„ endpoints, åœ¨ eks é‡Œ, å°±æ˜¯ vpc cni plugin åˆ†é…ç»™ pod çš„ vpc ip(ä¸æ˜¯ cluster ip), å’Œç›´æ¥ä½¿ç”¨ service cluster ip æ¯”, å¥½å¤„æ˜¯: å¯ä»¥æ”¯æŒ sticky session å¯ä»¥ç”¨ round robin ä¹‹å¤–çš„è´Ÿè½½å‡è¡¡ç®—æ³• å…·ä½“å®ç°æ˜¯, å½“ service çš„ endpoint åˆ—è¡¨å‘ç”Ÿå˜åŒ–æ—¶, nginx ingress controller æ”¶åˆ°é€šçŸ¥, å¯¹å®ƒç®¡ç†çš„ nginx è¿›ç¨‹å‘èµ·ä¸€ä¸ª http request, æ›´æ–° endpoint ip list(nginx å†…ç½®çš„ lua æ¥ä¿®æ”¹å†…å­˜ä¸­çš„ ip list) è¿™æ ·çš„é—®é¢˜æ˜¯, ä» pod è¢«å¹²æ‰åˆ° nginx æ›´æ–°ä¹‹é—´</description>
    </item>
    
    <item>
      <title>K8S: å‰©ä¸‹çš„é—®é¢˜</title>
      <link>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 30 Jun 2019 16:11:06 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>å‡†å¤‡å·¥ä½œéƒ½å·®ä¸å¤šäº†, æ²¡æ„å¤–ä¸‹å‘¨å°±è¯¥å¼€å§‹æŠŠçº¿ä¸Šçš„æœåŠ¡å¾€ k8s ä¸Šè¿ç§»äº†. è®°å½•å‡ ä¸ªé—®é¢˜ï¼Œæš‚æ—¶ä¸ block æˆ‘çš„è¿ç§»è¿›ç¨‹, ä½†éœ€è¦æŒç»­å…³æ³¨. DNS timeout and conntrack çœ‹åˆ°æœ‰ä¸ªå…³äº DNS çš„issue: #56903 ç°è±¡æ˜¯ k8s cluster å†…éƒ¨ dns æŸ¥è¯¢é—´æ­‡æ€§ä¼š 5s è¶…æ—¶, å¤§è‡´åŸå› æ˜¯ coredns ä½œä¸ºä¸­å¿ƒ dns çš„æ—¶å€™, è¦é€šè¿‡ iptables æŠŠ coredns çš„ cluster ip, è½¬åŒ–åˆ°å®ƒçœŸå®çš„å¯è·¯ç”± ip, ä¸­é—´éœ€è¦ SNAT, DNAT, å¹¶åœ¨ conntrack å†…è®°å½•æ˜ å°„å…³ç³». è¿™å¯èƒ½ä¼šå¸¦æ¥ä¸¤ä¸ªé—®é¢˜: conntrack table è¢« udp çš„ dns æŸ¥è¯¢å¡«æ»¡ udp æ˜¯æ— è¿æ¥çš„, tcp å…³é—­é“¾æ¥å°±ä¼šæ¸…ç† conntrack å†…è®°å½•, udp ä¸ä¼šï¼Œåªèƒ½ç­‰è¶…æ—¶, é»˜</description>
    </item>
    
    <item>
      <title>Random Talk</title>
      <link>https://blog.monsterxx03.com/2019/05/30/random-talk/</link>
      <pubDate>Thu, 30 May 2019 18:48:23 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/05/30/random-talk/</guid>
      <description>Just some random complains and notes about server infra management. I think those are my motivations to move to kubernetes.
Won&amp;rsquo;t explain k8s or docker in detail, and how they solve those problems in this post.
Infrastructure level(on AWS) We use following services provided by AWS.
Compute: EC2 AutoScaling Group Lambda network: VPC (SDN network) DNS (route53) CDN (CloudFront) Loadbalancer: ELB (L4) NLB (L4, ELB successor, support static IP) ALB (L7) Storage: EBS (block storage) EFS (hosted NFS) RDS(MySQl/PostgreSQL &amp;hellip;) Redshift (data warehouse) DynamoDB (KV) S3 (object storage) Glacier (cheap archive storage) Web Firewall (WAF) Monitor (CloudWatch) DMS (ETL) &amp;hellip; For infra management, in early days, we just click, click, click&amp;hellip; or write some simple scripts to call AWS api.
With infra resources growing, management became complex, a concept called Infrastructure as Code rising.
AWS provides CloudFormation as orchestration tool, but we use terraform (for short: CloudFormation sucks, for long: Infrastructure as Code)
So far, not bad.(tweak those services internally is another story&amp;hellip; never belive work out of box)
Application level configuration management (setup nginx, jenkins, redis, twemproxy, ElasticSearch or WTF..) CI/CD dependency management They&amp;rsquo;re complicated, people developped bunch of tools to handle: puppet, chef, ansible, saltstack &amp;hellip;
They&amp;rsquo;re great and working, but writing correct code still a challenge when changes involves:</description>
    </item>
    
    <item>
      <title>Centralized Logging on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</link>
      <pubDate>Sun, 26 May 2019 14:27:38 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/05/26/centralized-logging-on-k8s/</guid>
      <description>æå®šäº†ç›‘æ§, ä¸‹ä¸€æ­¥åœ¨ k8s ä¸Šè¦åšçš„æ˜¯ä¸­å¿ƒåŒ–æ—¥å¿—, å¤§ä½“çœ‹äº†ä¸‹, æ„Ÿå…´è¶£çš„æœ‰ä¸¤ä¸ªé€‰æ‹©: ELK å¥—ä»¶, æˆ–fluent-bit + fluentd. ELK é‚£å¥—å¥½å¤„æ˜¯, å¯ä»¥æŠŠç›‘æ§å’Œæ—¥å¿—ä¸€ä½“åŒ–, filebeat æ”¶é›†æ—¥å¿—, metricbeat æ”¶é›† metrics, ç»Ÿä¸€å­˜å‚¨åœ¨ ElasticSearch é‡Œ, é€šè¿‡ç¬¬ä¸‰æ–¹é¡¹ç›®elastalert å¯ä»¥åšæŠ¥è­¦ï¼Œä¹Ÿèƒ½åœ¨ kibana é‡Œé›†æˆç•Œé¢. åå¤„æ˜¯ ElasticSearch å­˜å‚¨æˆæœ¬é«˜, åƒèµ„æº. æˆ‘ä»¬å¯¹å­˜å‚¨çš„æ—¥å¿—ä½¿ç”¨éœ€æ±‚åŸºæœ¬å°±æ˜¯ debug, æ²¡æœ‰ç‰¹åˆ«å¤æ‚çš„BIéœ€æ±‚, ä¸Šä¸€æ•´å¥— ELK è¿˜æ˜¯å¤ªé‡äº†. é€‰æ‹© fluent-bit + fluentd è¿˜æœ‰ä¸ªå¥½å¤„æ˜¯, ä¹‹å‰å†…éƒ¨æœ‰å¥—æ”¶é›† m</description>
    </item>
    
    <item>
      <title>Prometheus on K8S</title>
      <link>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</link>
      <pubDate>Tue, 14 May 2019 13:52:02 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/05/14/prometheus-on-k8s/</guid>
      <description>Why move to prometheus? æŠŠç”Ÿäº§ç¯å¢ƒè¿ç§»åˆ° k8s çš„ç¬¬ä¸€æ­¥æ˜¯è¦æå®šç›‘æ§, ç›®å‰çº¿ä¸Šç›‘æ§ç”¨çš„æ˜¯å•†ä¸šçš„ datadog, åœ¨ container ç¯å¢ƒä¸‹ datadog ç›‘æ§è¿˜è¦æŒ‰ container æ•°ç›®æ”¶è´¹, å• host åªæœ‰ 10 ä¸ªçš„é¢åº¦, è¶…è¿‡è¦åŠ é’±, é«˜å¯†åº¦éƒ¨ç½²ä¸‹å¾ˆä¸åˆ’ç®—. ä¸€ä¸ª server è·‘ 20 ä¸ªä»¥ä¸Š container æ˜¯å¾ˆæ­£å¸¸çš„äº‹æƒ…, å•å° server çš„ç›‘æ§è´¹ç”¨ç«‹é©¬ç¿»å€. tracing è¿™å—ä¹‹å‰ç”¨çš„ä¹Ÿæ˜¯ datadog, ä½†å¤ªè´µäº†,ä¸€ç›´ä¹Ÿæƒ³æ¢å¼€æºå®ç°, ç´¢æ€§ç›‘æ§æŠ¥è­¦ä¹Ÿæ¢äº†, è¸©ä¸€æŠŠå‘å§. vendor lock æ€»æ˜¯ä¸çˆ½çš„&amp;hellip; Metrics in k8s å…ˆä¸æ prometheus, k8s ä¸­ metrics æ¥æºæœ‰é‚£ä¹ˆå‡ ä¸ª: metrics-serever metrics-server (å–ä»£ heapster), ä» node ä¸Š kubelet çš„ summary api æŠ“å–</description>
    </item>
    
    <item>
      <title>Jenkins on K8S</title>
      <link>https://blog.monsterxx03.com/2019/04/29/jenkins-on-k8s/</link>
      <pubDate>Mon, 29 Apr 2019 15:56:12 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2019/04/29/jenkins-on-k8s/</guid>
      <description>æœ€è¿‘åœ¨æŠŠ jenkins è¿ç§»åˆ° k8s, å…·ä½“æ€ä¹ˆ setup çš„ä¸èµ˜è¿°äº†(helm chart, jenkins home ç›®å½•æŒ‚pvc, jenkins kubernetes-plugin). jenkins è·‘ k8s å¥½å¤„æ˜¯å¯ä»¥æ–¹ä¾¿å¾—åšåˆ†å¸ƒå¼ build, æ¯æ¬¡ trigger ä¸€ä¸ª job çš„æ—¶å€™è‡ªåŠ¨èµ·ä¸€ä¸ª pod ä½œä¸º jenkins slave agent, ç»“æŸäº†è‡ªåŠ¨åˆ æ‰. åœ¨ aws ä¸Šç»“åˆ cluster-autoscaler å¯ä»¥æå¤§å¾—æ‰©å±• ci çš„å¹¶è¡Œèƒ½åŠ›, é™ä½æˆæœ¬. è®°å½•ä¸€ç‚¹è¿‡ç¨‹ä¸­çš„å‘. è£…ä¸Š kubernetes-plugin å,è¦æƒ³è®© jenkins çš„ job åœ¨ pod ä¸­è·‘, å¿…é¡»ç”¨ pipeline çš„æ–¹å¼ç¼–å†™ job å®šä¹‰. script å’Œ declarative ä¸¤ç§æ–¹å¼éƒ½æ”¯æŒå¯åŠ¨ pod. å¦‚æœç”¨ shell çš„æ–¹å¼ç¼–å†™, ä¸ä¼šè·‘åœ¨ pod é‡Œï¼Œä¼šç›´æ¥åœ¨ master çš„ workspace é‡Œ build. declarative æ–¹å¼çš„ä¾‹å­: pipeline { agent { kubernetes { label &#39;test-deploy&#39; yamlFile &#39;test-deploy.yaml&#39;</description>
    </item>
    
    <item>
      <title>AWS Aurora DB</title>
      <link>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</link>
      <pubDate>Wed, 31 Oct 2018 15:23:45 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/10/31/aws-aurora-db/</guid>
      <description>æœ€è¿‘åœ¨æŠŠéƒ¨åˆ†ç”¨ RDS çš„ MySQL è¿ç§»åˆ° aurora ä¸Šå», è¯»äº†ä¸‹ aurora çš„ paper, é¡ºä¾¿å’Œ RDS çš„æ¶æ„åšäº›å¯¹æ¯”. Paper notes å­˜å‚¨è®¡ç®—åˆ†ç¦» redo log ä¸‹æ¨åˆ°å­˜å‚¨å±‚ å‰¯æœ¬: 6 å‰¯æœ¬ 3 AZ(2 per az), å¤±å»ä¸€ä¸ª AZ + 1 additoinal node ä¸ä¼šä¸¢æ•°æ®(å¯è¯»ä¸å¯å†™). å¤±å»ä¸€ä¸ª AZ (æˆ–ä»»æ„2 node) ä¸å½±å“æ•°æ®å†™å…¥. 10GB ä¸€ä¸ª segment, æ¯ä¸ª segment 6 å‰¯æœ¬ä¸€ä¸ª PG (protection group), ä¸€ AZ ä¸¤å‰¯æœ¬. åœ¨ 10Gbps çš„ç½‘ç»œä¸Š, ä¿®å¤ä¸€ä¸ª 10GB çš„segment éœ€è¦ 10s. MySQL ä¸€ä¸ªåº”ç”¨å±‚çš„å†™ä¼šåœ¨åº•å±‚äº§ç”Ÿå¾ˆå¤šé¢å¤–çš„å†™æ“ä½œï¼Œä¼šå¸¦æ¥å†™æ”¾å¤§é—®é¢˜: redo log ç”¨æ¥ crash recovery, binlog ä¼šä¸Šä¼  s3 ç”¨äº point in time restore. åœ¨ aurora é‡Œï¼Œåª</description>
    </item>
    
    <item>
      <title>ä¸º service åˆ¶å®š SLO</title>
      <link>https://blog.monsterxx03.com/2018/10/15/%E4%B8%BA-service-%E5%88%B6%E5%AE%9A-slo/</link>
      <pubDate>Mon, 15 Oct 2018 11:31:05 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/10/15/%E4%B8%BA-service-%E5%88%B6%E5%AE%9A-slo/</guid>
      <description>é€šå¸¸æˆ‘ä»¬ä½¿ç”¨äº‘æœåŠ¡çš„æ—¶å€™, æœåŠ¡æä¾›å•†ä¼šæä¾› SLA(Service Level Aggrement),ä½œä¸ºä»–ä»¬æä¾›çš„æœåŠ¡è´¨é‡çš„æ ‡å‡†(å¸¸è¯´çš„å‡ ä¸ª9),è¾¾ä¸åˆ°ä¼šè¿›è¡Œèµ”å¿. æ¯”å¦‚ AWS çš„è®¡ç®—ç±»æœåŠ¡: https://aws.amazon.com/compute/sla/ . å¯¹å…¬å¸è‡ªå·± host çš„ service, æˆ‘ä»¬å†…éƒ¨ä¹Ÿéœ€è¦ä¸€äº›æŠ€æœ¯æŒ‡æ ‡æ¥ track æˆ‘ä»¬ä¸ºå®¢æˆ·æä¾›çš„æœåŠ¡è´¨é‡å¦‚ä½•, è¿™ä¸ªå«åš SLO(Service Level Objective). ä¹Ÿå¯ä»¥æŠŠä»–å½“æˆä¸€ä¸ªå¯¹å†…çš„,æ²¡æœ‰èµ”å¿åè®®çš„SLA. å®šä¹‰æŒ‡æ ‡ æˆ‘ä¸»è¦ track ä¸¤ä¸ªæŒ‡æ ‡: Availability (æœåŠ¡çš„å¯ç”¨æ€§) Quality (æœåŠ¡è´¨é‡) Availability çš„å®šä¹‰, ä»¥å‰ç”¨ç®€å•çš„ service uptime æ¥å®šä¹‰, åœ¨é›†ç¾¤å¤–éƒ¨ç”¨ä¸€</description>
    </item>
    
    <item>
      <title>EkS è¯„æµ‹ part-3</title>
      <link>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</link>
      <pubDate>Wed, 26 Sep 2018 10:16:42 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/26/eks-%E8%AF%84%E6%B5%8B-part-3/</guid>
      <description>è¿™ç¯‡è®°å½•å¯¹ ingress çš„æµ‹è¯•. ingress ç”¨æ¥å°†å¤–éƒ¨æµé‡å¯¼å…¥ k8s å†…çš„ service. å°† service çš„ç±»å‹è®¾ç½®ä¸º LoadBalancer / NodePort ä¹Ÿå¯ä»¥å°†å•ä¸ª service æš´éœ²åˆ°å…¬ç½‘, ä½†ç”¨ ingress å¯ä»¥åªä½¿ç”¨ä¸€ä¸ªå…¬ç½‘å…¥å£,æ ¹æ® host name æˆ– url path æ¥å°†è¯·æ±‚åˆ†å‘åˆ°ä¸åŒçš„ service. ä¸€èˆ¬ k8s å†…çš„èµ„æºéƒ½ä¼šç”±ä¸€ä¸ª controller æ¥è´Ÿè´£å®ƒçš„çŠ¶æ€ç®¡ç†, éƒ½ç”± kube-controller-manager è´Ÿè´£ï¼Œ ä½† ingress controller ä¸æ˜¯å®ƒçš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ˜¯è§†æƒ…å†µè‡ªå·±é€‰æ‹©åˆé€‚çš„ ingress controller. åœ¨ eks ä¸Šæˆ‘ä¸»è¦éœ€è¦ ingress-nginx å’Œ aws-alb-ingress-controller. æ³¨æ„, nginx inc è¿˜ç»´æŠ¤ä¸€ä¸ª kubernetes-ingress, å’Œå®˜æ–¹é‚£ä¸ªä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œ æ²¡æµ‹è¯•è¿‡. è¿™é‡Œä¸»è¦åªæµ‹è¯•äº† ingress-nginx, çœ‹äº†ä¸‹å†…éƒ¨å®ç°, æ•°æ®çš„è½¬å‘çœŸæ‰­æ›²</description>
    </item>
    
    <item>
      <title>eks è¯„æµ‹ part-2</title>
      <link>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</link>
      <pubDate>Fri, 21 Sep 2018 10:28:17 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/21/eks-%E8%AF%84%E6%B5%8B-part-2/</guid>
      <description>ä¸Šæ–‡æµ‹è¯•äº†ä¸€ä¸‹ EKS å’Œ cluster autoscaler, æœ¬æ–‡è®°å½•å¯¹ persisten volume çš„æµ‹è¯•. PersistentVolume åˆ›å»º gp2 ç±»å‹çš„ storageclass, å¹¶ç”¨ annotations è®¾ç½®ä¸ºé»˜è®¤ sc, dynamic volume provision ä¼šç”¨åˆ°: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2 annotations: storageclass.kubernetes.io/is-default-class: &amp;quot;true&amp;quot; provisioner: kubernetes.io/aws-ebs reclaimPolicy: Retain parameters: type: gp2 fsType: ext4 encrypted: &amp;quot;true&amp;quot; å› ä¸º eks æ˜¯åŸºäº 1.10.3 çš„, volume expansion è¿˜æ˜¯ alpha çŠ¶æ€, æ²¡æ³•è‡ªåŠ¨å¼€å¯(æ²¡æ³•æ”¹ api server é…ç½®), æ‰€ä»¥ storageclass çš„ allowVolumeExpansion, è®¾ç½®äº†ä¹Ÿæ²¡ç”¨. è¿™é‡Œ encrypted çš„å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸², å¦åˆ™ä¼šåˆ›å»ºå¤±è´¥, è€Œä¸”æŠ¥é”™è«åå…¶å¦™. åˆ›å»º pod çš„æ—¶å€™æŒ‡å®šä¸€ä¸ªå·²å­˜åœ¨çš„ ebs volume apiVersion: v1 kind: Pod metadata: name: test spec: volumes: - name: test awsElasticBlockStore: fsType: ext4 volumeID: vol-03670d6294ccf29fd containers: - image: nginx name: nginx volumeMounts: - name: test mountPath: /mnt kubectl -it test -- /bin/bash è¿›å»çœ‹ä¸€ä¸‹: root@test:/# df -h</description>
    </item>
    
    <item>
      <title>EKS è¯„æµ‹</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>EKS æ­£å¼ launch è¿˜æ²¡æœ‰æ­£ç»ç”¨è¿‡, æœ€è¿‘æ€»ç®—è¯•äº†ä¸€æŠŠ, è®°å½•ä¸€ç‚¹. Setup AWS å®˜æ–¹çš„ Guide åªæä¾›äº†ä¸€ä¸ª cloudformation template æ¥è®¾ç½® worker node, æˆ‘å–œæ¬¢ç”¨ terraform, å¯ä»¥è·Ÿç€è¿™ä¸ªæ–‡æ¡£å°è¯•:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html æ¥è®¾ç½®å®Œæ•´çš„ eks cluster å’Œç®¡ç† worker node çš„ autoscaling group. è®¾ç½®å®Œ EKS åéœ€è¦æ·»åŠ ä¸€æ¡ ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes è¿™æ · worker node èŠ‚ç‚¹æ‰èƒ½åŠ å…¥é›†ç¾¤. ç½‘</description>
    </item>
    
    <item>
      <title>AWS çš„ K8S CNI Plugin</title>
      <link>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</link>
      <pubDate>Mon, 09 Apr 2018 15:28:38 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</guid>
      <description>EKS è¿˜æ²¡æœ‰ launch, ä½† AWS å…ˆå¼€æºäº†è‡ªå·±çš„ CNI æ’ä»¶, ç®€å•çœ‹äº†ä¸‹, è¯´è¯´å®ƒçš„å®ç°å’Œå…¶ä»– K8S ç½‘ç»œæ–¹æ¡ˆçš„å·®åˆ«. K8S é›†ç¾¤å¯¹ç½‘ç»œæœ‰å‡ ä¸ªåŸºæœ¬è¦æ±‚: container ä¹‹é—´ç½‘ç»œå¿…é¡»å¯è¾¾ï¼Œä¸”ä¸é€šè¿‡ NAT æ‰€æœ‰ node å¿…é¡»å¯ä»¥å’Œæ‰€æœ‰ container é€šä¿¡, ä¸”ä¸é€šè¿‡ NAT container è‡ªå·±çœ‹åˆ°çš„ IP, å¿…é¡»å’Œå…¶ä»– container çœ‹åˆ°çš„å®ƒçš„ ip ç›¸åŒ. Flannel in VPC flannel æ˜¯ K8S çš„ä¸€ä¸ª CNI æ’ä»¶, åœ¨ VPC é‡Œä½¿ç”¨ flannel çš„è¯, æœ‰å‡ ä¸ªé€‰æ‹©: é€šè¿‡ VXLAN/UDP è¿›è¡Œå°åŒ…, å°åŒ…å½±å“ç½‘ç»œæ€§èƒ½, è€Œä¸”ä¸å¥½ debug ç”¨ aws vpc backend, è¿™ç§æ–¹å¼ä¼šæŠŠæ¯å°ä¸»æœºçš„ docker ç½‘æ®µæ·»åŠ è¿› vpc routing table, ä½†é»˜è®¤ routing table é‡Œåªèƒ½æœ‰50æ¡è§„åˆ™</description>
    </item>
    
    <item>
      <title>AWS lambda çš„ä¸€äº›åº”ç”¨åœºæ™¯</title>
      <link>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</link>
      <pubDate>Fri, 23 Mar 2018 17:40:54 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/03/23/aws-lambda-%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</guid>
      <description>è¿™å‡ å¹´å¹ serverless çš„æ¯”è¾ƒå¤š, åœ¨å…¬å¸å†…éƒ¨ä¹Ÿç”¨ lambda , è®°å½•ä¸€ä¸‹, è¿™ä¸œè¥¿æŒºæœ‰ç”¨, ä½†è¿œä¸åˆ°ä¸‡èƒ½, åœºæ™¯æ¯”è¾ƒæœ‰é™. lambda çš„ä»£ç çš„éƒ¨ç½²ç”¨çš„ serverless æ¡†æ¶, æœ¬èº«æ”¯æŒå¤šç§ cloud å¹³å°, æˆ‘ä»¬å°±åªåœ¨ aws lambda ä¸Šäº†. æˆ‘åŸºæœ¬ä¸Šå°±æŠŠ lambda å½“æˆ trigger å’Œ web hook ç”¨. å’Œ auto scaling group ä¸€èµ·ç”¨ çº¿ä¸Šæ‰€æœ‰åˆ†ç»„çš„æœºå™¨éƒ½æ˜¯ç”¨ auto scaling group ç®¡ç†çš„, åªä¸è¿‡ stateless çš„ server å¼€äº†è‡ªåŠ¨ä¼¸ç¼©, å¸¦çŠ¶æ€çš„ (ElasticSearch cluster, redis cache cluster) åªç”¨æ¥ç»´æŠ¤å›ºå®š size. åœ¨å¾€ä¸€ä¸ª group é‡ŒåŠ  server çš„æ—¶å€™, è¦åšçš„äº‹æƒ…æŒºå¤šçš„, ç»™æ–° server æ·»åŠ ç»„å†…ç¼–å· tag, æ·»åŠ å†…ç½‘åŸŸå, provision, éƒ¨ç½²æœ€æ–°ä»£ç . è¿™äº›äº‹éƒ½ç”¨</description>
    </item>
    
    <item>
      <title>DynamoDB</title>
      <link>https://blog.monsterxx03.com/2017/12/15/dynamodb/</link>
      <pubDate>Fri, 15 Dec 2017 22:24:36 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/12/15/dynamodb/</guid>
      <description>DynamoDB æ˜¯ AWS çš„æ‰˜ç®¡ NoSQL æ•°æ®åº“ï¼Œå¯ä»¥å½“ä½œç®€å•çš„ KV æ•°æ®åº“ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ–‡æ¡£æ•°æ®åº“ä½¿ç”¨. Data model ç»„ç»‡æ•°æ®çš„å•ä½æ˜¯ table, æ¯å¼  table å¿…é¡»è®¾ç½® primary key, å¯ä»¥è®¾ç½®å¯é€‰çš„ sort key æ¥åšç´¢å¼•. æ¯æ¡æ•°æ®è®°ä½œä¸€ä¸ª item, æ¯ä¸ª item å«æœ‰ä¸€ä¸ªæˆ–å¤šä¸ª attribute, å…¶ä¸­å¿…é¡»åŒ…æ‹¬ primary key. attribute å¯¹åº”çš„ value æ”¯æŒä»¥ä¸‹å‡ ç§ç±»å‹: Number, ç”±äº DynamoDB çš„ä¼ è¾“åè®®æ˜¯ http + json, ä¸ºäº†è·¨è¯­è¨€çš„å…¼å®¹æ€§, number ä¸€å¾‹ä¼šè¢«è½¬æˆ string ä¼ è¾“. Binary, ç”¨æ¥è¡¨ç¤ºä»»æ„çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œä¼šç”¨ base64 encode åä¼ è¾“. Boolean, true or false Null Document ç±»å‹åŒ…å« List å’Œ Map, å¯ä»¥äº’ç›¸åµŒå¥—. List, ä¸ªæ•°æ— é™åˆ¶, æ€»å¤§å°</description>
    </item>
    
    <item>
      <title>Handle outage</title>
      <link>https://blog.monsterxx03.com/2017/12/10/handle-outage/</link>
      <pubDate>Sun, 10 Dec 2017 11:13:53 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/12/10/handle-outage/</guid>
      <description>A few weeks ago, production environment came to an outage, solve it cost me 8 hours (from 3am to 11am) although total down time is not long, really a bad expenrience. Finally, impact was mitigated, and I&amp;rsquo;m working on a long term solution. I learned some important things from this accident.
The outage I received alarms about live performance issue at 3am, first is server latency increaing, soon some service&amp;rsquo;s health check failed due to high load.
I did following:
Check monitor Identify the problem is caused by KV system Okay, problem is here, I know the problem is KV system&amp;rsquo;s performance issue. But I can&amp;rsquo;t figure out the root case right now, I need a temporary solution. Straightward way is redirect traffic to slave instance. But I know it won&amp;rsquo;t work (actually it is true), I come to similar issue before, did a fix for it, but seems it doesn&amp;rsquo;t work.
The real down time was not long, performance recovered to some degree soon, but latency was still high, not normal. I monitored it for long time, and tried to find out the root case until morning. Since traffic was growing when peak hour coming, performance became problem again.</description>
    </item>
    
    <item>
      <title>Python Web åº”ç”¨æ€§èƒ½è°ƒä¼˜</title>
      <link>https://blog.monsterxx03.com/2017/07/01/python-web-%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</link>
      <pubDate>Sat, 01 Jul 2017 23:38:24 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/07/01/python-web-%E5%BA%94%E7%94%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</guid>
      <description>Python web åº”ç”¨æ€§èƒ½è°ƒä¼˜ ä¸ºäº†å¿«é€Ÿä¸Šçº¿ï¼Œæ—©æœŸå¾ˆå¤šä»£ç åŸºæœ¬æ˜¯æ€ä¹ˆæ–¹ä¾¿æ€ä¹ˆæ¥ï¼Œè¿™æ ·å°±ç•™ä¸‹äº†å¾ˆå¤šéšæ‚£ï¼Œæ€§èƒ½ä¹Ÿä¸æ˜¯å¾ˆç†æƒ³ï¼Œpython å› ä¸º GIL çš„åŸå› ï¼Œåœ¨æ€§èƒ½ä¸Šæœ‰å¤©ç„¶åŠ£åŠ¿ï¼Œå³ä½¿ç”¨äº† gevent/eventlet è¿™ç§åç¨‹æ–¹æ¡ˆï¼Œä¹Ÿå¾ˆå®¹æ˜“å› ä¸ºè€—æ—¶çš„ CPU æ“ä½œé˜»å¡ä½æ•´ä¸ªè¿›ç¨‹ã€‚å‰é˜µå­å¯¹åŸºç¡€ä»£ç åšäº†äº›é‡æ„ï¼Œæ•ˆæœæ˜¾è‘—ï¼Œè®°å½•ä¸€äº›ã€‚ è®¾å®šç›®æ ‡: æ€§èƒ½æé«˜äº†ï¼Œæœ€ç›´æ¥çš„æ•ˆæœå½“ç„¶æ˜¯èƒ½ç”¨æ›´å°‘çš„æœºå™¨å¤„ç†ç›¸åŒæµé‡ï¼Œç›®æ ‡æ˜¯å…³é—­ 20% çš„ stateless webserver. å°½é‡åœ¨æ¡†æ¶ä»£ç ä¸Šåšæ”¹åŠ¨ï¼Œä¸åŠ¨ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚ ä½é£é™© (å†</description>
    </item>
    
    <item>
      <title>Build deb repository with fpm , aptly and s3</title>
      <link>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</link>
      <pubDate>Fri, 23 Jun 2017 09:40:58 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</guid>
      <description>Iâ€™m lazy, I donâ€™t want to be deb/rpm expert, I donâ€™t want to maintain repo server. I want as less maintenance effort as possible. ğŸ™‚
Combine tools fpm, aptly with aws s3, we can do it.
Use fpm to convert python package to deb fpm can transform python/gem/npm/dir/â€¦ to deb/rpm/solaris/â€¦ packages
Example:
fpm -s python -t deb -m xyj.asmy@gmail.com --verbose -v 0.10.1 --python-pip /usr/local/pip Flask It will transform Flask 0.10.1 package to deb. Output package will be python-flask_0.10.1_all.deb
Notes:
If python packages rely on some c libs like MySQLdb (libmysqlclient-dev), you need to install them on the machine to build deb binary. By default fpm use easy_install to build packages, some packages like httplib2 have permission bug with easy_install, so I use pip By default, msgpack-python will be convert to python-msgpack-python, I donâ€™t like it, so add -n python-msgpack to normalize the package name. Some packageâ€™s dependenciesâ€™ version number is not valid(eg: celery 3.1.25 deps pytz &amp;gt;= dev), so I replace the dependencies with --python-disable-dependency pytz -d &#39;pytz &amp;gt;= 2016.7&#39; fpm will not dowload packageâ€™s dependency automatically, you need to do it by your self Use aptly to setup deb repository aptly can help build a self host deb repository and publish it on s3.</description>
    </item>
    
    <item>
      <title>Infrastructure as Code</title>
      <link>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</link>
      <pubDate>Fri, 21 Apr 2017 16:25:07 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</guid>
      <description>Create virtual resource on AWS is very convenient, but how to manage them will be a problem when your size grow.
You will come to:
How to explain the detail online settings for your colleagues (like: how our prod vpc is setup?whatâ€™s the DHCP option set?), navigate around AWS console is okay, but not convenient. Who did what to which resource at when? AWS have a service called Config, can be used to track this change, but if you want to make things as clear as viewing git log, still a lot of works to do. Ideally, we should manage AWS resources like code, all changes kept in VCS, so called Infrastructure as Code.
Iâ€™ve tried three ways to do it:
ansible CloudFormation terraform In this article, I&amp;rsquo;ll compare them, however, the conclusion is to use terraform ğŸ™‚
Ansible Provision tools, like ansible/chef/puppet, all can be used to create aws resources, but they have some common problems:
Hard to track changes after bootstrap. No confident what it will do to existing resources. For example, I define a security group in ansibble:
ec2_group: name: &amp;quot;web&amp;quot; description: &amp;quot;security group in web&amp;quot; vpc_id: &amp;quot;vpc-xxx&amp;quot; region: &amp;quot;us-east-1&amp;quot; rules: - proto: tcp from_port: 80 to_port: 80 cidr_ip: 0.</description>
    </item>
    
    <item>
      <title>Migrate to encrypted RDS</title>
      <link>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</link>
      <pubDate>Fri, 28 Oct 2016 16:17:30 +0000</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/10/28/migrate-to-encrypted-rds/</guid>
      <description>&lt;p&gt;æœ€è¿‘å…¬å¸åœ¨åš HIPAA Compliance ç›¸å…³çš„äº‹æƒ…ï¼Œå…¶ä¸­è¦æ±‚ä¹‹ä¸€æ˜¯æ‰€æœ‰dbéœ€è¦å¼€å¯encryption.&lt;/p&gt;
&lt;p&gt;æ¯”è¾ƒéº»çƒ¦çš„æ˜¯rds çš„encryption åªèƒ½åœ¨åˆ›å»ºçš„æ—¶å€™è®¾å®šï¼Œæ— æ³•ä¹‹åä¿®æ”¹, æ‰€ä»¥å¿…é¡»å¯¹çº¿ä¸Šçš„db åšä¸€æ¬¡ migration.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL innodb buffer pool</title>
      <link>https://blog.monsterxx03.com/2016/07/16/mysql-innodb-buffer-pool/</link>
      <pubDate>Sat, 16 Jul 2016 16:07:14 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2016/07/16/mysql-innodb-buffer-pool/</guid>
      <description>&lt;p&gt;æœ€è¿‘åœ¨å¯¹å…¬å¸çš„ MySQL æœåŠ¡å™¨åšæ€§èƒ½ä¼˜åŒ–, ä¸€ç›´å¯¹ innodb çš„å†…å­˜ä½¿ç”¨æ–¹å¼ä¸æ˜¯å¾ˆæ¸…æ¥š, ä¹˜è¿™æœºä¼šåšç‚¹æ€»ç»“.&lt;/p&gt;
&lt;p&gt;åœ¨é…ç½® MySQL çš„æ—¶å€™, ä¸€èˆ¬éƒ½ä¼šéœ€è¦è®¾ç½® &lt;em&gt;innodb_buffer_pool_size&lt;/em&gt;, åœ¨å°† MySQL è®¾ç½®åœ¨å•ç‹¬çš„æœåŠ¡å™¨ä¸Šæ—¶, ä¸€èˆ¬ä¼šè®¾ç½®ä¸ºç‰©ç†å†…å­˜çš„80%.&lt;/p&gt;
&lt;p&gt;ä¹‹å‰ä¸€ç›´ç–‘æƒ‘ MySQL æ˜¯æ€ä¹ˆç¼“å­˜æ•°æ®çš„(ä¸æ˜¯æŒ‡query cache), ç›´è§‰åº”è¯¥æ˜¯LRU, ä½†å¦‚æœ query ä¸€ä¸‹ä»ç£ç›˜ä¸Šè¯»å–å¤§é‡çš„æ•°æ®çš„è¯(å…¨è¡¨æ‰«ææˆ–æ˜¯ mysqldump), æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“å°±ä¼šæŠŠçƒ­æ•°æ®ç»™è¸¢å‡ºå»?&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
