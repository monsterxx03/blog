<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Code-Infra on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/code-infra/</link>
    <description>Recent content in Code-Infra on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Tue, 07 Apr 2020 10:54:12 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/code-infra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>重构推送服务</title>
      <link>https://blog.monsterxx03.com/2020/04/07/%E9%87%8D%E6%9E%84%E6%8E%A8%E9%80%81%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Tue, 07 Apr 2020 10:54:12 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2020/04/07/%E9%87%8D%E6%9E%84%E6%8E%A8%E9%80%81%E6%9C%8D%E5%8A%A1/</guid>
      <description>&lt;p&gt;最近对业务里发送 apple APNS, google FCM 部分的代码进行了重构, 抽出了一个单独的 service, 本文记录下整个过程.&lt;/p&gt;&#xA;&lt;h2 id=&#34;存在的问题&#34;&gt;存在的问题&lt;/h2&gt;&#xA;&lt;p&gt;我们有好几个 mobile app, 每个 app 会有一套对应的 server 端 service 做业务逻辑, 因为历史原因, 每个 service 里面其实有很多重复代码, 大多只是一些配置和错误处理上有差异.&#xA;给 app 发推送是个典型, 原来的做法是当要发推送的时候, 往 python 的 celery 队列里扔一个 task, 由 celery 异步得去发.&lt;/p&gt;&#xA;&lt;p&gt;有如下问题:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;celery 性能不佳, worker class 用的是 gevent, 但在高峰时候任务队列里还是有大量 pending, 代码上之前做过多次优化, 但收效甚微.&lt;/li&gt;&#xA;&lt;li&gt;当 pending 了大量发推送的 task 之后, 会导致其他异步 task 跟着延时, 造成用户体验上的一些问题.&lt;/li&gt;&#xA;&lt;li&gt;历史代码的问题, 每个 app 里对接 APNS/FCM 都有一套单独的代码, 在部分错误处理和统计 metrics 上有差异.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;问题1, 目前已经迁移到 k8s, 其实可以无脑开 HPA, 做 auto scaling, 来提高发送效率. 问题是 python 性能实在太差, 峰值时候估计会 scale 到原先的好多倍, 而 celery 里&#xA;除了发推送还有很多 task 有 db 操作, scale 过多, 会带来其他问题, 那是另一个问题, 不想在当前这个问题里解决.&lt;/li&gt;&#xA;&lt;li&gt;问题２, celery 里可以设 routing, 简单说可以用单独的 celery instance 来专门处理推送相关 task, 也可以规避1里的 db 问题.&lt;/li&gt;&#xA;&lt;li&gt;问题3, 单纯代码问题, 统一用一套代码重构就行了.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;讲真, 碰到的 celery 的 bug 实在太多了, 存量代码用用就行了, 新做业务实在不想用. 而发推送是个很独立的业务, 最后打算拿 go 来单独跑推送.&lt;/p&gt;</description>
    </item>
    <item>
      <title>老代码里和 MySQL 的事务隔离相关的一个bug</title>
      <link>https://blog.monsterxx03.com/2019/10/31/%E8%80%81%E4%BB%A3%E7%A0%81%E9%87%8C%E5%92%8C-mysql-%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B8%80%E4%B8%AAbug/</link>
      <pubDate>Thu, 31 Oct 2019 11:45:44 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/10/31/%E8%80%81%E4%BB%A3%E7%A0%81%E9%87%8C%E5%92%8C-mysql-%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B8%80%E4%B8%AAbug/</guid>
      <description>&lt;p&gt;这两天在调试代码的时候, 发现 db 层的代码在每次把 connection 放回 db pool 的时候,即使之前执行的是 select 语句,也会 rollback 一下,&#xA;这代码很古老, 我也不知道为啥, 尝试把 rollback 去掉, 结果单元测试挂了一堆, 大多都是数据不一致的问题, debug 了一下, 最后发现这坑还挺大的.&lt;/p&gt;&#xA;&lt;p&gt;为什么去掉 select 的 rollback 后会出现数据不一致?&lt;/p&gt;&#xA;&lt;p&gt;pymysql 默认关闭了 autocommit, connection A 进行 select 之后, 其实 MySQL 内部为 select 也开启了一个 transaction(Repeatable Read),&#xA;可以通过 &lt;code&gt;SELECT * FROM information_schema.innodb_trx\G&lt;/code&gt;  查看.&lt;/p&gt;&#xA;&lt;p&gt;所以当 connection A 先 select 一次, connection B 在 transaction 内更新数据并 commit, connection A 再次 select (之前的 transaction 并没 rollback 或 commit),&#xA;得到了老的数据. MySQL 在 repeatable read 下, 为了 consistent read 会使用一个 snapshot, 时间是第一次 read 发生的时间: &lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_consistent_read&#34;&gt;https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_consistent_read&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Celery Time Limit 的坑</title>
      <link>https://blog.monsterxx03.com/2019/03/28/celery-time-limit-%E7%9A%84%E5%9D%91/</link>
      <pubDate>Thu, 28 Mar 2019 17:37:29 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/03/28/celery-time-limit-%E7%9A%84%E5%9D%91/</guid>
      <description>&lt;p&gt;之前用 celery 做的 task 都是一些很简单轻量级的 task, 从来没触发过 timeout, 最近加入了一些复杂很耗时的 task, 碰到一些 time limit 的坑.&lt;/p&gt;&#xA;&lt;p&gt;celery 中 time limit 有两种, soft_time_limit 和 hard_time_limit, 区别是 soft_time_limit 会在内部抛一个 Exception, task 可以 catch 自行处理.&#xA;hard time limit 没法被 catch.&lt;/p&gt;&#xA;&lt;p&gt;使用如下:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; myapp &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; app&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; celery.exceptions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SoftTimeLimitExceeded&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@app.task&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mytask&lt;/span&gt;():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        do_work()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; SoftTimeLimitExceeded:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        clean_up_in_a_hurry()&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&#xA;&lt;p&gt;我 celery pool 用的是 gevent, 实际上在现在的实现里 gevent 做 worker pool 的时候会忽略 soft_time_limit, 只有 hard_time_limit 会被触发(通过 gevent.Timeout 实现).&lt;/p&gt;&#xA;&lt;p&gt;坑爹的是文档里写的是错的: &lt;a href=&#34;http://docs.celeryproject.org/en/latest/userguide/workers.html#time-limit&#34;&gt;http://docs.celeryproject.org/en/latest/userguide/workers.html#time-limit&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;soft_time_limit 只在 prefork pool 里支持.&lt;/p&gt;&#xA;&lt;p&gt;我现在想让 celery 把这个 hard timeout 的情况 report 到 sentry, 看了圈代码并没法从外面 override timeout 的 callback. 只能很丑得做了个 monkey patch, 在初始化 celeryapp&#xA;的代码里:&lt;/p&gt;</description>
    </item>
    <item>
      <title>从去年的一个patch说起</title>
      <link>https://blog.monsterxx03.com/2018/12/29/%E4%BB%8E%E5%8E%BB%E5%B9%B4%E7%9A%84%E4%B8%80%E4%B8%AApatch%E8%AF%B4%E8%B5%B7/</link>
      <pubDate>Sat, 29 Dec 2018 15:14:46 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/12/29/%E4%BB%8E%E5%8E%BB%E5%B9%B4%E7%9A%84%E4%B8%80%E4%B8%AApatch%E8%AF%B4%E8%B5%B7/</guid>
      <description>&lt;p&gt;去年对线上业务做了一些性能优化, 当时把 http client 从 requests 换成了 geventhttpclient ,&#xA;上线后发起 rpc 调用的 server 整体负载低了很多, 但 client 端 latency 却高了很多, 经过 debug&#xA;觉得问题是 geventhttpclient 把 header 和 body 通过两次 sock send 发出的额外开销造成的, 尝试&#xA;修改成一次 send 后 latency 就恢复了: &lt;a href=&#34;https://github.com/gwik/geventhttpclient/pull/85&#34;&gt;https://github.com/gwik/geventhttpclient/pull/85&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;最近在调试 gunicorn 的代码时候, 看到它建立 socket 的时候设置了 TCP_NODELAY, 在很多项目里看到过这个&#xA;tcp option, 但没细究过, &lt;code&gt;man tcp&lt;/code&gt; 得知是用来关闭 tcp 里的 nagle 算法的. nagle 在 linux 的&#xA;默认 tcp 协议栈里是开启的, 当发送的数据包 size 小于 mss 的时候会在内存里 buffer 起来, 积累起来后再发送,&#xA;目的是提高带宽利用率, 毕竟 payload 只发一次字节也要带上 40 字节的 ip+tcp header.&lt;/p&gt;</description>
    </item>
    <item>
      <title>用 Bloom filter 给推荐列表去重</title>
      <link>https://blog.monsterxx03.com/2018/11/17/%E7%94%A8-bloom-filter-%E7%BB%99%E6%8E%A8%E8%8D%90%E5%88%97%E8%A1%A8%E5%8E%BB%E9%87%8D/</link>
      <pubDate>Sat, 17 Nov 2018 13:58:27 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/11/17/%E7%94%A8-bloom-filter-%E7%BB%99%E6%8E%A8%E8%8D%90%E5%88%97%E8%A1%A8%E5%8E%BB%E9%87%8D/</guid>
      <description>&lt;p&gt;之前产品里有一个功能是每天给用户推荐一批文章,要保证最后推给用户的文章每天不重复. 原先的实现很直接, 每次推送时候记录下用户 id 和 topic id 的键值对, 拿到新 topic 列表后,取出曾经给该用户推送过的文章列表, 两个 set 去重.&lt;/p&gt;&#xA;&lt;p&gt;这个实现的问题很明显, 存储空间量太大(M * N), user id (int64) + topic id (int64) = 16 bytes, 1 million 的用户, 每天给用户推送10篇文章, 一年要存储: 16 * 10 * 365 * 1M = 54.4GB. 查询效率也很低,要么一次取所有已读 topic id, 要么把要推送的 topic id 都丢进数据库去重.&lt;/p&gt;</description>
    </item>
    <item>
      <title>升级celery 到 4.2.0 碰到的坑</title>
      <link>https://blog.monsterxx03.com/2018/06/22/%E5%8D%87%E7%BA%A7celery-%E5%88%B0-4.2.0-%E7%A2%B0%E5%88%B0%E7%9A%84%E5%9D%91/</link>
      <pubDate>Fri, 22 Jun 2018 16:10:41 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/06/22/%E5%8D%87%E7%BA%A7celery-%E5%88%B0-4.2.0-%E7%A2%B0%E5%88%B0%E7%9A%84%E5%9D%91/</guid>
      <description>&lt;p&gt;在把代码往 python3 迁移的过程中需要升级一些第三方库, 升级了 gevent 后发现 celery 有问题, 于是尝试把 celery 从3.1.25 升级到 4.2.0, 中间碰到了很多问题, 记录一点.&lt;/p&gt;&#xA;&lt;h2 id=&#34;配置的变化&#34;&gt;配置的变化&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;CELERY_ACCEPT_CONENT&lt;/code&gt; 之前默认是都允许的,  4.0 开始默认值只允许 json, 因为我用的是msgpack, 所以需要修改这个配置让它接受 msgpack.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;CELERY_RESULT_SERIALIZER&lt;/code&gt; 之前默认是pickle, 现在默认也变成了json, 如果task 的返回结果是 binary 的话, json 无法处理,要么把结果 base64 编码, 要么把&lt;code&gt;CELERY_RESULT_SERIALIZER&lt;/code&gt; 配置成 msgpack,  pickle 明显 py2 / 3 不兼容, 没用.&lt;/p&gt;</description>
    </item>
    <item>
      <title>编写 python 2/3 兼容代码</title>
      <link>https://blog.monsterxx03.com/2018/06/16/%E7%BC%96%E5%86%99-python-2/3-%E5%85%BC%E5%AE%B9%E4%BB%A3%E7%A0%81/</link>
      <pubDate>Sat, 16 Jun 2018 14:38:26 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/06/16/%E7%BC%96%E5%86%99-python-2/3-%E5%85%BC%E5%AE%B9%E4%BB%A3%E7%A0%81/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://blog.monsterxx03.com/2018/06/07/from-python2-to-python3/&#34;&gt;上一篇&lt;/a&gt; 里简单得提了一点开始做 python 2 到 python3 迁移时候碰到的问题, 和工具的选择(推荐用 six).这篇讲下编写 python 2 / 3 兼容代码要注意的事情.&lt;/p&gt;&#xA;&lt;h2 id=&#34;_future_&#34;&gt;_&lt;em&gt;future&lt;/em&gt;_&lt;/h2&gt;&#xA;&lt;p&gt;python2 里自带的向后兼容模块，将 python3 的一些语法行为 backport 到 python2 里, 使用的时候需要在文件头部声明, 作用域只在当前文件.&lt;/p&gt;&#xA;&lt;p&gt;首先是几个在 python 2.7 里不用特意写，已经默认开启的特性:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;from __future__ import nested_scopes&lt;/code&gt; 2.2 开始就默认开启了，用于修改嵌套函数内的变量搜索作用域, 在此之前, 全局模块的优先级比被嵌套函数的父函数要高, 现在都没这个问题了.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;from __future__ import generators&lt;/code&gt;, yield 关键词, 2.3 默认支持.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;from __future__ import with_statement&lt;/code&gt;, with 关键词, 2.6 默认支持.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;我显示开启的两个特性:&lt;/p&gt;</description>
    </item>
    <item>
      <title>From python2 to python3</title>
      <link>https://blog.monsterxx03.com/2018/06/07/from-python2-to-python3/</link>
      <pubDate>Thu, 07 Jun 2018 16:41:57 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/06/07/from-python2-to-python3/</guid>
      <description>&lt;p&gt;This article won&amp;rsquo;t provide perfect guide for porting py2 code to py3, just list the solutions I tried, the&#xA;problems I come to, and my choices. I haven&amp;rsquo;t finished this project, also I haven&amp;rsquo;t gave up so far :).&lt;/p&gt;&#xA;&lt;p&gt;Won&amp;rsquo;t explain too much about the differences between py2 and py3, will write down some corner&#xA;cases which are easy to miss.&lt;/p&gt;&#xA;&lt;p&gt;The codebase I&amp;rsquo;m working on:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Only support python2.7, don&amp;rsquo;t consider python2.6&lt;/li&gt;&#xA;&lt;li&gt;1X repos, about half a million lines of code in total (calculated by cloc).&lt;/li&gt;&#xA;&lt;li&gt;These repos will import each other, bad design from early days, not easy to resolve, which means I can&amp;rsquo;t switch to py3 one by one, I need write&#xA;py2/3 compatiblility code for them, and switch together(I&amp;rsquo;m also considering solve the import problem first).&lt;/li&gt;&#xA;&lt;li&gt;Test coverage is not good, best is around 80%, lowest is 30%.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;2to3&lt;/code&gt;, a command line tools packaged with py2, it&amp;rsquo;s a oneway porting to convert your code to py3, new code won&amp;rsquo;t work under&#xA;py2, since I need be compatible with py2 and py3 for long time, didn&amp;rsquo;t try it.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://python-future.org/&#34;&gt;future&lt;/a&gt;, it tries to make you write single clean python3.x code without ugly hack with six. I used it it first,&#xA;but come to many problems, will explain later.&lt;/p&gt;</description>
    </item>
    <item>
      <title>在python3.7 中实现python2.7 的内置 hash 函数</title>
      <link>https://blog.monsterxx03.com/2018/06/01/%E5%9C%A8python3.7-%E4%B8%AD%E5%AE%9E%E7%8E%B0python2.7-%E7%9A%84%E5%86%85%E7%BD%AE-hash-%E5%87%BD%E6%95%B0/</link>
      <pubDate>Fri, 01 Jun 2018 17:03:24 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/06/01/%E5%9C%A8python3.7-%E4%B8%AD%E5%AE%9E%E7%8E%B0python2.7-%E7%9A%84%E5%86%85%E7%BD%AE-hash-%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;p&gt;最近着手准备从 python2.7 迁移到 python3.7, 还没开始就碰到一个问题. 老系统里有一部分竟然是将 python 内置 hash 函数的结果存进了数据库, 这个做法绝对是错的,&#xA;hash 的结果本来就没有保证过在各个版本的 python 中保证一致. 而且 python3 中算法完全变了, 默认在进程初始化的时候会用随机种子加进 hash 过程, 所以python 进程&#xA;一重启结果就不一样了. 木已成舟， 目前看将数据库里的值全部改掉是不可能了, 只能在 python3 中重新实现一下这个算法.&lt;/p&gt;&#xA;&lt;p&gt;python2.7 中的hash 算法是 fnv (有修改), python3 中变成了 sip, &lt;a href=&#34;https://www.python.org/dev/peps/pep-0456&#34;&gt;pep-456&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Use SNS &amp; SQS to build Pub/Sub System</title>
      <link>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</link>
      <pubDate>Wed, 23 May 2018 18:05:28 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</guid>
      <description>&lt;p&gt;Recently, we build pub/sub system based on AWS&amp;rsquo;s SNS &amp;amp; SQS service, take some notes.&lt;/p&gt;&#xA;&lt;p&gt;Originally, we have an pub/sub system based on redis(use BLPOP to listen to a redis list). It&amp;rsquo;s&#xA;really simple, and mainly for cross app operations. Now we have needs to enhance it to support more complex&#xA;pubsub logic, eg: topic based distribution. It don&amp;rsquo;t support redelivery as well, if subscribers failed to process&#xA;the message, message will be dropped.&lt;/p&gt;&#xA;&lt;p&gt;There&amp;rsquo;re three obvious choices in my mind:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;kafka&lt;/li&gt;&#xA;&lt;li&gt;AMQP based system (rabbitmq,activemq &amp;hellip;)&lt;/li&gt;&#xA;&lt;li&gt;SNS + SQS&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My demands for this system are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support message persistence.&lt;/li&gt;&#xA;&lt;li&gt;Support topic based message distribution.&lt;/li&gt;&#xA;&lt;li&gt;Easy to manage.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The data volume won&amp;rsquo;t be very large, so performance and throughput won&amp;rsquo;t be critical concerns.&lt;/p&gt;&#xA;&lt;p&gt;I choose SNS + SQS, main concerns are from operation side:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;kafka need zookeeper to support cluster.&lt;/li&gt;&#xA;&lt;li&gt;rabbitmq need extra configuration for HA, and AMQP model is relatively complex for programming.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So my decision is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;application publish message to SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Setup multi SQS queues to subscribe SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Let different application processes to subscribe to different queues to finish its logic.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;SQS and SNS is very simple, not too much to say, just some notes:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;SQS queue have two types, FIFO queue and standard queue. FIFO queue will ensure message order, and ensure exactly once delivery, tps is limited(3000/s)&#xA;standard queue is at least once delivery, message order is not ensured, tps is unlimited. In my case, I use standard queue, order is not very important.&lt;/li&gt;&#xA;&lt;li&gt;SQS message size limit is 256KB.&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;a href=&#34;https://github.com/p4tin/goaws&#34;&gt;goaws&lt;/a&gt; for local development, it has problem on processing message attributes, but I just use message body. messages only store in ram,&#xA;will be cleared after restarted.&lt;/li&gt;&#xA;&lt;li&gt;If you failed to deliver message to sqs from sns, can setup topic&amp;rsquo;s &lt;code&gt;sqs failure feedback role&lt;/code&gt; to log to cloudwatch, in most case it&amp;rsquo;s caused by iam permission.&lt;/li&gt;&#xA;&lt;li&gt;Message in sqs can retain at most 14 days.&lt;/li&gt;&#xA;&lt;li&gt;Once a message is received by a client, it will be invisible to other clients in &lt;code&gt;visibility_timeout_seconds&lt;/code&gt;(default 30s). It means if your client failed to process&#xA;the message, it will be redelivered after 30s.&lt;/li&gt;&#xA;&lt;li&gt;SQS client use long polling to receive message, set &lt;code&gt;receive_wait_time_seconds&lt;/code&gt; to reduce api call to reduce fee.&lt;/li&gt;&#xA;&lt;li&gt;If your client failed to process a message due to bug, the message will be redelivered looply, set &lt;code&gt;redrive_policy&lt;/code&gt; for the queue to limit retry count, and set a dead letter&#xA;queue to store those messages. You can decide how to handle them late.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I setup SNS and SQS via terraform, used following resources:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Migrate to Sqlalchemy</title>
      <link>https://blog.monsterxx03.com/2018/05/20/migrate-to-sqlalchemy/</link>
      <pubDate>Sun, 20 May 2018 15:11:31 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/05/20/migrate-to-sqlalchemy/</guid>
      <description>&lt;p&gt;最近把公司 db 层的封装代码基于 sqlalchemy 重写了, 记录一些.&lt;/p&gt;&#xA;&lt;p&gt;原来的 db 层代码历史非常古老(10年以上&amp;hellip;), 最早写代码的人早就不在了, 问题很多:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;完全没有单元测试.&lt;/li&gt;&#xA;&lt;li&gt;暴露出的接口命名很混乱, 多数是为了兼容一些历史问题.&lt;/li&gt;&#xA;&lt;li&gt;里面带一套 client 端 db sharding 的逻辑, 但在新项目里完全用不到, 还导致无法做 join, 无法子查询, 很不方便.&lt;/li&gt;&#xA;&lt;li&gt;老的 db 代码没有 model 层, 和 db migration 通过一种很 trick 的方式绑定在一起实现的, 导致开发时候对着代码完全无法知道数据库表结构，只能直接看数据库.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;重写时候要考虑到的:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
