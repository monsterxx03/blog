<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lang-En on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/lang-en/</link>
    <description>Recent content in Lang-En on Shining Moon</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Thu, 30 May 2019 18:48:23 +0800</lastBuildDate>
    <atom:link href="https://blog.monsterxx03.com/tags/lang-en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Random Talk</title>
      <link>https://blog.monsterxx03.com/2019/05/30/random-talk/</link>
      <pubDate>Thu, 30 May 2019 18:48:23 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2019/05/30/random-talk/</guid>
      <description>&lt;p&gt;Just some random complains and notes about server infra management. I think those are my motivations to move to kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;Won&amp;rsquo;t explain k8s or docker in detail, and how they solve those problems in this post.&lt;/p&gt;&#xA;&lt;h2 id=&#34;infrastructure-levelon-aws&#34;&gt;Infrastructure level(on AWS)&lt;/h2&gt;&#xA;&lt;p&gt;We use following services provided by AWS.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Compute:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;EC2&lt;/li&gt;&#xA;&lt;li&gt;AutoScaling Group&lt;/li&gt;&#xA;&lt;li&gt;Lambda&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;network:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VPC (SDN network)&lt;/li&gt;&#xA;&lt;li&gt;DNS (route53)&lt;/li&gt;&#xA;&lt;li&gt;CDN (CloudFront)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Loadbalancer:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ELB (L4)&lt;/li&gt;&#xA;&lt;li&gt;NLB (L4, ELB successor, support static IP)&lt;/li&gt;&#xA;&lt;li&gt;ALB (L7)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Storage:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;EBS (block storage)&lt;/li&gt;&#xA;&lt;li&gt;EFS (hosted NFS)&lt;/li&gt;&#xA;&lt;li&gt;RDS(MySQl/PostgreSQL &amp;hellip;)&lt;/li&gt;&#xA;&lt;li&gt;Redshift (data warehouse)&lt;/li&gt;&#xA;&lt;li&gt;DynamoDB (KV)&lt;/li&gt;&#xA;&lt;li&gt;S3 (object storage)&lt;/li&gt;&#xA;&lt;li&gt;Glacier (cheap archive storage)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Web Firewall (WAF)&lt;/li&gt;&#xA;&lt;li&gt;Monitor (CloudWatch)&lt;/li&gt;&#xA;&lt;li&gt;DMS (ETL)&#xA;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For infra management, in early days, we just click, click, click&amp;hellip; or write some simple scripts to call AWS api.&lt;/p&gt;&#xA;&lt;p&gt;With infra resources growing, management became complex, a concept called &lt;code&gt;Infrastructure as Code&lt;/code&gt; rising.&lt;/p&gt;&#xA;&lt;p&gt;AWS provides CloudFormation as orchestration tool, but we use &lt;a href=&#34;https://www.terraform.io/&#34;&gt;terraform&lt;/a&gt; (for short: CloudFormation sucks, for long: &lt;a href=&#34;https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/&#34;&gt;Infrastructure as Code&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;So far, not bad.(tweak those services internally is another story&amp;hellip; never belive &lt;code&gt;work out of box&lt;/code&gt;)&lt;/p&gt;&#xA;&lt;h2 id=&#34;application-level&#34;&gt;Application level&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;configuration management (setup nginx, jenkins, redis, twemproxy, ElasticSearch or WTF..)&lt;/li&gt;&#xA;&lt;li&gt;CI/CD&lt;/li&gt;&#xA;&lt;li&gt;dependency management&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;They&amp;rsquo;re complicated, people developped bunch of tools to handle: puppet, chef, ansible, saltstack &amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;They&amp;rsquo;re great and working, but writing correct code still a challenge when changes involves:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes in Action Notes</title>
      <link>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</link>
      <pubDate>Mon, 03 Sep 2018 18:20:46 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</guid>
      <description>&lt;p&gt;Miscellaneous notes when reading &lt;code&gt;&amp;lt;Kubernetes in Action&amp;gt;&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;api-group-and-api-version&#34;&gt;api group and api version&lt;/h2&gt;&#xA;&lt;p&gt;core api group need&amp;rsquo;t specified in &lt;code&gt;apiVersion&lt;/code&gt; field.&lt;/p&gt;&#xA;&lt;p&gt;For example, &lt;code&gt;ReplicationController&lt;/code&gt; is on core api group, so only:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: ReplicationController&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;ReplicationSet&lt;/code&gt; is added later in &lt;code&gt;app&lt;/code&gt; group, &lt;code&gt;v1beta2&lt;/code&gt; version (k8s v1.8):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1beta2            1&#xA;kind: ReplicaSet           &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&#34;&gt;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;replicationcontroller-vs-replicationset&#34;&gt;ReplicationController VS ReplicationSet&lt;/h3&gt;&#xA;&lt;p&gt;ReplicationController is replaced by ReplicationSet, which has more expressive pod selectors.&lt;/p&gt;&#xA;&lt;p&gt;ReplicationController&amp;rsquo;s label selector only allows matching pods that include a certain label, ReplicationSet can&#xA;meet multi labels at same time.&lt;/p&gt;&#xA;&lt;p&gt;rs also support operator on key value: &lt;code&gt;In, NotIn, Exists, DoesNotExist&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;If migrate from rc to rs, can delete rc with &lt;code&gt;--cascade=false&lt;/code&gt; option, it will delete&#xA;rc only, but left pods running, then we can create a rs with same selector to make pods under management.&lt;/p&gt;&#xA;&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;&#xA;&lt;p&gt;DaemonSet ensures pod run exact one copy on one node, useful for processes like monitor agent and log collector. Use &lt;code&gt;node-selector&lt;/code&gt;&#xA;to make ds only run on specific nodes.&lt;/p&gt;&#xA;&lt;p&gt;If node is made unschedulable, normal pods won&amp;rsquo;t be scheduled to deploy on them, but ds will still be deployed to it, since ds will bypass&#xA;scheduler.&lt;/p&gt;&#xA;&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;&#xA;&lt;p&gt;Job is used to run a single completable task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From python2 to python3</title>
      <link>https://blog.monsterxx03.com/2018/06/07/from-python2-to-python3/</link>
      <pubDate>Thu, 07 Jun 2018 16:41:57 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/06/07/from-python2-to-python3/</guid>
      <description>&lt;p&gt;This article won&amp;rsquo;t provide perfect guide for porting py2 code to py3, just list the solutions I tried, the&#xA;problems I come to, and my choices. I haven&amp;rsquo;t finished this project, also I haven&amp;rsquo;t gave up so far :).&lt;/p&gt;&#xA;&lt;p&gt;Won&amp;rsquo;t explain too much about the differences between py2 and py3, will write down some corner&#xA;cases which are easy to miss.&lt;/p&gt;&#xA;&lt;p&gt;The codebase I&amp;rsquo;m working on:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Only support python2.7, don&amp;rsquo;t consider python2.6&lt;/li&gt;&#xA;&lt;li&gt;1X repos, about half a million lines of code in total (calculated by cloc).&lt;/li&gt;&#xA;&lt;li&gt;These repos will import each other, bad design from early days, not easy to resolve, which means I can&amp;rsquo;t switch to py3 one by one, I need write&#xA;py2/3 compatiblility code for them, and switch together(I&amp;rsquo;m also considering solve the import problem first).&lt;/li&gt;&#xA;&lt;li&gt;Test coverage is not good, best is around 80%, lowest is 30%.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;2to3&lt;/code&gt;, a command line tools packaged with py2, it&amp;rsquo;s a oneway porting to convert your code to py3, new code won&amp;rsquo;t work under&#xA;py2, since I need be compatible with py2 and py3 for long time, didn&amp;rsquo;t try it.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://python-future.org/&#34;&gt;future&lt;/a&gt;, it tries to make you write single clean python3.x code without ugly hack with six. I used it it first,&#xA;but come to many problems, will explain later.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Use SNS &amp; SQS to build Pub/Sub System</title>
      <link>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</link>
      <pubDate>Wed, 23 May 2018 18:05:28 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/05/23/use-sns-sqs-to-build-pub/sub-system/</guid>
      <description>&lt;p&gt;Recently, we build pub/sub system based on AWS&amp;rsquo;s SNS &amp;amp; SQS service, take some notes.&lt;/p&gt;&#xA;&lt;p&gt;Originally, we have an pub/sub system based on redis(use BLPOP to listen to a redis list). It&amp;rsquo;s&#xA;really simple, and mainly for cross app operations. Now we have needs to enhance it to support more complex&#xA;pubsub logic, eg: topic based distribution. It don&amp;rsquo;t support redelivery as well, if subscribers failed to process&#xA;the message, message will be dropped.&lt;/p&gt;&#xA;&lt;p&gt;There&amp;rsquo;re three obvious choices in my mind:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;kafka&lt;/li&gt;&#xA;&lt;li&gt;AMQP based system (rabbitmq,activemq &amp;hellip;)&lt;/li&gt;&#xA;&lt;li&gt;SNS + SQS&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My demands for this system are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Support message persistence.&lt;/li&gt;&#xA;&lt;li&gt;Support topic based message distribution.&lt;/li&gt;&#xA;&lt;li&gt;Easy to manage.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The data volume won&amp;rsquo;t be very large, so performance and throughput won&amp;rsquo;t be critical concerns.&lt;/p&gt;&#xA;&lt;p&gt;I choose SNS + SQS, main concerns are from operation side:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;kafka need zookeeper to support cluster.&lt;/li&gt;&#xA;&lt;li&gt;rabbitmq need extra configuration for HA, and AMQP model is relatively complex for programming.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So my decision is:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;application publish message to SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Setup multi SQS queues to subscribe SNS topic&lt;/li&gt;&#xA;&lt;li&gt;Let different application processes to subscribe to different queues to finish its logic.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;SQS and SNS is very simple, not too much to say, just some notes:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;SQS queue have two types, FIFO queue and standard queue. FIFO queue will ensure message order, and ensure exactly once delivery, tps is limited(3000/s)&#xA;standard queue is at least once delivery, message order is not ensured, tps is unlimited. In my case, I use standard queue, order is not very important.&lt;/li&gt;&#xA;&lt;li&gt;SQS message size limit is 256KB.&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;a href=&#34;https://github.com/p4tin/goaws&#34;&gt;goaws&lt;/a&gt; for local development, it has problem on processing message attributes, but I just use message body. messages only store in ram,&#xA;will be cleared after restarted.&lt;/li&gt;&#xA;&lt;li&gt;If you failed to deliver message to sqs from sns, can setup topic&amp;rsquo;s &lt;code&gt;sqs failure feedback role&lt;/code&gt; to log to cloudwatch, in most case it&amp;rsquo;s caused by iam permission.&lt;/li&gt;&#xA;&lt;li&gt;Message in sqs can retain at most 14 days.&lt;/li&gt;&#xA;&lt;li&gt;Once a message is received by a client, it will be invisible to other clients in &lt;code&gt;visibility_timeout_seconds&lt;/code&gt;(default 30s). It means if your client failed to process&#xA;the message, it will be redelivered after 30s.&lt;/li&gt;&#xA;&lt;li&gt;SQS client use long polling to receive message, set &lt;code&gt;receive_wait_time_seconds&lt;/code&gt; to reduce api call to reduce fee.&lt;/li&gt;&#xA;&lt;li&gt;If your client failed to process a message due to bug, the message will be redelivered looply, set &lt;code&gt;redrive_policy&lt;/code&gt; for the queue to limit retry count, and set a dead letter&#xA;queue to store those messages. You can decide how to handle them late.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I setup SNS and SQS via terraform, used following resources:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Access sensitive variables on AWS lambda</title>
      <link>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</link>
      <pubDate>Wed, 28 Feb 2018 21:45:23 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/02/28/access-sensitive-variables-on-aws-lambda/</guid>
      <description>&lt;p&gt;AWS lambda is convenient to run simple serverless application, but how to access sensitive data in code? like password,token&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Usually, we inject secrets as environment variables, but they&amp;rsquo;re still visable on lambda console. I don&amp;rsquo;t use it in aws lambda.&lt;/p&gt;&#xA;&lt;p&gt;The better way is use &lt;a href=&#34;https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html&#34;&gt;aws parameter store&lt;/a&gt; as configuration center. It can work with KMS to encrypt your data.&lt;/p&gt;&#xA;&lt;p&gt;Code example:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;client = boto3.client(&#39;ssm&#39;)&#xA;resp = client.get_parameter(&#xA;    Name=&#39;/redshift/admin/password&#39;,&#xA;    WithDecryption=True&#xA;)&#xA;resp:&#xA;&#xA;    {&#xA;        &amp;quot;Parameter&amp;quot;: {&#xA;            &amp;quot;Name&amp;quot;: &amp;quot;/redshift/admin/password&amp;quot;,&#xA;            &amp;quot;Type&amp;quot;: &amp;quot;SecureString&amp;quot;,&#xA;            &amp;quot;Value&amp;quot;: &amp;quot;password value&amp;quot;,&#xA;            &amp;quot;Version&amp;quot;: 1&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Things you need to do to make it work:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create a new KMS key&lt;/li&gt;&#xA;&lt;li&gt;Use new created KMS key to encrypt your data in parameter store.&lt;/li&gt;&#xA;&lt;li&gt;Set a execution role for your lambda function.&lt;/li&gt;&#xA;&lt;li&gt;In the KMS key&amp;rsquo;s setting page, add the lambda execution role to the list which can read this KMS key.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Then your lambda code can access encrypted data at runtime, and you needn&amp;rsquo;t set aws access_key/secret_key, lambda execution role enable access to data in parameter store.&lt;/p&gt;&#xA;&lt;p&gt;BTW, parameter store support hierarchy(at most 15 levels), splitted by &lt;code&gt;/&lt;/code&gt;. You can retrive data under same level in one call, deltails can be found in doc, eg: &lt;a href=&#34;http://boto3.readthedocs.io/en/latest/reference/services/ssm.html#SSM.Client.get_parameters_by_path&#34;&gt;http://boto3.readthedocs.io/en/latest/reference/services/ssm.html#SSM.Client.get_parameters_by_path&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get Real Client Ip on AWS</title>
      <link>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</link>
      <pubDate>Thu, 01 Feb 2018 15:20:37 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2018/02/01/get-real-client-ip-on-aws/</guid>
      <description>&lt;p&gt;If you run a webserver on AWS, get real client ip will be tricky if you didn&amp;rsquo;t configure server right and write code correctly.&lt;/p&gt;&#xA;&lt;p&gt;Things related to client real ip:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CloudFront (cdn)&lt;/li&gt;&#xA;&lt;li&gt;ALB (loadbalancer)&lt;/li&gt;&#xA;&lt;li&gt;nginx (on ec2)&lt;/li&gt;&#xA;&lt;li&gt;webserver (maybe a python flask application).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Request sequence diagram will be like following:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.monsterxx03.com/posts/images/cf-alb-nginx.png&#34; alt=&#34;req&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;User&amp;rsquo;s real client ip is forwarded by front proxies one by one in head &lt;code&gt;X-Forwarded-For&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;For CloudFront:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If user&amp;rsquo;s req header don&amp;rsquo;t  have &lt;code&gt;X-Forwarded-For&lt;/code&gt;, it will set user&amp;rsquo;s ip(from tcp connection) in &lt;code&gt;X-Forwarded-For&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;If user&amp;rsquo;s req already have &lt;code&gt;X-Forwarded-For&lt;/code&gt;, it will append user&amp;rsquo;s ip(from tcp connection) to the end of &lt;code&gt;X-Forwarded-For&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For ALB, rule is same as CloudFront, so the &lt;code&gt;X-Forwarded-For&lt;/code&gt; header pass to nginx will be the value received from CloudFront + CloudFront&amp;rsquo;s ip.&lt;/p&gt;&#xA;&lt;p&gt;For nginx, things will be tricky depends on your config.&lt;/p&gt;&#xA;&lt;p&gt;Things maybe involved in nginx:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;real ip module&lt;/li&gt;&#xA;&lt;li&gt;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If you didn&amp;rsquo;t use real ip module, you need to pass X-Forwarded-For head explictly.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&lt;/code&gt; will append ALB&amp;rsquo;s ip to the end of &lt;code&gt;X-Forwarded-For&lt;/code&gt; header received from ALB.&lt;/p&gt;&#xA;&lt;p&gt;So &lt;code&gt;X-Forwarded-For&lt;/code&gt; header your webserver received will be &lt;code&gt;user ip,cloudfront ip, alb ip&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Or you can use real ip module to trust the value passed from ALB.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Handle outage</title>
      <link>https://blog.monsterxx03.com/2017/12/10/handle-outage/</link>
      <pubDate>Sun, 10 Dec 2017 11:13:53 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2017/12/10/handle-outage/</guid>
      <description>&lt;p&gt;A few weeks ago, production environment came to an outage, solve it cost me 8 hours (from 3am to 11am) although total down time is not long, really a bad expenrience. Finally, impact was mitigated, and I&amp;rsquo;m working on a long term solution. I learned some important things from this accident.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-outage&#34;&gt;The outage&lt;/h2&gt;&#xA;&lt;p&gt;I received alarms about live performance issue at 3am, first is server latency increaing, soon some service&amp;rsquo;s health check failed due to high load.&lt;/p&gt;&#xA;&lt;p&gt;I did following:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Check monitor&lt;/li&gt;&#xA;&lt;li&gt;Identify the problem is caused by KV system&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Okay, problem is here, I know the problem is KV system&amp;rsquo;s performance issue. But I can&amp;rsquo;t figure out the root case right now, I need a temporary solution.&#xA;Straightward way is redirect traffic to slave instance. But I know it won&amp;rsquo;t work (actually it is true), I come to similar issue before, did a fix for it, but seems it doesn&amp;rsquo;t work.&lt;/p&gt;&#xA;&lt;p&gt;The real down time was not long, performance recovered to some degree soon, but latency was still high, not normal. I monitored it for long time, and tried to find out the root case until morning. Since traffic was growing when peak hour coming, performance became problem again.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS DMS notes</title>
      <link>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</link>
      <pubDate>Sat, 14 Oct 2017 22:33:36 +0800</pubDate>
      <guid>https://blog.monsterxx03.com/2017/10/14/aws-dms-notes/</guid>
      <description>&lt;p&gt;AWS&amp;rsquo;s DMS (Data migration service) can be used to do incremental ETL between databases. I use it to load data from RDS (MySQL) to Redshift.&lt;/p&gt;&#xA;&lt;p&gt;It works, but have some concerns. Take some notes when doing this project.&lt;/p&gt;&#xA;&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;&#xA;&lt;p&gt;Source RDS must:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Enable automatic backups&lt;/li&gt;&#xA;&lt;li&gt;Increase binlog remain time, &lt;code&gt;call mysql.rds_set_configuration(&#39;binlog retention hours&#39;, 24);&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;binlog_format&lt;/code&gt; to &lt;code&gt;ROW&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Privileges on source RDS: &lt;code&gt;REPLICATION CLIENT &lt;/code&gt;, &lt;code&gt;REPLICATION SLAVE &lt;/code&gt;, &lt;code&gt;SELECT&lt;/code&gt; on replication target tables&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;ddl-on-source-table&#34;&gt;DDL on source table&lt;/h2&gt;&#xA;&lt;p&gt;Redshift has some limits on change columns:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;New column only must be added in the end&lt;/li&gt;&#xA;&lt;li&gt;Can&amp;rsquo;t rename columns&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So for DDL on source MySQL, you can&amp;rsquo;t add columns at non end postition, otherwise data in target table will corrupt. I disabled ddl changes target db:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    &amp;quot;ChangeProcessingDdlHandlingPolicy&amp;quot;:{  &#xA;        &amp;quot;HandleSourceTableDropped&amp;quot;:false,&#xA;        &amp;quot;HandleSourceTableTruncated&amp;quot;:false,&#xA;        &amp;quot;HandleSourceTableAltered&amp;quot;:false&#xA;    },&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;If source table schema changed, I just drop and reload target table on console.&lt;/p&gt;&#xA;&lt;h2 id=&#34;control-write-speed-on-redshift&#34;&gt;Control write speed on Redshift&lt;/h2&gt;&#xA;&lt;p&gt;Since Redshift is an OLAP database, write operation is slow and concurrency is low, streaming data directly will have big impact on it.&lt;/p&gt;&#xA;&lt;p&gt;And we have may analysis jobs running on redshift all the time, directly streaming will lock target table and make my analysis jobs timeout.&lt;/p&gt;&#xA;&lt;p&gt;So I need to batch apply changes on DMS. Follow settings need to tweak in task settings json:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get all invalid PTR record on  Route53</title>
      <link>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</link>
      <pubDate>Fri, 29 Sep 2017 08:55:18 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/09/29/get-all-invalid-ptr-record-on-route53/</guid>
      <description>&lt;p&gt;I use autoscaling group to manage stateless servers. Servers go up and down every day.&lt;/p&gt;&#xA;&lt;p&gt;Once server is up, I will add a PTR record for it‚Äôs internal ip. But when it‚Äôs down, I didn‚Äôt cleanup the PTR record. As times fly, a lot of invalid PTR records left in Route53.&lt;/p&gt;&#xA;&lt;p&gt;To cleanup those PTR records realtime, you can write a lambda function, use server termination event as trigger. But how to cleanup the old records at once?&lt;/p&gt;&#xA;&lt;p&gt;Straightforward way is write a script to call AWS API to get a PTR list, get ip from record, test whether the ip is live, if not, delete it.&lt;/p&gt;&#xA;&lt;p&gt;Since use awscli to delete a Route53 record is very troublesome (involve json format), you‚Äôd better write a python script to delete them. I just demo some ideas to collect those records via shell.&lt;/p&gt;&#xA;&lt;p&gt;You can do it in a single line, but make things clear and easy to debug, I split it into several steps.&lt;/p&gt;&#xA;&lt;h2 id=&#34;get-ptr-record-list&#34;&gt;Get PTR record list&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;aws route53 list-resource-record-sets  --hosted-zone-id xxxxx --query &amp;quot;ResourceRecordSets[?Type==&#39;PTR&#39;].Name&amp;quot; |  grep -Po &#39;&amp;quot;(.+?)&amp;quot;&#39; | tr -d \&amp;quot; &amp;gt; ptr.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;ptr.txt will contain lines like:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;1.0.0.10.in-addr.arpa.&#xA;2.0.0.10.in-addr.arpa.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h2 id=&#34;get-ip-list-from-ptr-records&#34;&gt;Get ip list from PTR records&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code&gt;cat ptr.txt | while read -r line ; do echo -n $line | tac -s. | cut -d. -f3- | sed &#39;s/.$//&#39; ; done &amp;gt; ip.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;ip.txt:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Build private static website on S3</title>
      <link>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</link>
      <pubDate>Sat, 19 Aug 2017 07:28:16 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/08/19/build-private-staticwebsite-on-s3/</guid>
      <description>&lt;p&gt;Build static website on S3 is very easy, but by default, it can be accessed by open internet.It will be super helpful if we can build website only available in VPC. Then we can use it to host internal deb repo, doc site‚Ä¶&lt;/p&gt;&#xA;&lt;p&gt;Steps are very easy, you only need VPC endpoints and S3 bucket policy.&lt;/p&gt;&#xA;&lt;p&gt;AWS api is open to internet, if you need to access S3 in VPC, your requests will pass through VPC‚Äôs internet gateway or NAT gateway. With VPC endpoints(can be found in VPC console), your requests to S3 will go through AWS‚Äôs internal network. Currently, VPC endpoints only support S3, support for dynamodb is in test.&lt;/p&gt;&#xA;&lt;p&gt;To restrict S3 bucket only available in your VPC, need to set bucket policy (to host static website, enable static website support first). At first, I didn‚Äôt check doc, try to restrict access by my VPC ip cidr, but it didn‚Äôt work, I need to restrict by VPC endpoint id:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,&#xA;  &amp;quot;Id&amp;quot;: &amp;quot;Policy1415115909152&amp;quot;,&#xA;  &amp;quot;Statement&amp;quot;: [&#xA;    {&#xA;      &amp;quot;Sid&amp;quot;: &amp;quot;Access-to-specific-VPCE-only&amp;quot;,&#xA;      &amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;,&#xA;      &amp;quot;Action&amp;quot;: &amp;quot;s3:GetObject&amp;quot;,&#xA;      &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,&#xA;      &amp;quot;Resource&amp;quot;: [&amp;quot;arn:aws:s3:::my_secure_bucket&amp;quot;,&#xA;                   &amp;quot;arn:aws:s3:::my_secure_bucket/*&amp;quot;],&#xA;      &amp;quot;Condition&amp;quot;: {&#xA;        &amp;quot;StringEquals&amp;quot;: {&#xA;          &amp;quot;aws:sourceVpce&amp;quot;: &amp;quot;vpce-1a2b3c4d&amp;quot;&#xA;        }&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;BTW, if you can config bucket policy restrict on VPC directly, with VPC endpoint you can limit to subnets. Details can be found in doc: &lt;a href=&#34;http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html&#34;&gt;http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-s3.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Build deb repository with fpm , aptly and s3</title>
      <link>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</link>
      <pubDate>Fri, 23 Jun 2017 09:40:58 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/06/23/build-deb-repository-with-fpm-aptly-and-s3/</guid>
      <description>&lt;p&gt;I‚Äôm lazy, I don‚Äôt want to be deb/rpm expert, I don‚Äôt want to maintain repo server. I want as less maintenance effort as possible. üôÇ&lt;/p&gt;&#xA;&lt;p&gt;Combine tools fpm, aptly with aws s3, we can do it.&lt;/p&gt;&#xA;&lt;h2 id=&#34;use-fpm-to-convert-python-package-to-deb&#34;&gt;Use fpm to convert python package to deb&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://fpm.readthedocs.io/en/latest/&#34;&gt;fpm&lt;/a&gt; can transform python/gem/npm/dir/‚Ä¶ to deb/rpm/solaris/‚Ä¶ packages&lt;/p&gt;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;fpm -s python -t  deb -m xyj.asmy@gmail.com --verbose  -v 0.10.1 --python-pip /usr/local/pip Flask&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;It will transform Flask 0.10.1 package to deb. Output package will be &lt;code&gt;python-flask_0.10.1_all.deb&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Notes:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If python packages rely on some c libs like &lt;code&gt;MySQLdb&lt;/code&gt; (libmysqlclient-dev), you need to install them on the machine to build deb binary.&lt;/li&gt;&#xA;&lt;li&gt;By default fpm use easy_install to build packages, some packages like httplib2 have permission bug with easy_install, so I use pip&lt;/li&gt;&#xA;&lt;li&gt;By default, msgpack-python will be convert to &lt;code&gt;python-msgpack-python&lt;/code&gt;, I don‚Äôt like it, so add &lt;code&gt;-n python-msgpack&lt;/code&gt; to normalize the package name.&lt;/li&gt;&#xA;&lt;li&gt;Some package‚Äôs dependencies‚Äô version number is not valid(eg: celery 3.1.25 deps pytz &amp;gt;= dev), so I replace the dependencies with &lt;code&gt;--python-disable-dependency pytz -d &#39;pytz &amp;gt;= 2016.7&#39;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;fpm will not dowload package‚Äôs dependency automatically, you need to do it by your self&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;use-aptly-to-setup-deb-repository&#34;&gt;Use aptly to setup deb repository&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.aptly.info/&#34;&gt;aptly&lt;/a&gt; can help build a self host deb repository and publish it on s3.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Debug python performance issue with pyflame</title>
      <link>https://blog.monsterxx03.com/2017/06/05/debug-python-performance-issue-with-pyflame/</link>
      <pubDate>Mon, 05 Jun 2017 09:50:44 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/06/05/debug-python-performance-issue-with-pyflame/</guid>
      <description>&lt;p&gt;pyflame is an opensource tool developed by uber: &lt;a href=&#34;https://github.com/uber/pyflame&#34;&gt;https://github.com/uber/pyflame&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;It can take snapshots of running python process, combined with &lt;a href=&#34;https://raw.githubusercontent.com/brendangregg/FlameGraph/master/flamegraph.pl&#34;&gt;flamegraph.pl&lt;/a&gt;, can output flamegraph picture of python call stacks. Help analyze bottleneck of python program, needn‚Äôt inject any perf code into your application, and overhead is very low.&lt;/p&gt;&#xA;&lt;h2 id=&#34;basic-usage&#34;&gt;Basic Usage&lt;/h2&gt;&#xA;&lt;p&gt;sudo pyflame -s 10 -x -r 0.001 $pid | ./flamegraph.pl &amp;gt; perf.svg&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;-s, how many seconds to run&lt;/li&gt;&#xA;&lt;li&gt;-r, sample rate (seconds)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Your output will be something like following:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.monsterxx03.com/wp-content/uploads/2017/06/Screen-Shot-2017-06-05-at-5.18.23-PM-300x160.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Longer bar means more sample points located in it, which means this part code is slower so it has a higher chance seen by pyflame.&lt;/p&gt;&#xA;&lt;p&gt;In my case, the output graph has a long IDLE part. Pyflame can detect call stacks who are holding GIL, so if the running code doesn‚Äôt hold GIL, pyflame don‚Äôt know what it‚Äôs doing, it will label them as IDLE. Following cases will be considered as IDLE:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Your process is sleeping, do nothing.&lt;/li&gt;&#xA;&lt;li&gt;Waiting for IO.(eg: Your application is calling a very slow RPC server)&lt;/li&gt;&#xA;&lt;li&gt;Call libs written in C.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The right part is real application logic code. You can check this part to get a sense of overview performance of your code.&lt;/p&gt;&#xA;&lt;p&gt;Also you can exclude the IDLE part from graph if you don‚Äôt care about them, just apply &lt;code&gt;-x&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing data intensive application, reading notes, Part 2</title>
      <link>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</link>
      <pubDate>Wed, 17 May 2017 09:12:44 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/05/17/designing-data-intensive-application-reading-notes-part-2/</guid>
      <description>&lt;p&gt;Chapter 4, 5, 6&lt;/p&gt;&#xA;&lt;h2 id=&#34;encoding-formats&#34;&gt;Encoding formats&lt;/h2&gt;&#xA;&lt;p&gt;xml, json, msgpack are text based encoding format, they can‚Äôt carry binary bytes (useless you encode them in base64, size grows 33%). And they cary schema definition with data, wast a lot of space.&lt;/p&gt;&#xA;&lt;p&gt;thrift, protobuf are binary format, can take binary bytes, only carry data, the schema is defined with IDL(interface definition language). They have code generation tool to generate code to encode and decode data, along with check. Every field of data is binded with a tag(mapped to a field in IDL file). If a field is defined is required, it can‚Äôt by removed or change tag value, otherwise old code will not be able to decode it.&lt;/p&gt;&#xA;&lt;p&gt;avro (used in hadoop), have a write schema and a read schema, when store a large file in avro format(contain many records with same schema), the avro write schama file is appended to the data. If use avro in RPC, the avro schema is exchanged during connection setup. When decoding avro, the lib will look both write schema and read schema, and translate write schema into read schema. Forward compatibility means that you can have a new version of the schema as writer and an old version of the schema as reader, backward compatibility means that you can have a new version of the schema as reader and an old version as writer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing data intensive application, reading notes, Part 1</title>
      <link>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</link>
      <pubDate>Thu, 04 May 2017 16:27:52 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/05/04/designing-data-intensive-application-reading-notes-part-1/</guid>
      <description>&lt;p&gt;Notes when reading chapter 2 ‚ÄúData models and query languages‚Äù, chapter 3 ‚ÄúStorage and retrieval‚Äù&lt;/p&gt;</description>
    </item>
    <item>
      <title>Infrastructure as Code</title>
      <link>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</link>
      <pubDate>Fri, 21 Apr 2017 16:25:07 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/04/21/infrastructure-as-code/</guid>
      <description>&lt;p&gt;Create virtual resource on AWS is very convenient, but how to manage them will be a problem when your size grow.&lt;/p&gt;&#xA;&lt;p&gt;You will come to:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to explain the detail online settings for your colleagues (like: how our prod vpc is setup?what‚Äôs the DHCP option set?), navigate around AWS console is okay, but not convenient.&lt;/li&gt;&#xA;&lt;li&gt;Who did what to which resource at when? AWS have a service called &lt;code&gt;Config&lt;/code&gt;, can be used to track this change, but if you want to make things as clear as viewing git log, still a lot of works to do.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Ideally, we should manage AWS resources like code, all changes kept in VCS, so called &lt;code&gt;Infrastructure as Code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;I‚Äôve tried three ways to do it:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ansible&lt;/li&gt;&#xA;&lt;li&gt;CloudFormation&lt;/li&gt;&#xA;&lt;li&gt;terraform&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In this article, I&amp;rsquo;ll compare them, however, the conclusion is to use terraform üôÇ&lt;/p&gt;&#xA;&lt;h2 id=&#34;ansible&#34;&gt;Ansible&lt;/h2&gt;&#xA;&lt;p&gt;Provision tools, like ansible/chef/puppet, all can be used to create aws resources, but they have some common problems:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hard to track changes after bootstrap.&lt;/li&gt;&#xA;&lt;li&gt;No confident what it will do to existing resources.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For example, I define a security group in ansibble:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;ec2_group:&#xA;  name: &amp;quot;web&amp;quot;&#xA;  description: &amp;quot;security group in web&amp;quot;&#xA;  vpc_id: &amp;quot;vpc-xxx&amp;quot;&#xA;  region: &amp;quot;us-east-1&amp;quot;&#xA;  rules:&#xA;    - proto: tcp&#xA;      from_port: 80&#xA;      to_port: 80&#xA;      cidr_ip: 0.0.0.0/0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;It will create a security group named ‚Äúweb‚Äù in vpc-xxx. At first glance, it‚Äôs convenient and straightforward.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concurrency in Go, Reading Notes</title>
      <link>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</link>
      <pubDate>Wed, 19 Apr 2017 16:26:58 +0000</pubDate>
      <guid>https://blog.monsterxx03.com/2017/04/19/concurrency-in-go-reading-notes/</guid>
      <description>&lt;p&gt;A few notes taken when reading &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
