<!DOCTYPE html>
<html lang="zh-cn" >
<head>
	<meta name="generator" content="Hugo 0.82.0" />
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content="monsterxx03"/>

  

  

  
  <link rel="canonical" href="https://blog.monsterxx03.com/"/>

  
  <link rel="alternate" type="application/rss&#43;xml" href="https://blog.monsterxx03.com/index.xml">
  

  <title>Shining Moon</title>

  <link rel="shortcut icon" href="https://blog.monsterxx03.com/images/favicon.ico"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/animate.min.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/remixicon.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/zozo.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/highlight.css"/>

  
  
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/home.css">
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">Home</a>
      </li>
      
      <li>
        <a href="/posts/">Archive</a>
      </li>
      
      <li>
        <a href="/about/">About</a>
      </li>
      
      <li>
        <a href="/tags">Tags</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://blog.monsterxx03.com">
          <span>Shining Moon</span>
          <img width="90px" src="https://blog.monsterxx03.com/logo.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">百种弊病,皆从懒生</p>
      <div class="my_socials">
        
        
        <a href="https://github.com/monsterxx03" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        
        
        <a href="https://blog.monsterxx03.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    


<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/14/prometheus-on-k8s/'>Prometheus on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>Why move to prometheus? 把生产环境迁移到 k8s 的第一步是要搞定监控, 目前线上监控用的是商业的 datadog, 在 container 环境下 datadog 监控还要按 container 数目收费, 单 host 只有 10 个的额度, 超过要加钱, 高密度部署下很不划算. 一个 server 跑 20 个以上 container 是很正常的事情, 单台 server 的监控费用立马翻倍. tracing 这块之前用的也是 datadog, 但太贵了,一直也想换开源实现, 索性监控报警也换了, 踩一把坑吧. vendor lock 总是不爽的&hellip; Metrics in k8s 先不提 prometheus, k8s 中 metrics 来源有那么几个: metrics-serever metrics-server (取代 heapster), 从 node 上 kubelet 的 summary api 抓取......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.14</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/prometheus/">prometheus</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
          <a href="https://blog.monsterxx03.com/tags/server-infra/">server-infra</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/07/kubeconfig-%E5%92%8C-aws-named-profile-%E7%AE%A1%E7%90%86%E7%9A%84-tips/'>kubeconfig 和 aws named profile 管理的 tips</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>我有两个 EKS 集群 (sandbox + production), 这两个集群分处两个 aws 帐号中. 所以管理的时候也需要两套 aws credential. 同时我用 helm-secrets 来管理 helm charts 中需要加密的一些配置. helm-secrets 只是 sops 的一个 shell wrapper, 实际加密是通过 sops 进行的. sops 支持 aws KMS, gcp KMS, azure key vault.. 等加密服务. 我用的是 aws KMS, 在 KMS 里创建一个 key, 授权允许我这个 iam 帐号能用它来进行加解密. 这带来了一个问题, kubectl 和 helm-secrets 都需要 aws credential, 如果两边用的不一样就会执行失败. 我统一使用 aws 的 named profiles 来管理 credential. 不在环境变量里设 aws 的 access key/secret key(如果设置了, 优先......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.07</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/29/jenkins-on-k8s/'>Jenkins on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>最近在把 jenkins 迁移到 k8s, 具体怎么 setup 的不赘述了(helm chart, jenkins home 目录挂pvc, jenkins kubernetes-plugin). jenkins 跑 k8s 好处是可以方便得做分布式 build, 每次 trigger 一个 job 的时候自动起一个 pod 作为 jenkins slave agent, 结束了自动删掉. 在 aws 上结合 cluster-autoscaler 可以极大得扩展 ci 的并行能力, 降低成本. 记录一点过程中的坑. 装上 kubernetes-plugin 后,要想让 jenkins 的 job 在 pod 中跑, 必须用 pipeline 的方式编写 job 定义. script 和 declarative 两种方式都支持启动 pod. 如果用 shell 的方式编写, 不会跑在 pod 里，会直接在 master 的 workspace 里 build. declarative 方式的例子: pipeline { agent { kubernetes { label 'test-deploy' yamlFile 'test-deploy.yaml'......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.29</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/jenkins/">jenkins</a>
          
          <a href="https://blog.monsterxx03.com/tags/server-infra/">server-infra</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/12/k8s-volume-resize-on-eks/'>K8s Volume Resize on EKS</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>从 k8s 1.8 开始支持 PersistentVolumeClaimResize. 但 api 是 alpha 状态, 默认不开启, eks launch 的时候版本是 1.10, 因为没法改 control plane, 所以没法直接在 k8s 内做 ebs 扩容. 后来升级到了 1.11, 这个 feature 默认被打开了, 尝试了下直接在 EKS 内做 ebs 的扩容. 注意: 这个 feature 只能对通过 pvc 管理的 volume 做扩容, 如果直接挂的是 pv, 只能自己按传统的 ebs 扩容流程在 eks 之外做. 用来创建 pvc 的 storageclass 上必须设置 allowVolumeExpansion 为 true 在 eks 上使用 pv/pvc, 对于需要 retain 的 volume, 我一般的流程是: 在 eks 之外手工创建 ebs volume. 在 eks 中创建 pv, 指向 ebs 的 volume id 在 eks 中创建 pvc, 指向 pv 示例 yaml:......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.12</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
          <a href="https://blog.monsterxx03.com/tags/aws/">aws</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/10/snet-dev-note/'>snet dev note</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>完成 SNET 初版后又做了些后续更新, 记录一点. 支持 http tunnel 配置文件里增加一个 proxy-type 选项, 默认为 ss, 可改成 http, 这样可以将 支持 http tunnel 的代理服务器作为 upstream(例如 squid). 填上 http-proxy- 开头 的选项就行. 实现上 client 端要对接 http tunnel 非常简单: client 发送请求: Connect tgt-host:tgt-port HTTP/1.1 server response: HTTP/1.1 200, 即表示 server 端支持 http tunnel client 后续向该 tcp connection 写入的数据都会被 server 转发到 tgt-host:tgt-port 改动的时候把 upstream proxy 的部分重构了一下, 抽了个 Proxy interface 出来, 后续想对接其他协议方便扩展. 对 udp 支持的尝试 对 tcp 流量的转发能通过 iptables REDIRECT......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.10</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/network/">network</a>
          
          <a href="https://blog.monsterxx03.com/tags/golang/">golang</a>
          
          <a href="https://blog.monsterxx03.com/tags/snet/">snet</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/03/31/snet-transparent-ss-proxy-on-linux/'>snet: transparent ss proxy on Linux</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>日常使用 Linux 工作, Linux 下实现全局透明代理可以用 iptables + ss-redir, 要有比较好的上网体验还需要 ChinaDNS 配合 dnsmasq, 这一整套在路由器上搞一遍就算了, 在本地太麻烦了. 仔细想想这几个加起来的功能实现起来也并不复杂, 前阵子就写了个小东西, 用一个进程完成全局透明代理 + ChinaDNS + 国内外分流: https://github.com/monsterxx03/snet 目前的限制: 不支持 ipv6 只支持 tcp (因为我的测试服务器不支持 udp, 以后再加上吧) 上游 server 只支持 ss 目的是一个进程 + 一个配置文件完成所有事情. 需要的 iptable 规则也全部内置了(包括......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.03.31</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/network/">network</a>
          
          <a href="https://blog.monsterxx03.com/tags/golang/">golang</a>
          
          <a href="https://blog.monsterxx03.com/tags/snet/">snet</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/03/28/celery-time-limit-%E7%9A%84%E5%9D%91/'>Celery Time Limit 的坑</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>之前用 celery 做的 task 都是一些很简单轻量级的 task, 从来没触发过 timeout, 最近加入了一些复杂很耗时的 task, 碰到一些 time limit 的坑. celery 中 time limit 有两种, soft_time_limit 和 hard_time_limit, 区别是 soft_time_limit 会在内部抛一个 Exception, task 可以 catch 自行处理. hard time limit 没法被 catch. 使用如下: from myapp import app from celery.exceptions import SoftTimeLimitExceeded @app.task def mytask(): try: do_work() except SoftTimeLimitExceeded: clean_up_in_a_hurry() 我 celery pool 用的是 gevent, 实际上在现在的实现里 gevent 做 worker pool 的时候会忽略 soft_time_limit, 只有 hard_time_limit 会被触发(通过 gevent.Timeout 实现). 坑爹的是文档里写的是错的: http://docs.celeryproject.org/en/latest/userguide/workers.html#time-limit soft_time_limit 只在 prefork pool 里支持. 我现在想让 celery 把这个 hard timeout 的情况 report 到 sentry, 看了圈代码并没法......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.03.28</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/python/">python</a>
          
          <a href="https://blog.monsterxx03.com/tags/celery/">celery</a>
          
          <a href="https://blog.monsterxx03.com/tags/code-infra/">code-infra</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/02/12/%E7%AE%A1%E7%90%86%E8%B4%9F%E8%BD%BD/'>管理负载</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>最近在看 google 的 &lt;The Site Reliablity Workbook&gt;, 其中有一章是&quot;Manage load&quot;, 内容还挺详细的, 结合在 aws 上的经验做点笔记. Load Balancing 流量的入口是负载均衡, 最最简单的做法是在 DNS 上做 round robin, 但这样很依赖 client, 不同的 client 可能不完全遵守 DNS 的 TTL, 当地的 ISP 也会有缓存. google 用 anycast 技术在自己的网络中通过 BGP 给一个域名发布多个 endpoint, 共享一个 vip(virtual ip), 通过 BGP routing 来将用户的数据包发送到最近的 frontend server, 以此来减少 latency. 但只依赖 BGP 会带来两个问题: 某个地区的用户过多会给最近的 frontend server 带来过高的负......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.02.12</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/gce/">gce</a>
          
          <a href="https://blog.monsterxx03.com/tags/aws/">aws</a>
          
          <a href="https://blog.monsterxx03.com/tags/book/">book</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2018/12/29/%E4%BB%8E%E5%8E%BB%E5%B9%B4%E7%9A%84%E4%B8%80%E4%B8%AApatch%E8%AF%B4%E8%B5%B7/'>从去年的一个patch说起</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>去年对线上业务做了一些性能优化, 当时把 http client 从 requests 换成了 geventhttpclient , 上线后发起 rpc 调用的 server 整体负载低了很多, 但 client 端 latency 却高了很多, 经过 debug 觉得问题是 geventhttpclient 把 header 和 body 通过两次 sock send 发出的额外开销造成的, 尝试 修改成一次 send 后 latency 就恢复了: https://github.com/gwik/geventhttpclient/pull/85 最近在调试 gunicorn 的代码时候, 看到它建立 socket 的时候设置了 TCP_NODELAY, 在很多项目里看到过这个 tcp option, 但没细究过, man tcp 得知是用来关闭 tcp 里的 nagle 算法的. nagle 在 linux 的 默认 tcp 协议栈里是开启的, 当发送的数据包 size 小于 mss 的时候会在内存里 buffer......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2018.12.29</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/http/">http</a>
          
          <a href="https://blog.monsterxx03.com/tags/tcp/">tcp</a>
          
          <a href="https://blog.monsterxx03.com/tags/code-infra/">code-infra</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2018/12/16/kubernetes-%E4%B8%AD%E7%9A%84-pod-%E8%B0%83%E5%BA%A6/'>Kubernetes 中的 pod 调度</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>定义 pod 的时候通过添加 node selector 可以让 pod 调度到有特定 label 的 node 上去, 这是最简单的调度方式. 其他还有更复杂的调度方式: node-taints/tolerations, node-affinity, pod-affinity, 来达到让某些类型的 pod 调度到一起, 让某些类型的 pod 不跑一起的效果. Taints and Tolerations 如果 node 有 taints, 那只有能 tolerate 这些 taints 的 pod 才能调度到上面. taint 的基本格式是: &lt;key&gt;&lt;operator&gt;&lt;value&gt;:&lt;effect&gt; kubectl describe node xxx 可以看到节点的 taints, 比如 master 节点上会有: Taints: node-role.kubernetes.io/master:NoSchedule 这里 key 是 node-role.kubernetes.io/master, 没有等号和 value, operator 就是 Exists , effect 是 NoSchedule. master 节点上的这条 taint 就定义了只有能 tolerate 它的 pod 能调度到上面, 一般都是些系统 pod. 比如看......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2018.12.16</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>


<div class="pagination">
  
  
  <a href="/page/3/" class="pre">
    返回上一页
  </a>
  
  
  <a href="/page/5/" class="next">
    阅读更多文章
  </a>
  
  
</div>

  </div>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="https://blog.monsterxx03.com/js/jquery-3.3.1.min.js"></script>
<script src="https://blog.monsterxx03.com/js/zozo.js"></script>
<script src="https://blog.monsterxx03.com/js/highlight.pack.js"></script>
<link  href="https://blog.monsterxx03.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://blog.monsterxx03.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-98667627-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</body>
</html>
