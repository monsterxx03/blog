<!DOCTYPE html>
<html lang="zh-cn" >
<head>
	<meta name="generator" content="Hugo 0.60.0" />
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content="monsterxx03"/>

  

  

  
  <link rel="canonical" href="https://blog.monsterxx03.com/"/>

  
  <link rel="alternate" type="application/rss&#43;xml" href="https://blog.monsterxx03.com/index.xml">
  

  <title>Shining Moon</title>

  <link rel="shortcut icon" href="https://blog.monsterxx03.com/images/favicon.ico"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/animate.min.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/remixicon.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/zozo.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/highlight.css"/>

  
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">Home</a>
      </li>
      
      <li>
        <a href="/posts/">Archive</a>
      </li>
      
      <li>
        <a href="/about/">About</a>
      </li>
      
      <li>
        <a href="/tags">Tags</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://blog.monsterxx03.com">
          <span>Shining Moon</span>
          <img width="90px" src="https://blog.monsterxx03.com/logo.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">百种弊病,皆从懒生</p>
      <div class="my_socials">
        
        
        <a href="https://github.com/monsterxx03" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        
        
        <a href="https://blog.monsterxx03.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    


<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/'> 迁移到 k8s 过程中碰到的问题</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &hellip;&hellip; 记录一下(大多都不是 k8s 的问题). nginx ingress controller 的问题 zero-downtime pods upgrade 默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip), 和直接使用 service cluster ip 比, 好处是: 可以支持 sticky session 可以用 round robin 之外的负载均衡算法 具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip list(nginx 内置的 lua 来修改内存中的 ip list) 这样的问题是, 从 pod 被干掉到 nginx 更新之间......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.07.23</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/'>K8S: 剩下的问题</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程, 但需要持续关注. DNS timeout and conntrack 看到有个关于 DNS 的issue: #56903 现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候, 要通过 iptables 把 coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在 conntrack 内记录映射关系. 这可能会带来两个问题: conntrack table 被 udp 的 dns 查询填满 udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.06.30</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/06/20/snet-dev-note-support-macos/'>snet dev note: support MacOS</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>这两天得了空, 让 snet 支持了下 MacOS. snet 的大致原理是通过系统防火墙的流量重定向功能,将所有去往国外的流量导到 snet 监听的端口, 在程序内部 将流量传递给上游的 proxy server(ss, http), 拿到响应后再回给客户端. 实现关键是要在 snet 内部获取到流量的原目标地址, 因为重定向之后 tcp connnection 的目标地址变成了 snet 监听 的地址. Linux 上的实现，以前讲过: https://blog.monsterxx03.com/2019/03/31/snet-transparent-ss-proxy-on-linux/ 是通过 SO_ORIGINAL_DST 这个 socket option 实现的. MacOS 上没有 iptables, 类似的工具是系统自带的 pfctl, 捣鼓了一下也能实现一样的功能. 用 pfctl 做流量重定向 pfctl 的文档可......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.06.20</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/network/">network</a>
          
          <a href="https://blog.monsterxx03.com/tags/golang/">golang</a>
          
          <a href="https://blog.monsterxx03.com/tags/snet/">snet</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/30/random-talk/'>Random Talk</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>Just some random complains and notes about server infra management. I think those are my motivations to move to kubernetes.
Won't explain k8s or docker in detail, and how they solve those problems in this post.
Infrastructure level(on AWS) We use following services provided by AWS.
 Compute:  EC2 AutoScaling Group Lambda   network:  VPC (SDN network) DNS (route53) CDN (CloudFront)   Loadbalancer:  ELB (L4) NLB (L4, ELB successor, support static IP) ALB (L7)   Storage:  EBS (block storage) EFS (hosted NFS) RDS(MySQl/PostgreSQL &hellip;) Redshift (data warehouse) DynamoDB (KV) S3 (object storage) Glacier (cheap archive storage)   Web Firewall (WAF) Monitor (CloudWatch) DMS (ETL) &hellip;  For infra management, in early days, we just click, click, click&hellip; or write some simple scripts to call AWS api.
With infra resources growing, management became complex, a concept called Infrastructure as Code rising.
AWS provides CloudFormation as orchestration tool, but we use terraform (for short: CloudFormation sucks, for long: Infrastructure as Code)
So far, not bad.(tweak those services internally is another story&hellip; never belive work out of box)
Application level  configuration management (setup nginx, jenkins, redis, twemproxy, ElasticSearch or WTF..) CI/CD dependency management  They're complicated, people developped bunch of tools to handle: puppet, chef, ansible, saltstack &hellip;......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.30</span>
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/26/centralized-logging-on-k8s/'>Centralized Logging on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>搞定了监控, 下一步在 k8s 上要做的是中心化日志, 大体看了下, 感兴趣的有两个选择: ELK 套件, 或fluent-bit + fluentd. ELK 那套好处是, 可以把监控和日志一体化, filebeat 收集日志, metricbeat 收集 metrics, 统一存储在 ElasticSearch 里, 通过第三方项目elastalert 可以做报警，也能在 kibana 里集成界面. 坏处是 ElasticSearch 存储成本高, 吃资源. 我们对存储的日志使用需求基本就是 debug, 没有特别复杂的BI需求, 上一整套 ELK 还是太重了. 选择 fluent-bit + fluentd 还有个好处是, 之前内部有套收集 m......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.26</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/fluentd/">fluentd</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/14/prometheus-on-k8s/'>Prometheus on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>Why move to prometheus? 把生产环境迁移到 k8s 的第一步是要搞定监控, 目前线上监控用的是商业的 datadog, 在 container 环境下 datadog 监控还要按 container 数目收费, 单 host 只有 10 个的额度, 超过要加钱, 高密度部署下很不划算. 一个 server 跑 20 个以上 container 是很正常的事情, 单台 server 的监控费用立马翻倍. tracing 这块之前用的也是 datadog, 但太贵了,一直也想换开源实现, 索性监控报警也换了, 踩一把坑吧. vendor lock 总是不爽的&hellip; Metrics in k8s 先不提 prometheus, k8s 中 metrics 来源有那么几个: metrics-serever metrics-server (取代 heapster), 从 node 上 kubelet 的 summary api 抓取......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.14</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/prometheus/">prometheus</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/07/kubeconfig-%E5%92%8C-aws-named-profile-%E7%AE%A1%E7%90%86%E7%9A%84-tips/'>kubeconfig 和 aws named profile 管理的 tips</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>我有两个 EKS 集群 (sandbox + production), 这两个集群分处两个 aws 帐号中. 所以管理的时候也需要两套 aws credential. 同时我用 helm-secrets 来管理 helm charts 中需要加密的一些配置. helm-secrets 只是 sops 的一个 shell wrapper, 实际加密是通过 sops 进行的. sops 支持 aws KMS, gcp KMS, azure key vault.. 等加密服务. 我用的是 aws KMS, 在 KMS 里创建一个 key, 授权允许我这个 iam 帐号能用它来进行加解密. 这带来了一个问题, kubectl 和 helm-secrets 都需要 aws credential, 如果两边用的不一样就会执行失败. 我统一使用 aws 的 named profiles 来管理 credential. 不在环境变量里设 aws 的 access key/secret key(如果设置了, 优先......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.07</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/29/jenkins-on-k8s/'>Jenkins on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>最近在把 jenkins 迁移到 k8s, 具体怎么 setup 的不赘述了(helm chart, jenkins home 目录挂pvc, jenkins kubernetes-plugin). jenkins 跑 k8s 好处是可以方便得做分布式 build, 每次 trigger 一个 job 的时候自动起一个 pod 作为 jenkins slave agent, 结束了自动删掉. 在 aws 上结合 cluster-autoscaler 可以极大得扩展 ci 的并行能力, 降低成本. 记录一点过程中的坑. 装上 kubernetes-plugin 后,要想让 jenkins 的 job 在 pod 中跑, 必须用 pipeline 的方式编写 job 定义. script 和 declarative 两种方式都支持启动 pod. 如果用 shell 的方式编写, 不会跑在 pod 里，会直接在 master 的 workspace 里 build. declarative 方式的例子: pipeline { agent { kubernetes { label 'test-deploy' yamlFile 'test-deploy.yaml'......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.29</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/jenkins/">jenkins</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/12/k8s-volume-resize-on-eks/'>K8s Volume Resize on EKS</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>从 k8s 1.8 开始支持 PersistentVolumeClaimResize. 但 api 是 alpha 状态, 默认不开启, eks launch 的时候版本是 1.10, 因为没法改 control plane, 所以没法直接在 k8s 内做 ebs 扩容. 后来升级到了 1.11, 这个 feature 默认被打开了, 尝试了下直接在 EKS 内做 ebs 的扩容. 注意: 这个 feature 只能对通过 pvc 管理的 volume 做扩容, 如果直接挂的是 pv, 只能自己按传统的 ebs 扩容流程在 eks 之外做. 用来创建 pvc 的 storageclass 上必须设置 allowVolumeExpansion 为 true 在 eks 上使用 pv/pvc, 对于需要 retain 的 volume, 我一般的流程是: 在 eks 之外手工创建 ebs volume. 在 eks 中创建 pv, 指向 ebs 的 volume id 在 eks 中创建 pvc, 指向 pv 示例 yaml:......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.12</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
          <a href="https://blog.monsterxx03.com/tags/aws/">aws</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/10/snet-dev-note/'>snet dev note</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>完成 SNET 初版后又做了些后续更新, 记录一点. 支持 http tunnel 配置文件里增加一个 proxy-type 选项, 默认为 ss, 可改成 http, 这样可以将 支持 http tunnel 的代理服务器作为 upstream(例如 squid). 填上 http-proxy- 开头 的选项就行. 实现上 client 端要对接 http tunnel 非常简单: client 发送请求: Connect tgt-host:tgt-port HTTP/1.1 server response: HTTP/1.1 200, 即表示 server 端支持 http tunnel client 后续向该 tcp connection 写入的数据都会被 server 转发到 tgt-host:tgt-port 改动的时候把 upstream proxy 的部分重构了一下, 抽了个 Proxy interface 出来, 后续想对接其他协议方便扩展. 对 udp 支持的尝试 对 tcp 流量的转发能通过 iptables REDIRECT......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.10</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/network/">network</a>
          
          <a href="https://blog.monsterxx03.com/tags/golang/">golang</a>
          
          <a href="https://blog.monsterxx03.com/tags/snet/">snet</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>


<div class="pagination">
  
  
  <a href="/" class="pre">
    返回上一页
  </a>
  
  
  <a href="/page/3/" class="next">
    阅读更多文章
  </a>
  
  
</div>

  </div>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="https://blog.monsterxx03.com/js/jquery-3.3.1.min.js"></script>
<script src="https://blog.monsterxx03.com/js/zozo.js"></script>
<script src="https://blog.monsterxx03.com/js/highlight.pack.js"></script>
<link  href="https://blog.monsterxx03.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://blog.monsterxx03.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-98667627-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</body>
</html>
