<!DOCTYPE html>
<html lang="zh-cn" >
<head>
	<meta name="generator" content="Hugo 0.73.0" />
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content="monsterxx03"/>

  

  

  
  <link rel="canonical" href="https://blog.monsterxx03.com/"/>

  
  <link rel="alternate" type="application/rss&#43;xml" href="https://blog.monsterxx03.com/index.xml">
  

  <title>Shining Moon</title>

  <link rel="shortcut icon" href="https://blog.monsterxx03.com/images/favicon.ico"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/animate.min.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/remixicon.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/zozo.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/highlight.css"/>

  
  
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/home.css">
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">Home</a>
      </li>
      
      <li>
        <a href="/posts/">Archive</a>
      </li>
      
      <li>
        <a href="/about/">About</a>
      </li>
      
      <li>
        <a href="/tags">Tags</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://blog.monsterxx03.com">
          <span>Shining Moon</span>
          <img width="90px" src="https://blog.monsterxx03.com/logo.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">百种弊病,皆从懒生</p>
      <div class="my_socials">
        
        
        <a href="https://github.com/monsterxx03" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        
        
        <a href="https://blog.monsterxx03.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    


<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/07/28/pyflame-%E7%9A%84-kubectl-plugin/'>Pyflame 的 kubectl plugin</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>pyflame 可以比较方便得生成 python 进程的调用函数栈火焰图, 来 debug 一些性能瓶颈, 做了个 kubectl 的小插件, 来方便得对 k8s pod 中的 python 进程进行 debug: https://github.com/monsterxx03/kube-pyflame 直接把 svg 文件下载到本地. 要对 pod 中的 python 进程进行 profiling, 大致思路有两种: 直接在 container 内使用 pyflame, 但这样要把 pyflame 做到所有的 base 镜像里去, 而且目标 container要在 SecurityContext 加上 SYS_PTRACE 在 host 上用 pyflame debug 对应进程, pyflame 自身是能识别跑在 container 里的进程, 自动执行 setns 的. 我希望保持线上环境干净, 最后的做法是, 把 pyflame 单独做一个镜像, 先用 kubectl 找到目......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.07.28</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/python/">python</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/07/24/%E6%9D%82%E8%B0%88/'>杂谈</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>回首一看, 大半年没写过杂谈系列了, 今年都没休假, 也是醉了. 随便唠叨几句. 小区门口的大爷养的花猫生了二胎, 长特别好看, 随它们妈, 颜值吊打白猫大哥, 每天下班路过看它们在马路口咬报纸也是很有意思. 犹豫了几天要不要抱只回来, 犹豫着犹豫着就都被别人领走了, 叹. 最近在看西泽保彦的书, 看完了一本&lt;死了7次的男人&gt;, 正在读&lt;解体诸因&gt;. 简直是发现了宝藏, 好久没有看到喜欢的推理小说了, 真的是......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.07.24</span>
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/'> 迁移到 k8s 过程中碰到的问题</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &hellip;&hellip; 记录一下(大多都不是 k8s 的问题). nginx ingress controller 的问题 zero-downtime pods upgrade 默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip), 和直接使用 service cluster ip 比, 好处是: 可以支持 sticky session 可以用 round robin 之外的负载均衡算法 具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip list(nginx 内置的 lua 来修改内存中的 ip list) 这样的问题是, 从 pod 被干掉到 nginx 更新之间......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.07.23</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/'>K8S: 剩下的问题</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程, 但需要持续关注. DNS timeout and conntrack 看到有个关于 DNS 的issue: #56903 现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候, 要通过 iptables 把 coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在 conntrack 内记录映射关系. 这可能会带来两个问题: conntrack table 被 udp 的 dns 查询填满 udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.06.30</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/06/20/snet-dev-note-support-macos/'>snet dev note: support MacOS</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>这两天得了空, 让 snet 支持了下 MacOS. snet 的大致原理是通过系统防火墙的流量重定向功能,将所有去往国外的流量导到 snet 监听的端口, 在程序内部 将流量传递给上游的 proxy server(ss, http), 拿到响应后再回给客户端. 实现关键是要在 snet 内部获取到流量的原目标地址, 因为重定向之后 tcp connnection 的目标地址变成了 snet 监听 的地址. Linux 上的实现，以前讲过: https://blog.monsterxx03.com/2019/03/31/snet-transparent-ss-proxy-on-linux/ 是通过 SO_ORIGINAL_DST 这个 socket option 实现的. MacOS 上没有 iptables, 类似的工具是系统自带的 pfctl, 捣鼓了一下也能实现一样的功能. 用 pfctl 做流量重定向 pfctl 的文档可......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.06.20</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/network/">network</a>
          
          <a href="https://blog.monsterxx03.com/tags/golang/">golang</a>
          
          <a href="https://blog.monsterxx03.com/tags/snet/">snet</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/30/random-talk/'>Random Talk</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>Just some random complains and notes about server infra management. I think those are my motivations to move to kubernetes.
Won&rsquo;t explain k8s or docker in detail, and how they solve those problems in this post.
Infrastructure level(on AWS) We use following services provided by AWS.
 Compute:  EC2 AutoScaling Group Lambda   network:  VPC (SDN network) DNS (route53) CDN (CloudFront)   Loadbalancer:  ELB (L4) NLB (L4, ELB successor, support static IP) ALB (L7)   Storage:  EBS (block storage) EFS (hosted NFS) RDS(MySQl/PostgreSQL &hellip;) Redshift (data warehouse) DynamoDB (KV) S3 (object storage) Glacier (cheap archive storage)   Web Firewall (WAF) Monitor (CloudWatch) DMS (ETL) &hellip;  For infra management, in early days, we just click, click, click&hellip; or write some simple scripts to call AWS api.
With infra resources growing, management became complex, a concept called Infrastructure as Code rising.
AWS provides CloudFormation as orchestration tool, but we use terraform (for short: CloudFormation sucks, for long: Infrastructure as Code)
So far, not bad.(tweak those services internally is another story&hellip; never belive work out of box)
Application level  configuration management (setup nginx, jenkins, redis, twemproxy, ElasticSearch or WTF..) CI/CD dependency management  They&rsquo;re complicated, people developped bunch of tools to handle: puppet, chef, ansible, saltstack &hellip;......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.30</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/lang-en/">lang-en</a>
          
          <a href="https://blog.monsterxx03.com/tags/server-infra/">server-infra</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/26/centralized-logging-on-k8s/'>Centralized Logging on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>搞定了监控, 下一步在 k8s 上要做的是中心化日志, 大体看了下, 感兴趣的有两个选择: ELK 套件, 或fluent-bit + fluentd. ELK 那套好处是, 可以把监控和日志一体化, filebeat 收集日志, metricbeat 收集 metrics, 统一存储在 ElasticSearch 里, 通过第三方项目elastalert 可以做报警，也能在 kibana 里集成界面. 坏处是 ElasticSearch 存储成本高, 吃资源. 我们对存储的日志使用需求基本就是 debug, 没有特别复杂的BI需求, 上一整套 ELK 还是太重了. 选择 fluent-bit + fluentd 还有个好处是, 之前内部有套收集 m......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.26</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/fluentd/">fluentd</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/14/prometheus-on-k8s/'>Prometheus on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>Why move to prometheus? 把生产环境迁移到 k8s 的第一步是要搞定监控, 目前线上监控用的是商业的 datadog, 在 container 环境下 datadog 监控还要按 container 数目收费, 单 host 只有 10 个的额度, 超过要加钱, 高密度部署下很不划算. 一个 server 跑 20 个以上 container 是很正常的事情, 单台 server 的监控费用立马翻倍. tracing 这块之前用的也是 datadog, 但太贵了,一直也想换开源实现, 索性监控报警也换了, 踩一把坑吧. vendor lock 总是不爽的&hellip; Metrics in k8s 先不提 prometheus, k8s 中 metrics 来源有那么几个: metrics-serever metrics-server (取代 heapster), 从 node 上 kubelet 的 summary api 抓取......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.14</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/prometheus/">prometheus</a>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/05/07/kubeconfig-%E5%92%8C-aws-named-profile-%E7%AE%A1%E7%90%86%E7%9A%84-tips/'>kubeconfig 和 aws named profile 管理的 tips</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>我有两个 EKS 集群 (sandbox + production), 这两个集群分处两个 aws 帐号中. 所以管理的时候也需要两套 aws credential. 同时我用 helm-secrets 来管理 helm charts 中需要加密的一些配置. helm-secrets 只是 sops 的一个 shell wrapper, 实际加密是通过 sops 进行的. sops 支持 aws KMS, gcp KMS, azure key vault.. 等加密服务. 我用的是 aws KMS, 在 KMS 里创建一个 key, 授权允许我这个 iam 帐号能用它来进行加解密. 这带来了一个问题, kubectl 和 helm-secrets 都需要 aws credential, 如果两边用的不一样就会执行失败. 我统一使用 aws 的 named profiles 来管理 credential. 不在环境变量里设 aws 的 access key/secret key(如果设置了, 优先......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.05.07</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>

<div class="post animated fadeInDown">
  <div class="post_title">
    <h2><a href='/2019/04/29/jenkins-on-k8s/'>Jenkins on K8S</a></h2>
  </div>
  
  <div class="list">
    <div class="post_content markdown">
      <p>最近在把 jenkins 迁移到 k8s, 具体怎么 setup 的不赘述了(helm chart, jenkins home 目录挂pvc, jenkins kubernetes-plugin). jenkins 跑 k8s 好处是可以方便得做分布式 build, 每次 trigger 一个 job 的时候自动起一个 pod 作为 jenkins slave agent, 结束了自动删掉. 在 aws 上结合 cluster-autoscaler 可以极大得扩展 ci 的并行能力, 降低成本. 记录一点过程中的坑. 装上 kubernetes-plugin 后,要想让 jenkins 的 job 在 pod 中跑, 必须用 pipeline 的方式编写 job 定义. script 和 declarative 两种方式都支持启动 pod. 如果用 shell 的方式编写, 不会跑在 pod 里，会直接在 master 的 workspace 里 build. declarative 方式的例子: pipeline { agent { kubernetes { label 'test-deploy' yamlFile 'test-deploy.yaml'......</p>
    </div>
  </div>
  
  <div class="post_footer">
    <div class="meta">
      <div class="info">
        <span class="field">
          <i class="remixicon-map-pin-time-line"></i>
          <span class="date">2019.04.29</span>
        </span>
        
        <span class="field tags">
          <i class="remixicon-stack-line"></i>
          
          <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
          
          <a href="https://blog.monsterxx03.com/tags/jenkins/">jenkins</a>
          
        </span>
        
      </div>
    </div>
  </div>
</div>


<div class="pagination">
  
  
  <a href="/page/2/" class="pre">
    返回上一页
  </a>
  
  
  <a href="/page/4/" class="next">
    阅读更多文章
  </a>
  
  
</div>

  </div>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="https://blog.monsterxx03.com/js/jquery-3.3.1.min.js"></script>
<script src="https://blog.monsterxx03.com/js/zozo.js"></script>
<script src="https://blog.monsterxx03.com/js/highlight.pack.js"></script>
<link  href="https://blog.monsterxx03.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://blog.monsterxx03.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-98667627-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</body>
</html>
