<!DOCTYPE html>
<html lang="zh-cn" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content="monsterxx03"/>

  
  <meta name="description" content="开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &amp;hellip;&amp;hellip; 记录一下(大多都不是 k8s 的问题). nginx ingress controller 的问题 zero-downtime pods upgrade 默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip), 和直接使用 service cluster ip 比, 好处是: 可以支持 sticky session 可以用 round robin 之外的负载均衡算法 具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip list(nginx 内置的 lua 来修改内存中的 ip list) 这样的问题是, 从 pod 被干掉到 nginx 更新之间"/>
  

  

  
  <link rel="canonical" href="https://blog.monsterxx03.com/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>

  

  <title> 迁移到 k8s 过程中碰到的问题 &middot; Shining Moon</title>

  <link rel="shortcut icon" href="https://blog.monsterxx03.com/images/favicon.ico"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/animate.min.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/remixicon.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/zozo.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/highlight.css"/>

  
  
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/home.css">
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">Home</a>
      </li>
      
      <li>
        <a href="/posts/">Archive</a>
      </li>
      
      <li>
        <a href="/about/">About</a>
      </li>
      
      <li>
        <a href="/tags">Tags</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://blog.monsterxx03.com">
          <span>Shining Moon</span>
          <img width="90px" src="https://blog.monsterxx03.com/logo.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">百种弊病,皆从懒生</p>
      <div class="my_socials">
        
        
        <a href="https://github.com/monsterxx03" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        
        
        <a href="https://blog.monsterxx03.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='/2019/07/23/%E8%BF%81%E7%A7%BB%E5%88%B0-k8s-%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/'> 迁移到 k8s 过程中碰到的问题</a></h2>
          <span class="date">2019.07.23</span>
        </div>
        <div class="post_content markdown"><p>开始把线上流量往 k8s 集群里面导了, 中间碰到了茫茫多的问题 &hellip;&hellip; 记录一下(大多都不是 k8s 的问题).</p>
<h2 id="nginx-ingress-controller-的问题">nginx ingress controller 的问题</h2>
<h3 id="zero-downtime-pods-upgrade">zero-downtime pods upgrade</h3>
<p>默认配置下, nginx ingress controller 的 upstream 是 service 的 endpoints, 在 eks 里, 就是 vpc cni plugin 分配给 pod 的 vpc ip(不是 cluster ip),
和直接使用 service cluster ip 比, 好处是:</p>
<ul>
<li>可以支持 sticky session</li>
<li>可以用 round robin 之外的负载均衡算法</li>
</ul>
<p>具体实现是, 当 service 的 endpoint 列表发生变化时, nginx ingress controller 收到通知, 对它管理的 nginx 进程发起一个 http request, 更新 endpoint ip
list(nginx 内置的 lua 来修改内存中的 ip list)</p>
<p>这样的问题是, 从 pod 被干掉到 nginx 更新之间有个时间差, 部分请求会挂掉, 解决方法可以给 pod 设置一个 preStop hook, sleep 几秒, 等 nginx ingress controller
更新完成.</p>
<p>或者给 ingress 设置 annotation 让它使用 cluster ip + port:  <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md#service-upstream">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md#service-upstream</a>, 当然这样就只能靠 iptables 做 round roubin 了, sticky session 也不支持.</p>
<h3 id="externaltrafficpolicy-local-不工作">ExternalTrafficPolicy Local 不工作</h3>
<p>这个问题比较奇怪, 我两个 eks cluster, 一个设置了 Local policy 之后 没有问题, 另一个却导致了 elb 后所有节点 healthcheck 失败.</p>
<p>实在没什么头绪, 跟踪下 ExternalTrafficPolicy 更改时候做了些什么吧, 一路跟踪到这里(eks 版本是 1.13.7) <a href="https://github.com/kubernetes/kubernetes/blob/v1.13.7/pkg/proxy/endpoints.go#L221">https://github.com/kubernetes/kubernetes/blob/v1.13.7/pkg/proxy/endpoints.go#L221</a></p>
<pre><code>isLocal := addr.NodeName != nil &amp;&amp; *addr.NodeName == ect.hostname
</code></pre>
<p>NodeName 必须和 hostname 相等, 看到这, 想起了早些时候测试 metrics-server 碰到的问题, 也是因为 hostname 造成的. 登到 worker node 上看下 hostname, 果然不是默认的 <code>ip-x-x-x-x.ec2.internal</code>, 变成了 <code>ip-x-x-x-x.yyy.net</code>, 想起 yyy.net 是早些时候在 vpc 的 dhcp option 里设置的 search domain, 应该是 cloud init 在 launch 的时候设置的.</p>
<p>自己给自己埋了个大坑, 去掉 dhcp option, 重新 launch work node, hostname 正常了, ExternalTrafficPolicy 设置成 Local 也没问题了.</p>
<h2 id="datadog-agent-的-bug">datadog-agent 的 bug</h2>
<p>当我开始把部分流量切到 k8s 上时, 发现以 daemonset 运行的 datadog-agent(一开始没有设置 cpu limit) cpu 占用异常高(800m+), 而且只在跑 nginx ingress conroller 的节点上高.
进去运行一下 <code>agent status</code>, 发现 network check 花了30s, 其他节点都只要几百ms.</p>
<p>datadog agent 的 check 都是 python 写的, 比较好调试, 发现是在获取当前 conntrack 数目的地方耗时很久,　实现是遍历挂载上去的 <code>/host/proc/net/nf_conntrack</code> 内容, 统计 conntrack 数目, nginx ingress controller 的节点上高峰时 conntrack 数目大概有 9w 多, 怪不得慢.</p>
<p>看了下 github, 新版已经修复了, 改成了读取 <code>/host/proc/sys/net/netfilter/nf_conntrack_count</code> 里的计数值: <a href="https://github.com/DataDog/integrations-core/commit/0497a14bbd1c0601f230c3e9f4a81aa2eed5ec7b#diff-dd651adb689f92282ee490f6dd77a033L418">https://github.com/DataDog/integrations-core/commit/0497a14bbd1c0601f230c3e9f4a81aa2eed5ec7b#diff-dd651adb689f92282ee490f6dd77a033L418</a>, 更新下镜像, 搞定.</p>
<h2 id="fluent-bit-的-bug">fluent-bit 的 bug</h2>
<p>fluent-bit 作为 daemonset 运行, 一边收集本地 container log, 一边作为 fluentd 的 forwarder 工作, 用来收集应用记录的 metrics events.</p>
<p>现象是 fluentd 那边记录下来的 metrics events 有大量重复. 开始不太确定是 fluentd 的问题还是 fluent-bit 的问题. 先在 fluentd 上抓了下包, 收到的 event 和记录下的是能对的上的, 那就是 fluent-bit 发重复了. 看下 github 果然有对应 issue, 新版里面也修复了: <a href="https://github.com/fluent/fluent-bit/commit/20c9cda33c33073ec3d5c0868c1a2ca5e80b5fcb">https://github.com/fluent/fluent-bit/commit/20c9cda33c33073ec3d5c0868c1a2ca5e80b5fcb</a>
, 更新搞定.</p>
<h2 id="uwsgi-的问题">uwsgi 的问题</h2>
<p>原先在 vm 上的结构是, backend 是几组 uwsgi 做容器的 rpc server(http), 前端的 web 项目调用它们, rpc server 前面套了层 nginx, 通过 <code>uwsgi_pass</code> 走 unix domain socket 和 uwsgi 进程通讯.</p>
<p>流量切换的时候, 在 k8s 内的 service 用 nginx ingress controller 暴露一个 load balancer, 在 route53 上配置权重 dns, 让 vm 和 k8s 同时 handle rpc 请求, 所以 k8s 内 service
前面也走了 nginx. 等测试稳定后, 把 k8s 内部分 service 的 base url 换成 k8s 内域名, 跳过 elb 和 nginx, 直接走 iptables 负载均衡.</p>
<p>结果一切换, 很快报了大量 502. 看了下 k8s 内 service 的 response header, 没有 Content-Length, 也没有 <code>Transfer-Encoding: chunked</code>, 原因是 uwsgi 配置错误, 我用了 <code>http-socket</code>, 这种模式需要前面加 nginx, 改成 <code>http</code>, 这才是 native 的 http 支持.</p>
<p>正确配置:</p>
<pre><code>http = :8080
http-keepalive = 75
http-auto-chunked = true
add-header = Connection: Keep-Alive
</code></pre>
<p>测试 http keepalive:</p>
<pre><code>curl -v http://localhost:8080 &gt; /dev/null http://localhost:8080 &gt; /dev/null
</code></pre>
<p>看到 <code>* Re-using existing connection! (#0) with host localhost</code> 就对了.</p>
<p>修改配置再次上线, 502 倒是没了, latency 却比之前套 nginx 的时候更高了, 明明网络少了 1 跳(elb).</p>
<p>benchmark 下 uwsgi 的 http router, latency 就是比 <code>nginx + uwsgi_pass</code> 高. 中间用 tcpdump 抓了下 uwsgi 的回包, 相同 request 数下,比 nginx 数目要多不少, 可能和某些 socket option 有关, 暂时没精力细究.
尝试把 wsgi 的容器 server 换成 gunicorn, latency 恢复了正常. 唉, 真想换 grpc 啊&hellip;..</p>
</div>
        <div class="post_footer">
          
          <div class="meta">
            <div class="info">
              <span class="field tags">
                <i class="remixicon-stack-line"></i>
                
                <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
                
                <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
                
                <a href="https://blog.monsterxx03.com/tags/server-infra/">server-infra</a>
                
              </span>
            </div>
          </div>
          
        </div>
      </div>
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mx03" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>△</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="https://blog.monsterxx03.com/js/jquery-3.3.1.min.js"></script>
<script src="https://blog.monsterxx03.com/js/zozo.js"></script>
<script src="https://blog.monsterxx03.com/js/highlight.pack.js"></script>
<link  href="https://blog.monsterxx03.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://blog.monsterxx03.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-98667627-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</body>
</html>
