<!DOCTYPE html>
<html lang="zh-cn" >
<head>
  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  
  <meta name="author"
        content=""/>

  
  <meta name="description" content="准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程, 但需要持续关注.
DNS timeout and conntrack 看到有个关于 DNS 的issue: #56903
现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候, 要通过 iptables 把　coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在 conntrack 内记录映射关系.
这可能会带来两个问题:
conntrack table 被 udp 的 dns 查询填满 udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默认 30s(net.netfilter.nf_conntrack_udp_timeout) 短时间内大量 udp 查询可能填满 conntrack, 导致丢包.
"/>
  

  

  
  <link rel="canonical" href="https://blog.monsterxx03.com/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/"/>

  

  <title>K8S: 剩下的问题 &middot; Shining Moon</title>

  <link rel="shortcut icon" href="https://blog.monsterxx03.com/images/favicon.ico"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/animate.min.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/remixicon.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/zozo.css"/>
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/highlight.css"/>

  
  
  <link rel="stylesheet" href="https://blog.monsterxx03.com/css/home.css">
  
</head>

<body>
<div class="main animated">
  <div class="nav_container animated fadeInDown">
  <div class="site_nav" id="site_nav">
    <ul>
      
      <li>
        <a href="/">Home</a>
      </li>
      
      <li>
        <a href="/posts/">Archive</a>
      </li>
      
      <li>
        <a href="/about/">About</a>
      </li>
      
      <li>
        <a href="/tags">Tags</a>
      </li>
      
    </ul>
  </div>
  <div class="menu_icon">
    <a id="menu_icon"><i class="remixicon-links-line"></i></a>
  </div>
</div>

  <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <h1>
        <a href="https://blog.monsterxx03.com/">
          <span>Shining Moon</span>
          <img width="90px" src="https://blog.monsterxx03.com/logo.png"/>
        </a>
      </h1>
    </div>
    <div class="description">
      <p class="sub_title">百种弊病,皆从懒生</p>
      <div class="my_socials">
        <a href="https://github.com/monsterxx03" title="github" target="_blank"><i class="remixicon-github-fill"></i></a>
        <a href="/index.xml" type="application/rss+xml" title="rss" target="_blank"><i class="remixicon-rss-fill"></i></a>
      </div>
    </div>
  </div>
</div>

  <div class="content">
    <div class="post_page">
      <div class="post animated fadeInDown">
        <div class="post_title post_detail_title">
          <h2><a href='/2019/06/30/k8s-%E5%89%A9%E4%B8%8B%E7%9A%84%E9%97%AE%E9%A2%98/'>K8S: 剩下的问题</a></h2>
          <span class="date">2019.06.30</span>
        </div>
        <div class="post_content markdown"><p>准备工作都差不多了, 没意外下周就该开始把线上的服务往 k8s 上迁移了. 记录几个问题，暂时不 block 我的迁移进程,
但需要持续关注.</p>
<h2 id="dns-timeout-and-conntrack">DNS timeout and conntrack</h2>
<p>看到有个关于 DNS 的issue: <a href="https://github.com/kubernetes/kubernetes/issues/56903">#56903</a></p>
<p>现象是 k8s cluster 内部 dns 查询间歇性会 5s 超时, 大致原因是 coredns 作为中心 dns 的时候,
要通过 iptables 把　coredns 的 cluster ip, 转化到它真实的可路由 ip, 中间需要 SNAT, DNAT, 并在
conntrack 内记录映射关系.</p>
<p>这可能会带来两个问题:</p>
<h3 id="conntrack-table-被-udp-的-dns-查询填满">conntrack table 被 udp 的 dns 查询填满</h3>
<p>udp 是无连接的, tcp 关闭链接就会清理 conntrack 内记录, udp 不会，只能等超时, 默认 30s(<code>net.netfilter.nf_conntrack_udp_timeout</code>)
短时间内大量 udp 查询可能填满 conntrack, 导致丢包.</p>
<p>怀疑是这个原因的话, 首先看系统允许的最大 conntrack 是多少(默认值由内存大小决定,不同发行版和内核版本上好像参数名不太一样, 我在 amazon linux 2,kernel 4.14 下测试):</p>
<pre><code>sysctl net.netfilter.nf_conntrack_max
</code></pre>
<p>查看当前已使用 conntrack 数目:</p>
<pre><code>sysctl net.netfilter.nf_conntrack_count
</code></pre>
<h3 id="snat-dnat-的资源竞争导致丢包">SNAT, DNAT 的资源竞争导致丢包</h3>
<p>在多线程 dns 查询下, SNAT,DNAT 可能会有个资源竞争的 bug, 导致部分 udp 包被 drop, 体现在 client 端就是5s 超时后重试, 详细解释和可能的 workaround 可以看这两篇:</p>
<ul>
<li><a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts">https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts</a></li>
<li><a href="https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/">https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/</a></li>
</ul>
<p>检查 conntrack -S:</p>
<pre><code>cpu=0           found=24202529 invalid=1 ignore=1419323 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=8754
cpu=1           found=23588937 invalid=1 ignore=1418117 insert=0 insert_failed=0 drop=0 early_drop=0 error=0 search_restart=8585
</code></pre>
<p>看 <code>insert_failed</code> 那项, 如果不为0, 就是这个原因造成的.</p>
<h3 id="测试">测试</h3>
<p>conntrack 填满的问题对我影响不大, 在 server 上抓了下包, 查询量不大, 4GB 内存的测试节点上, <code>nf_conntrack_max</code> 值默认有 13w 多, 完全够用.</p>
<p>SNAT, DNAT 资源竞争的问题比较搞, 写了个简单的程序来 benchmark dns 查询: <a href="https://gist.github.com/monsterxx03/8217a664d301a1b3c07c983eefcb1a3b">https://gist.github.com/monsterxx03/8217a664d301a1b3c07c983eefcb1a3b</a></p>
<p>用法:</p>
<pre><code>./dnsbench -c 100 -n 10000 -d google.com -s 5
</code></pre>
<p>并发数100, 进行 10000 次 dns 查询, 如果 response 超过 5s 就打印出来.</p>
<p>先在 host 上进行了测试, 没有一次超过 5s, 在 容器内跑, 会出现大量超过 5s 的请求. 但是在 host 上 conntrack -S 没看到任何 <code>insert_failed</code>, conntrack table
也没满.</p>
<p>怀疑是 coredns 的问题，把 pod 的 dnsPolicy 改为 Default, 这样 /etc/resolv.conf 里就会直接用 vpc 的 dns. 还是出现了大量 5s.　</p>
<p>这就比较郁闷了, 现象和已知 issue 不符. 但我不会有很大量的 dns 查询, 这个问题就先放一边了.</p>
<p>如果是 conntrack 引起的问题, 在 1.13 里有个 addon: <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/20190424-NodeLocalDNS-beta-proposal.md">NodeLocalDNS</a> 来做 workaround.</p>
<p>它大致做的是, 用 coredns 做 daemonset 来做本地的 dns cache, 在 iptables 里对去往 local dns 的 udp 查询加上 NOTRACK(可以 bypass conntrack), local dns 用 tcp 向上游 coredns 查询结果返回客户端.</p>
<h3 id="tips">tips</h3>
<p>如果 pod 的 dnsPolicy 是 ClusterFirst, /etc/resolv.conf 里是这样的:</p>
<pre><code>nameserver 172.20.0.10
search default.svc.cluster.local svc.cluster.local cluster.local ec2.internal
options ndots:5
</code></pre>
<p>会有多个 search domain, 如果此时查询一个外部域名, eg: google.com, 会依次查询:</p>
<ul>
<li>google.com.default.svc.cluster</li>
<li>google.com.svc.cluster.local</li>
<li>google.com.cluster.local</li>
<li>google.com.ec2.internal</li>
<li>google.com</li>
</ul>
<p>要第5次才能得到结果, workaround 是使用 <code>google.com.</code> 这样以点号结尾的 fqdn, 会跳过 search domain.</p>
<p>如果是 cluster 内域名, 跨 namespace 使用完整域名: <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code></p>
<h2 id="ingress-的问题">Ingress 的问题</h2>
<p>目前用的比较多的是 nginx ingress controller. 它实际就是创建了一个 type 为 Loadbalancer 的 service,
在 AWS 上会 provision 一个 ELB(4层lb), 在所有 worker 节点上创建一个 NodePort, 挂载到 ELB 后面.</p>
<p>流量进来的时候，会通过 ELB 路由到任意一个 node 的 NodePort, 问题是 nginx ingress controller 创建的 nginx
pod 不一定在该节点上, 需要下一跳找到 nginx 所在节点, 然后 nginx 再根据 ingress 规则转发到后端的 pod.</p>
<p>实际路径就可能是这样的:</p>
<pre><code>ELB -&gt; random node -&gt; nginx -&gt; service pod
</code></pre>
<p>和传统方案比多了一跳, 流量可能要到随机节点上转一圈, 还不能控制哪些节点可挂载到 ELB 后面, 如果集群中任
一节点负载过高, 对 latency 会造成影响.</p>
<p>Workaround 的方式是可以把 nginx-ingress-controller 的创建的 service 的 <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">externalTrafficPolicy</a> 改为 <code>Local</code>(默认为 Cluster).
这样 ELB 会修改 health check 的目标(Cluster 模式下是直接检查 nginx pod 的 NodePort), nginx pod 所在节点的 kube-proxy 会另
起一个端口作为 ELB health check 的目标, 其余节点的 kube-prox 不会有这个端口, health check 就会失败，不会把节点挂上去.</p>
<p>Local 模式也有缺点, 多个 pod 在多个 node 上可能分布不均匀, 但 ELB 只能对 node 做 round robin 的分发, 有可能导致 pod 负载不均衡.</p>
<p>比较理想的方式是直接把 pod ip 挂到 Cloud 的 LB 后面. <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller">aws-alb-ingress-controller</a> 配合 aws 的 vpc cni plugin 可以做到.</p>
<p>vpc cni plugin 会给每个 pod 分配一个 vpc 中的 ip, 这样对 aws 的 service 来说, pod 就直接可达.
默认情况下, alb ingress controller 也是给 service 在 全部 node 上创建一个 NodePort, 全部挂到
ALB 后面去, 但可以在 ingress 上设置 <a href="https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/#target-type">annotation</a>: <code>alb.ingress.kubernetes.io/target-type: ip</code> (默认为 instance).
ALB 就能直接把 pod ip 加到 target group 里去.</p>
<p>但目前 aws-alb-ingress-controller 有个问题是, 你每创建一个 ingress, 它都会傻傻得给你 provision 一个 ALB, 太浪费了, 有个 ingress group 功能解决这个问题, 但还没 merge: <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller/issues/914">#914</a>．</p>
<h2 id="service-topology-routing">Service topology routing</h2>
<p>k8s 内的 service 通过 iptables 做了简单的 loadbalancing， 举个栗子:</p>
<p>kube-dns 设置两个 replica, 相关的 iptables 规则如下：</p>
<pre><code>-A KUBE-SERVICES -d 172.30.0.10/32 -p udp -m comment --comment &quot;kube-system/kube-dns:dns cluster IP&quot; -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU
-A KUBE-SVC-TCOU7JCQXEZGVUNU -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-C6KPJNW6I7F56RNE
-A KUBE-SVC-TCOU7JCQXEZGVUNU -j KUBE-SEP-GSC4P52JXK5C7AQB
-A KUBE-SEP-GSC4P52JXK5C7AQB -p udp -m udp -j DNAT --to-destination 10.0.36.78:53
-A KUBE-SEP-C6KPJNW6I7F56RNE -p udp -m udp -j DNAT --to-destination 10.0.34.112:53
</code></pre>
<p>KUBE-SVC 是 service 的 chain, KUBE-SEP 是 service 对应 endpoint 的 chain.</p>
<p>在 service chain 里用 iptables 的 statistic 模块实现了个简单的 round robin 算法, 相关代码在这里: <a href="https://github.com/kubernetes/kubernetes/blob/v1.15.0/pkg/proxy/iptables/proxier.go#L1193">https://github.com/kubernetes/kubernetes/blob/v1.15.0/pkg/proxy/iptables/proxier.go#L1193</a> (syncProxyRules 这段代码真是茫茫长&hellip;)</p>
<p>这么做太简陋了, 实际业务中一个 service 的 pods 往往分布在不同的 node, available zone, 甚至跨 region 也可能.</p>
<p>在我的业务里, 一个前端的 web service 后面会调用 N 个 rpc service, 但主要流量都是去往其中某一个 service, 所以我希望通过设置 podAffinity, 让 web service pod 和那个 rpc service pod
尽量调度到一个 node 上, 并优先访问本地的 pod, 如果本地没有或挂了再访问同一 az 中其他节点的 pod, 还不行就再去找其他的 az 的 pod.</p>
<p>目前 k8s 中并没有对应的实现, 但有个 KEP 所提的内容和我想要的是一样的: <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0033-service-topology.md">https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0033-service-topology.md</a></p>
<p>设计是在 service spec 中增加一个 <code>topologyKeys</code>, 用法例子:</p>
<pre><code>kind: Service
metadata:
  name: service-local
spec:
  topologyKeys: [&quot;host&quot;, &quot;zone&quot;]
</code></pre>
<p>优先找同 host 的 pod, 找不到就找同一 zone 的, 还没有就让 connection 失败(hard topology).</p>
<p>如果 topologyKeys 设置为 [&ldquo;host&rdquo;, &ldquo;&rdquo;], 就优先找同 host pod, 找不到就找其他节点的， 不让 connection 失败(soft topology).</p>
</div>
        <div class="post_footer">
          
          <div class="meta">
            <div class="info">
              <span class="field tags">
                <i class="remixicon-stack-line"></i>
                
                <a href="https://blog.monsterxx03.com/tags/k8s/">k8s</a>
                
                <a href="https://blog.monsterxx03.com/tags/eks/">eks</a>
                
                <a href="https://blog.monsterxx03.com/tags/server-infra/">server-infra</a>
                
              </span>
            </div>
          </div>
          
        </div>
      </div>
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mx03" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
    </div>
  </div>
  <a id="back_to_top" href="#" class="back_to_top"><span>△</span></a>
</div>
<footer class="footer">
  <div class="powered_by">
    <a href="https://zeuk.me">Designed by Zeuk,</a>
    <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
  </div>

  <div class="footer_slogan">
    <span></span>
  </div>
</footer>



<script src="https://blog.monsterxx03.com/js/jquery-3.3.1.min.js"></script>
<script src="https://blog.monsterxx03.com/js/zozo.js"></script>
<script src="https://blog.monsterxx03.com/js/highlight.pack.js"></script>
<link  href="https://blog.monsterxx03.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://blog.monsterxx03.com/js/fancybox.min.js"></script>

<script>hljs.initHighlightingOnLoad()</script>


  <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <script async src="https://www.googletagmanager.com/gtag/js?id=G-P32CTT1G2G"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-P32CTT1G2G');
        }
      </script>


</body>
</html>
